[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Vladimir Kukushkin, and I have over 10 years of experience as a data scientist / data analyst. The slash signifies that I typically work at the intersection of product analytics and data science, tackling problems that require advanced techniques like predictive analytics, causal inference, and building statistical models. My specialization is user behavior analysis a.k.a. quantitative UX research or, more broadly, sequential data analysis.\nMy projects:\n\nretentioneering. An open-source Python library for user behavior analysis.\n\nMy publications:\n\nPrediction of Hourly Earnings and Completion Time on a Crowdsourcing Platform (KDD-2020)\nOn the Relation Between Assessor’s Agreement and Accuracy in Gamified Relevance Assessment (SIGIR-2015)"
  },
  {
    "objectID": "posts/2024-10-22_Welcome/index.html",
    "href": "posts/2024-10-22_Welcome/index.html",
    "title": "Welcome to Journal Papers Product Minds Club",
    "section": "",
    "text": "I’ve been working as a data scientist / data analyst over 10 years. The slash signifies that I typically work at the intersection of product analytics and data science, tackling problems that require advanced techniques like predictive analytics, causal inference, and building statistical models. My specialization is in user behavior analysis a.k.a. quantitative UX research or, more broadly, in analyzing sequential data. In addition to working as a data analyst, I played a key role in developing an open-source Python library retentioneering, which aims to simplify research in this area.\nIn my view, this field deserves much more attention than it currently receives. While these analyses are more complex than traditional funnel or cohort analysis, quantitative UX provides a deeper understanding of how users interact with your product. It reveals insights such as:\n\nWhat drives users to churn, make purchases, subscribe, etc. (causal inference),\nWhat segments users form (cluster analysis),\nWhat caused unexpected behavior in dashboard charts (root cause analysis)\nWhat distinguishes one user group from another, e.g. mobile VS desktop, AB-experiment groups, before and after release.\n\nThese research questions are not an empty sound. Finding the answers helps identify product bottlenecks, understand what drives user satisfaction or dissatisfaction, and ultimately improve your product. This is the core purpose of product analytics.\nGiven the lack of substantial literature on the subject, here I aim to collect and review papers, books, and publications that I find insightful.\nLike how Sgt. Pepper’s Lonely Hearts Club Band expanded horizons in music, I hope this journal club will attract data analysts and data scientists eager to broaden their understanding of these topics. The term “club” in the title reflects my wish to engage with enthusiasts and foster discussions around these fascinating subjects.  ♫ Music playing ♫ We’re Sgt. Pepper’s Lonely Hearts Club Band We hope you will enjoy the show"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Modeling DAU with Markov chain\n\n\n\n\n\n\nproduct analytics\n\n\npredictive analytics\n\n\n\n\n\n\n\n\n\nDec 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSequen-C: A Multilevel Overview of Temporal Event Sequences\n\n\n\n\n\n\njournalclub\n\n\nclickstream visualizations\n\n\n\n\n\n\n\n\n\nNov 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to Journal Papers Product Minds Club\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 22, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-11-05_Sequen-C/index.html",
    "href": "posts/2024-11-05_Sequen-C/index.html",
    "title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences",
    "section": "",
    "text": "My name is Vladimir Kukushkin. I’ve been working as a data analyst/data scientist for more than 10 years. A couple of years ago I heavily shifted to quantitative UX research area which turned into my passion. While eagerly trying to find some sources to read regarding this topic, I realized that there are not so many of them. That’s why I decided to write a series of posts towards quantitative UX research. Basically, it will be reviews of papers, books and other publications that I find insightful. I’d be happy to find here enthusiasts who are also interested in this domain.\nLet me start our journal club with the most impressive paper related to clickstream visualizations I’ve ever read: Sequen-C: A Multilevel Overview of Temporal Event Sequences by Jessica Magallanes, Tony Stone, Paul D. Morris, Suzanne Mason, Steven Wood, and Maria-Cruz Villa-Uriol.\nThe authors address two very common issues that any quantitative UX researcher encounters:\nIn the paper, the authors describe an interactive visualization tool that tackles both these issues and provides a methodology they call Align-Score-Simplify."
  },
  {
    "objectID": "posts/2024-11-05_Sequen-C/index.html#methodology",
    "href": "posts/2024-11-05_Sequen-C/index.html#methodology",
    "title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences",
    "section": "Methodology",
    "text": "Methodology\nThe first problem is addressed by applying (agglomerative) clustering. I’ll explain further why they chose this algorithm. The second problem is solved using their framework called Align-Score-Simplify.\n\nAlign\nThis part is the most elegant, amazing, and surprizing in the whole paper. They apply the MSA algorithm (Multiple Sequence Alignment) stemming from bioinformatics. Originally, it was created to align amino acids or nucleotides in DNA sequences to identify common subsequences across DNAs belonging to multiple species. But this is exactly one of the goals we pursue in clickstream data analysis. Once we align user trajectories in similar way, we understand what events are common for the most users at some specific steps.\n\n\n\n\nInserting - as gaps to align events in sequences. The coloured events might be considered as common at some steps. The others are considered as noise.\n\n\n\n\n\nScore\nLet \\(\\lambda\\) be the output of the MSA algorithm applied to a set of unique sequences \\(S\\) (the frequency of each path is denoted as \\(P_i\\)). They calculate the information score \\(I_j\\) for each column \\(j\\) in \\(\\lambda\\) as a measure of the column’s impurity, somewhat similar to entropy but with some additional penalty for the high amount of gaps:\n\\[\nI_j = 1 - \\frac{E_j}{\\log_2(|A| + 1)}, \\;\\;\\;\\; E_j = \\sum_{a\\in A_j\\cup\\{-\\}}\n\\begin{cases}\n-P_a\\log_2\\left(\\frac{P_a}{G_j}\\right)\\, & \\text{if } a = \\text{'-'}\\\\\n-P_a\\log_2 P_a, & \\text{otherwise},\n\\end{cases}\n\\]\nwhere\n\n\\(A\\) – the set of unique events in \\(S\\),\n\\(A_j\\) – the set of unique events in column \\(j\\),\n\\(P_a\\) – the probability of the event \\(a\\) in column \\(j\\),\n\\(G_j\\) – is the count of gaps in column \\(j\\).\n\n\n\nSimplify\nOnce the \\(I_j\\) score is calculated for each column \\(j\\), we can treat each column that exceeds a certain threshold \\(I_\\tau\\) as an event that prevails in this column, while all other columns might be collapsed as noise. As a result, we get a new table \\(\\alpha\\) that simplifies the initial clickstream \\(S\\), as shown in the image below.\n\n\n\n\nThe outline of the Align-Score-Simplify workflow.\n\n\n\nFinally, we can granulate our clickstream representation using two axes:\n\nWe can consider different clusters of the agglomerative clustering output tree \\(T\\). Now it’s clear why they preferred this clustering algorithm instead of, say, K-Means. Considering different levels of \\(T\\), we obtain more/less homogeneous clusters. More homogeneous clusters provide better and clearer output of the MSA algorithm.\n\\(\\tau\\). By varying \\(\\tau\\), we can make the sequence visualization more/less coarse.\n\nAs a result, we get the interface like this:\n\n\n\n\nThe outline of the whole framework. (A) Building aggregate tree \\(T\\) for input unique sequences \\(S = \\{s_1, ...s_6\\}\\). (B) Each node in \\(T\\) has an alignment matrix \\(\\lambda\\) for its child sequences, a row-wise probabilities vector \\(P\\), and a column-wise information score vector \\(I\\). Two or more consecutive columns in \\(\\lambda\\) with \\(I_j &lt; 0.8\\) are not coloured. (C) Multilevel overviews for a range of number of clusters \\(k\\) retrieved from \\(T\\), where black blocks represent merged columns. Image by authors.\n\n\n\nIn the following sections, the authors describe the GUI of a more complex tool developed as an application for a couple of particular datasets from the public health domain. I won’t talk about it here since its underlying idea is the same, while the interface is more sophisticated and includes some information about events distribution, unique sequence view, and individual sequence view.\nThe datasets are more related to process mining, so the event cardinality is low. This is much lower than what happens in product analytics. The paths are very structured with few deviations from the main flow. That’s why I’m quite skeptical about applying this framework to product analytics clickstream data. Additionally, it’s not clear how fast MSA would work with clickstream data.\nAnyway, the idea of the paper and the framework are super interesting to me. It would be great to implement such a tool someday."
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "",
    "text": "Doubtlessly, DAU, WAU, and MAU are critical business metrics. An article “How Duolingo reignited user growth” by Jorge Mazal, former CPO of Duolingo is #1 the Growth section of Lenny’s Newsletter blog. In this article Jorge payed special attention to the methodology Duolingo used to model DAU metrics (see another article “Meaningful metrics: how data sharpened the focus of product teams” by Erin Gustafson). This methodology has multiple strenghts but I focus here on how one can use it for DAU forecasting.\nNew year is coming soon, so many companies are planning their budgets for the next year these days. Cost estimations are often require DAU forecast. In this article I’ll show you how you can get this prediction using the Duolingo’s growth model and share a DAU & MAU “calculator” designed as a Google Spreadsheet calculator."
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#duolingos-growth-model",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#duolingos-growth-model",
    "title": "DAU & MAU prediction",
    "section": "Duolingo’s growth model",
    "text": "Duolingo’s growth model\nA quick recap on how the Duolingo’s growth model works. At day \\(d\\) (\\(d=1,2,\\ldots,\\)) of a user’s lifetime the user can be in one of the following 7 (mutually-exclusive) states:\n\n\n\n\nstate\n\n\nd = 1\n\n\nis active today\n\n\nwas active in [d-6, d-1]\n\n\nwas active in [d-29, d-7]\n\n\nwas active before d-30\n\n\n\n\n\nnew\n\n\n✅\n\n\n❓\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n\n\ncurrent\n\n\n❌\n\n\n✅\n\n\n✅\n\n\n❓\n\n\n❓\n\n\n\n\nreactivated\n\n\n❌\n\n\n✅\n\n\n❌\n\n\n✅\n\n\n❓\n\n\n\n\nresurrected\n\n\n❌\n\n\n✅\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n\n\nat_risk_wau\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n❓\n\n\n❓\n\n\n\n\nat_risk_mau\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n❓\n\n\n\n\ndormant\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n\nHaving these states defined, we can consider a user’s lifetime trajectory as a Markov chain. Let \\(M\\) be a transition matrix\nThe beauty and simplicity of this approach is that now we can fully describe states of the all users registered by a given day.\nIndeed, let \\(M\\) be the transition matrix derived from the historical data."
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#methodology",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#methodology",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "Methodology",
    "text": "Methodology\nA quick recap on how the Duolingo’s growth model works. At day \\(d\\) (\\(d=1,2,\\ldots,\\)) of a user’s lifetime the user can be in one of the following 7 (mutually-exclusive) states:\n\n\n\n\nstate\n\n\nd = 1\n\n\nis active today\n\n\nwas active in [d-6, d-1]\n\n\nwas active in [d-29, d-7]\n\n\nwas active before d-30\n\n\n\n\n\nnew\n\n\n✅\n\n\n❓\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n\n\ncurrent\n\n\n❌\n\n\n✅\n\n\n✅\n\n\n❓\n\n\n❓\n\n\n\n\nreactivated\n\n\n❌\n\n\n✅\n\n\n❌\n\n\n✅\n\n\n❓\n\n\n\n\nresurrected\n\n\n❌\n\n\n✅\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n\n\nat_risk_wau\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n❓\n\n\n❓\n\n\n\n\nat_risk_mau\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n❓\n\n\n\n\ndormant\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n\nHaving these states defined (as set \\(S\\)), we can consider a user’s lifetime trajectory as a Markov chain. Let \\(M\\) be a transition matrix associated with this Markov chain: \\(m_{i, j} = P(s_j | s_i)\\) are the probabilities that a user moves to state \\(s_j\\) right after being at state \\(s_i\\), \\(s_i, s_j \\in S\\). The matrix values are easily fetched from the historical data.\nThe beauty and simplicity of this approach is that matrix \\(M\\) fully describes states of the all users in the future. Suppose that vector \\(u_0\\) of length 7 contains the counts of users being in certain states at some calendar day denoted as 0. Thus, according to the Markov model, in the next day \\(u_1\\) we expect to have the following amount of users:\n\\[\n\\underbrace{\n\\begin{pmatrix}  \\#New_1 \\\\ \\#Current_1 \\\\ \\#Reactivated_1 \\\\ \\#Resurrected_1 \\\\ \\#AtRiskWau_1 \\\\ \\#AtRiskMau_1 \\\\ \\#Dormant_1 \\end{pmatrix}\n}_{u_1} = M^T \\cdot\n\\underbrace{\n\\begin{pmatrix}  \\#New_0 \\\\ \\#Current_0 \\\\ \\#Reactivated_0 \\\\ \\#Resurrected_0 \\\\ \\#AtRiskWau_0 \\\\ \\#AtRiskMau_0 \\\\ \\#Dormant_0 \\end{pmatrix}\n}_{u_0}\n\\]\nApplying this formula recursevely, we derive the amount of the users at any arbitrary day \\(t &gt; 0\\) in the future. The only thing we need to provide despite of the initial distribution \\(u_0\\) is to the amount of new users that would appear in the product each day in the future. We’ll get it by using historical data on new users appeared in the past and appyling the prophet library.\nNow, having \\(u_t\\) calculated, we can calculate DAU values at day t: \\[DAU_t = \\#New_t + \\#Current_t + \\#Reactivated_t +\\#Resurrected_t.\\]\nAdditionally, we can easily calculate WAU and MAU metrics: \\[WAU_t = DAU_t +\\#AtRiskWau_t,\\] \\[MAU_t = DAU_t +\\#AtRiskWau_t + \\#AtRiskMau_t.\\]\nFinally, the algorithm looks like this:\n\nCalculate initial counts \\(u_0\\) corresponding to the day right before prediction.\nFor each prediction day \\(t=1, ..., T\\) calculate the expected amount of new users \\(\\#New_1, \\ldots, \\#New_T\\).\nCalculate recursively \\(u_{t+1} = M^T u_t\\).\nCalculate DAU, WAU, MAU for each prediction day \\(t=1, ..., T\\)."
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#getting-the-states",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#getting-the-states",
    "title": "Modeling DAU, WAU, MAU with Markov chain",
    "section": "Getting the states",
    "text": "Getting the states\nWe use a simulated dataset that is stored in the event_log.csv file.\n\nimport duckdb\nimport pandas as pd"
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#pred",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#pred",
    "title": "DAU prediction",
    "section": "Pred",
    "text": "Pred"
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#predicting-new-users-amount",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#predicting-new-users-amount",
    "title": "Modeling DAU, WAU, MAU with Markov chain",
    "section": "Predicting new users amount",
    "text": "Predicting new users amount"
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#predicting-dau",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#predicting-dau",
    "title": "Modeling DAU, WAU, MAU with Markov chain",
    "section": "Predicting DAU",
    "text": "Predicting DAU"
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#prediction-testing",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#prediction-testing",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "Prediction testing",
    "text": "Prediction testing\n\nstate0 = get_state0('2022-06-30')\nM = get_transition_matrix(transitions, '2022-06-23', '2022-06-30')\ndau_pred = predict_dau(M, state0, '2022-07-01', '2022-10-31', new_users)\n\ndau_true\\\n    .join(dau_pred[['dau']], how='inner', lsuffix='_true', rsuffix='_pred')\\\n    .plot()"
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#conclusions",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#conclusions",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "Conclusions",
    "text": "Conclusions"
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#discussion",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#discussion",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#implementation",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#implementation",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "Implementation",
    "text": "Implementation\nSuppose that today is 2022-10-31 and we want to predict the DAU metric for the next year. We define PREDICTION_START and PREDICTION_END constants to set this prediction range.\n\nPREDICTION_START = '2022-11-01'\nPREDICTION_END = '2023-12-31'\n\nWe use a simulated dataset which is based on historical data of some SAAS app. The data is stored in the dau_data.csv file and contains three columns user_id, date, registration_date. Each record indicates a day when a user was active.\nThe data includes the activity indicators for all users spotted in the app from 2020-11-01 to 2022-10-31. An additional month 2020-10 is included in order to calculate user states correctly (AtRiskMau and dormant states requires data 1 month behind).\n\nimport pandas as pd\n\ndf = pd.read_csv('dau_data.csv.gz', compression='gzip')\nprint(f'Shape: {df.shape}')\nprint(f'Total users: {df['user_id'].nunique()}')\nprint(f'Data range: [{df['date'].min()}, {df['date'].max()}]')\ndf.head()\n\nShape: (447048, 3)\nTotal users: 38093\nData range: [2020-10-01, 2022-10-31]\n\n\n\n\n\n\n\n\n\nuser_id\ndate\nregistration_date\n\n\n\n\n0\n7a010840-b4d1-543d-bd4c-fbb4ae2198c5\n2020-10-01\n2020-08-26\n\n\n1\nd565a211-1996-538b-b067-11a38616c8cf\n2020-10-01\n2020-10-01\n\n\n2\n41fb29c7-8122-59db-a690-e9d9502f3c38\n2020-10-01\n2020-09-02\n\n\n3\n310c15a2-fe92-5703-be1f-80270c1b83bd\n2020-10-01\n2020-08-27\n\n\n4\n234c3035-145f-5694-bd7f-bec58a5f0c5c\n2020-10-01\n2020-05-31\n\n\n\n\n\n\n\nIn practice, the most calculations are reasonable to run as SQL queries to a database where the data is stored. To simulate this, I’ll use the duckdb library.\n\nimport duckdb\n\n\nPredicting new users amount\nLet’s start from the new users prediction. One of the easiest ways to do this is to use the prophet library. It simply requires a historical time-series and extends it in the future. The new_users Series contains such a historical data. A condition that indicates that a new user appeared is df['date'] == df['registration_date'].\n\nnew_users = df[df['date'] == df['registration_date']]\\\n    .assign(date=pd.to_datetime(df['date']))\\\n    .groupby('date').size()\n\nnew_users.head()\n\ndate\n2020-10-01    4\n2020-10-02    3\n2020-10-03    3\n2020-10-04    4\n2020-10-05    8\ndtype: int64\n\n\nprohet requires a time-series as a DataFrame containing two columns, so we reformat new_users Series to new_users_prophet DataFrame. Another thing we need to prepare is to create the future variable containing certain days for prediction: from PREDICTION_START to PREDICTION_END. The plot illustrates predictions for both past and future dates.\n\nfrom prophet import Prophet\n\nm = Prophet()\nnew_users_prophet = pd.DataFrame({'ds': new_users.index, 'y': new_users.values})\nm.fit(new_users_prophet)\n\nperiods = len(pd.date_range(PREDICTION_START, PREDICTION_END))\nfuture = m.make_future_dataframe(periods=periods)\nm.plot(m.predict(future));\n\n21:53:31 - cmdstanpy - INFO - Chain [1] start processing\n21:53:31 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n\n\n\n\nNow, let’s create a Series new_users_pred where we’ll keep predictions for the new users amount.\n\nnew_users_pred = m.predict(future)\\\n    .assign(yhat=lambda _df: _df['yhat'].astype(int))\\\n    .rename(columns={'ds': 'date', 'yhat': 'count'})\\\n    .loc[lambda _df: _df['date'] &gt;= PREDICTION_START]\\\n    .set_index('date')\\\n    ['count']\n\nnew_users_pred.head()\n\ndate\n2022-11-01    49\n2022-11-02    47\n2022-11-03    43\n2022-11-04    39\n2022-11-05    41\nName: count, dtype: int64\n\n\n\n\nGetting the states\nSo we want assign one of 7 states for each day of a user’s lifetime (meaning lifetime within the app). According to the definition, for each day we need to consider at least 30 days in the past. This is when SQL window functions come in. However, since df data contains only active day indicators, we need to explicitly extend it with the days when a user was not active.\nFor readability purposes we split the next SQL query into multiple subqueries.\n\ndau. Simply transforms dates from string to date data type.\nfull_range. Creates a full sequence of dates for each user.\n\n\nDATASET_START = '2021-11-01'\nDATASET_END = '2022-10-31'\nOBSERVATION_START = '2021-10-01'\n\nquery = f\"\"\"\nWITH\ndau AS (\n    SELECT\n        user_id,\n        date::date AS date,\n        registration_date::date AS registration_date\n    FROM df\n),\nfull_range AS (\n    SELECT\n        user_id, UNNEST(generate_series(greatest(registration_date, '{OBSERVATION_START}'), date '{DATASET_END}', INTERVAL 1 DAY))::date AS date\n    FROM (\n        SELECT DISTINCT user_id, registration_date FROM dau\n    )\n),\ndau_full AS (\n    SELECT\n        fr.user_id,\n        fr.date,\n        dau.date IS NOT NULL AS is_active,\n        registration_date\n    FROM full_range AS fr\n    LEFT JOIN dau USING(user_id, date)\n),\nstates AS (\n    SELECT\n        user_id,\n        date,\n        is_active,\n        first_value(registration_date IGNORE NULLS) OVER (PARTITION BY user_id ORDER BY date) AS registration_date,\n        SUM(is_active::int) OVER (PARTITION BY user_id ORDER BY date ROWS BETWEEN 6 PRECEDING and 1 PRECEDING) AS active_days_back_6d,\n        SUM(is_active::int) OVER (PARTITION BY user_id ORDER BY date ROWS BETWEEN 29 PRECEDING and 1 PRECEDING) AS active_days_back_29d,\n        CASE\n            WHEN date = registration_date THEN 'new'\n            WHEN is_active = TRUE AND active_days_back_6d BETWEEN 1 and 6 THEN 'current'\n            WHEN is_active = TRUE AND active_days_back_6d = 0 AND IFNULL(active_days_back_29d, 0) &gt; 0 THEN 'reactivated'\n            WHEN is_active = TRUE AND active_days_back_6d = 0 AND IFNULL(active_days_back_29d, 0) = 0 THEN 'resurrected'\n            WHEN is_active = FALSE AND active_days_back_6d &gt; 0 THEN 'at_risk_wau'\n            WHEN is_active = FALSE AND active_days_back_6d = 0 AND ifnull(active_days_back_29d, 0) &gt; 0 THEN 'at_risk_mau'\n            ELSE 'dormant'\n        END AS state\n    FROM dau_full\n)\nSELECT user_id, date, state FROM states\nWHERE date BETWEEN '{DATASET_START}' AND '{DATASET_END}'\nORDER BY user_id, date\n\"\"\"\nstates = duckdb.sql(query).df()\n\n\nquery = f\"\"\"\nSELECT\n    date,\n    state_from,\n    state_to,\n    COUNT(*) AS cnt,\nFROM (\n    SELECT\n        date,\n        state AS state_to,\n        lag(state) OVER (PARTITION BY user_id ORDER BY date) AS state_from\n    FROM states\n)\nWHERE state_from IS NOT NULL\nGROUP BY date, state_from, state_to\nORDER BY date, state_from, state_to;\n\"\"\"\ntransitions = duckdb.sql(query).df()\n\n\nPREDICTION_START = '2022-11-01'\nPREDICTION_END = '2023-12-31'\n\nstates_order = ['new', 'current', 'reactivated', 'resurrected', 'at_risk_wau', 'at_risk_mau', 'dormant']\n\ndef get_transition_matrix(transitions, date1, date2):\n    probs = transitions\\\n        .loc[lambda _df: _df['date'].between(date1, date2)]\\\n        .groupby(['state_from', 'state_to'], as_index=False)\\\n        ['cnt'].sum()\\\n        .assign(\n            supp=lambda _df: _df.groupby('state_from')['cnt'].transform('sum'),\n            prob=lambda _df: _df['cnt'] / _df['supp']\n        )\n\n    M = probs.pivot(index='state_from', columns='state_to', values='prob')\\\n        .reindex(states_order, axis=0)\\\n        .reindex(states_order, axis=1)\\\n        .fillna(0)\\\n        .astype(float)\n\n    return M\n\n\n\nPredicting DAU\n\ndef predict_dau(M, state0, date1, date2, new_users):\n    \"\"\"\n    Predicts DAU over a given date range.\n\n    Parameters\n    ----------\n    M : pandas.DataFrame\n        Transition matrix representing user state changes.\n    state0 : pandas.Series\n        counts of initial state of users.\n    date1 : str\n        Start date of the prediction period in 'YYYY-MM-DD' format.\n    date2 : str\n        End date of the prediction period in 'YYYY-MM-DD' format.\n    new_users : int or pandas.Series\n        The expected amount of new users for each day between date1 and date2.\n        If a Series, it should have dates as the index.\n        If an int, the same number is used for each day.\n        \n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame containing the predicted DAU, WAU, and MAU for each day in the date range,\n        with columns for different user states and tot.\n    \"\"\"\n    \n    dates = pd.date_range(date1, date2)\n    dates.name = 'date'\n    dau_pred = []\n    new_dau = state0.copy()\n    for date in dates:\n        new_dau = (M.transpose() @ new_dau).astype(int)\n        if isinstance(new_users, int):\n            new_users_today = new_users\n        else:\n            new_users_today = new_users.astype(int).loc[date] \n        new_dau.loc['new'] = new_users_today\n        dau_pred.append(new_dau.tolist())\n\n    dau_pred = pd.DataFrame(dau_pred, index=dates, columns=states_order)\n    dau_pred['dau'] = dau_pred['new'] + dau_pred['current'] + dau_pred['reactivated'] + dau_pred['resurrected']\n    dau_pred['wau'] = dau_pred['dau'] + dau_pred['at_risk_wau']\n    dau_pred['mau'] = dau_pred['dau'] + dau_pred['at_risk_wau'] + dau_pred['at_risk_mau']\n    \n    return dau_pred\n\n\ndef get_state0(date):\n    query = f\"\"\"\n    SELECT state, count(*) AS cnt\n    FROM states\n    WHERE date = '{date}'\n    GROUP BY state\n    \"\"\"\n\n    state0 = duckdb.sql(query).df()\n    state0 = state0.set_index('state').reindex(states_order)['cnt']\n    \n    return state0\n\n\nM = get_transition_matrix(transitions, '2022-10-25', '2022-10-31')\nstate0 = get_state0(DATASET_END)\ndau_pred = predict_dau(M, state0, PREDICTION_START, PREDICTION_END, new_users_pred)\ndau_pred\n\n\n\n\n\n\n\n\nnew\ncurrent\nreactivated\nresurrected\nat_risk_wau\nat_risk_mau\ndormant\ndau\nwau\nmau\n\n\n\n\n2022-11-01\n49\n444\n13\n13\n451\n1269\n16795\n519\n970\n2239\n\n\n2022-11-02\n47\n446\n13\n13\n462\n1267\n16830\n519\n981\n2248\n\n\n2022-11-03\n43\n448\n13\n13\n470\n1267\n16865\n517\n987\n2254\n\n\n2022-11-04\n39\n448\n13\n13\n474\n1268\n16900\n513\n987\n2255\n\n\n2022-11-05\n41\n447\n13\n13\n475\n1269\n16935\n514\n989\n2258\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-12-27\n135\n1172\n34\n36\n1266\n3247\n45910\n1377\n2643\n5890\n\n\n2023-12-28\n134\n1187\n34\n36\n1282\n3258\n45999\n1391\n2673\n5931\n\n\n2023-12-29\n132\n1201\n34\n36\n1297\n3271\n46088\n1403\n2700\n5971\n\n\n2023-12-30\n137\n1213\n34\n36\n1309\n3285\n46178\n1420\n2729\n6014\n\n\n2023-12-31\n151\n1227\n34\n36\n1323\n3300\n46268\n1448\n2771\n6071\n\n\n\n\n426 rows × 10 columns\n\n\n\n\nquery = \"\"\"\nSELECT date, COUNT(*) AS dau\nFROM states\nWHERE state IN ('new', 'current', 'reactivated', 'resurrected')\nGROUP BY date\nORDER BY date\n\"\"\"\n\ndau_true = duckdb.sql(query).df()\\\n    .set_index('date')\npd.concat([dau_true, dau_pred['dau']]).plot()"
  },
  {
    "objectID": "posts/2024-11-01_dau_prediction/dau_prediction.html",
    "href": "posts/2024-11-01_dau_prediction/dau_prediction.html",
    "title": "Modeling DAU with Markov chain (WIP)",
    "section": "",
    "text": "Doubtlessly, DAU, WAU, and MAU are critical business metrics. An article “How Duolingo reignited user growth” by Jorge Mazal, former CPO of Duolingo is #1 the Growth section of Lenny’s Newsletter blog. In this article Jorge payed special attention to the methodology Duolingo used to model DAU metrics (see another article “Meaningful metrics: how data sharpened the focus of product teams” by Erin Gustafson). This methodology has multiple strenghts but I focus here on how one can use it for DAU forecasting.\nNew year is coming soon, so many companies are planning their budgets for the next year these days. Cost estimations are often require DAU forecast. In this article I’ll show you how you can get this prediction using the Duolingo’s growth model and share a DAU & MAU “calculator” designed as a Google Spreadsheet calculator."
  },
  {
    "objectID": "posts/2024-11-01_dau_prediction/dau_prediction.html#methodology",
    "href": "posts/2024-11-01_dau_prediction/dau_prediction.html#methodology",
    "title": "Modeling DAU with Markov chain (WIP)",
    "section": "2. Methodology",
    "text": "2. Methodology\nA quick recap on how the Duolingo’s growth model works. At day \\(d\\) (\\(d=1,2,\\ldots,\\)) of a user’s lifetime the user can be in one of the following 7 (mutually-exclusive) states:\n\n\n\n\nstate\n\n\nd = 1\n\n\nactivetoday\n\n\nactive in[d-6, d-1]\n\n\nactive in[d-29, d-7]\n\n\nactivebefore d-30\n\n\n\n\n\nnew\n\n\n✅\n\n\n?\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\ncurrent\n\n\n❌\n\n\n✅\n\n\n✅\n\n\n❓\n\n\n❓\n\n\n\n\nreactivated\n\n\n❌\n\n\n✅\n\n\n❌\n\n\n✅\n\n\n?\n\n\n\n\nresurrected\n\n\n❌\n\n\n✅\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n\n\nat_risk_wau\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n?\n\n\n?\n\n\n\n\nat_risk_mau\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n?\n\n\n\n\ndormant\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n\nHaving these states defined (as set \\(S\\)), we can consider a user’s lifetime trajectory as a Markov chain. Let \\(M\\) be a transition matrix associated with this Markov chain: \\(m_{i, j} = P(s_j | s_i)\\) are the probabilities that a user moves to state \\(s_j\\) right after being at state \\(s_i\\), \\(s_i, s_j \\in S\\). The matrix values are easily fetched from the historical data.\nIf we assume that the user behavior is stationary, the matrix \\(M\\) fully describes states of the all users in the future. Suppose that vector \\(u_0\\) of length 7 contains the counts of users being in certain states at some calendar day denoted as 0. Thus, according to the Markov model, in the next day \\(u_1\\) we expect to have the following amount of users:\n\\[\n\\underbrace{\n\\begin{pmatrix}  \\#New_1 \\\\ \\#Current_1 \\\\ \\#Reactivated_1 \\\\ \\#Resurrected_1 \\\\ \\#AtRiskWau_1 \\\\ \\#AtRiskMau_1 \\\\ \\#Dormant_1 \\end{pmatrix}\n}_{u_1} = M^T \\cdot\n\\underbrace{\n\\begin{pmatrix}  \\#New_0 \\\\ \\#Current_0 \\\\ \\#Reactivated_0 \\\\ \\#Resurrected_0 \\\\ \\#AtRiskWau_0 \\\\ \\#AtRiskMau_0 \\\\ \\#Dormant_0 \\end{pmatrix}\n}_{u_0}\n\\]\nApplying this formula recursevely, we derive the amount of the users at any arbitrary day \\(t &gt; 0\\) in the future. The only thing we need to provide despite of the initial distribution \\(u_0\\) is to the amount of new users that would appear in the product each day in the future. We’ll get it by using historical data on new users appeared in the past and appyling the prophet library.\nNow, having \\(u_t\\) calculated, we can calculate DAU values at day t: \\[DAU_t = \\#New_t + \\#Current_t + \\#Reactivated_t +\\#Resurrected_t.\\]\nAdditionally, we can easily calculate WAU and MAU metrics: \\[WAU_t = DAU_t +\\#AtRiskWau_t,\\] \\[MAU_t = DAU_t +\\#AtRiskWau_t + \\#AtRiskMau_t.\\]\nFinally, the algorithm looks like this:\n\nFor each prediction day \\(t=1, ..., T\\) calculate the expected amount of new users \\(\\#New_1, \\ldots, \\#New_T\\).\nFor each lifetime day of each user define on of the 7 states.\nCalculate the transition matrix \\(M\\).\nCalculate initial counts \\(u_0\\) corresponding to \\(t=0\\) day.\nCalculate recursively \\(u_{t+1} = M^T u_t\\).\nCalculate DAU, WAU, MAU for each prediction day \\(t=1, ..., T\\)."
  },
  {
    "objectID": "posts/2024-11-01_dau_prediction/dau_prediction.html#implementation",
    "href": "posts/2024-11-01_dau_prediction/dau_prediction.html#implementation",
    "title": "Modeling DAU with Markov chain (WIP)",
    "section": "3. Implementation",
    "text": "3. Implementation\n\n3.1 Dataset\nWe use a simulated dataset based on historical data of a SAAS app. The data is stored in the dau_data.csv.gz file and contains three columns: user_id, date, and registration_date. Each record indicates a day when a user was active.\nThe data includes activity indicators for all users from 2020-11-01 to 2023-10-31. An additional month, October 2020, is included to calculate user states correctly (at_risk_mau and dormant states require data from one month prior).\n\nimport pandas as pd\n\ndf = pd.read_csv('dau_data.csv.gz', compression='gzip')\ndf['date'] = pd.to_datetime(df['date'])\ndf['registration_date'] = pd.to_datetime(df['registration_date'])\n\nprint(f'Shape: {df.shape}')\nprint(f'Total users: {df['user_id'].nunique()}')\nprint(f'Data range: [{df['date'].min()}, {df['date'].max()}]')\ndf.head()\n\nShape: (667236, 3)\nTotal users: 51480\nData range: [2020-10-01 00:00:00, 2023-10-31 00:00:00]\n\n\n\n\n\n\n\n\n\nuser_id\ndate\nregistration_date\n\n\n\n\n0\nd8c465ab-e9fd-5edd-9e4e-c77094700cb5\n2020-10-01\n2020-08-25\n\n\n1\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-01\n2020-08-25\n\n\n2\nbfeac474-5b66-566f-8654-262bb79c873e\n2020-10-01\n2020-05-31\n\n\n3\nd32fcac5-122c-5463-8aea-01b39b9ad0bb\n2020-10-01\n2020-09-30\n\n\n4\nc1ece677-e643-5bb3-8701-f1c59a0bf4cd\n2020-10-01\n2020-09-05\n\n\n\n\n\n\n\nThis is how the DAU time-series looks like up to 2023-10-31.\n\ndf.groupby('date').size()\\\n    .plot(title='DAU, historical')\n\n\n\n\n\n\n\n\nSuppose that today is 2022-10-31 and we want to predict the DAU metric for the next 2023 year. We define a couple of constants PREDICTION_START and PREDICTION_END which define the prediction period.\n\nPREDICTION_START = '2023-11-01'\nPREDICTION_END = '2024-12-31'\n\n\n\n3.2 Predicting new users amount\nLet’s start from the new users prediction. We use the prophet library as one of the easiest ways to predict time-series data. The new_users Series contains such data. We extract it from the original df dataset selecting the rows where the registration date is equal to the date.\n\n\nToggle the code\nnew_users = df[df['date'] == df['registration_date']]\\\n    .assign(date=pd.to_datetime(df['date']))\\\n    .groupby('date').size()\n\n\n\nnew_users.head()\n\ndate\n2020-10-01    4\n2020-10-02    4\n2020-10-03    3\n2020-10-04    4\n2020-10-05    8\ndtype: int64\n\n\nprophet requires a time-series as a DataFrame containing two columns ds and y, so we reformat the new_users Series to the new_users_prophet DataFrame. Another thing we need to prepare is to create the future variable containing certain days for prediction: from PREDICTION_START to PREDICTION_END. The plot illustrates predictions for both past and future dates.\n\n\nToggle the code\nimport logging\nimport matplotlib.pyplot as plt\nfrom prophet import Prophet\n\n# suppress prophet logs\nlogging.getLogger(\"prophet\").setLevel(logging.WARNING)\nlogging.getLogger(\"cmdstanpy\").disabled=True\n\ndef predict_new_users(prediction_start, prediction_end, new_users_train, show_plot=True):\n    m = Prophet()\n    new_users_train = new_users_train.loc[new_users_train.index &lt; prediction_start]\n    new_users_prophet = pd.DataFrame({'ds': new_users_train.index, 'y': new_users_train.values})\n    m.fit(new_users_prophet)\n\n    periods = len(pd.date_range(prediction_start, prediction_end))\n    future = m.make_future_dataframe(periods=periods)\n    new_users_pred = m.predict(future)\n    if show_plot:\n        m.plot(new_users_pred)\n        plt.title('New users prediction');\n\n    new_users_pred = new_users_pred\\\n        .assign(yhat=lambda _df: _df['yhat'].astype(int))\\\n        .rename(columns={'ds': 'date', 'yhat': 'count'})\\\n        .set_index('date')\\\n        .clip(lower=0)\\\n        ['count']\n\n    return new_users_pred\n\n\n/Users/v.kukushkin/Documents/private/wowone.github.io/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nnew_users_pred = predict_new_users(PREDICTION_START, PREDICTION_END, new_users)\n\n\n\n\n\n\n\n\nThe new_users_pred Series keeps the predicted users amount.\n\nnew_users_pred.tail(5)\n\ndate\n2024-12-27    52\n2024-12-28    56\n2024-12-29    71\n2024-12-30    79\n2024-12-31    74\nName: count, dtype: int64\n\n\n\n\n3.3 Getting the states\nIn practice, the most calculations are reasonable to execute as SQL queries to a database where the data is stored. Hereafter, we will simulate such quering with the duckdb library.\nWe want to assign one of the 7 states to each day of a user’s lifetime within the app. According to the definition, for each day, we need to consider at least the past 30 days. This is where SQL window functions come in. However, since the df data contains only records of active days, we need to explicitly extend it to include the days when a user was not active. In other words, instead of this list of records:\nuser_id    date          registration_date\n1234567    2023-01-01    2023-01-01\n1234567    2023-01-03    2023-01-01\nwe’d like to get a list like this:\nuser_id    date          is_active    registration_date\n1234567    2023-01-01    TRUE         2023-01-01\n1234567    2023-01-02    FALSE        2023-01-01\n1234567    2023-01-03    TRUE         2023-01-01\n1234567    2023-01-04    FALSE        2023-01-01\n1234567    2023-01-05    FALSE        2023-01-01\n...        ...           ...          ...\n1234567    2023-10-31    FALSE        2023-01-01\nFor readability purposes we split the following SQL query into multiple subqueries.\n\nfull_range: Create a full sequence of dates for each user.\ndau_full: Get the full list of both active and inactive records.\nstates: Assign one of the 7 states for each day of a user’s lifetime.\n\n\n\nToggle the code\nimport duckdb\n\nDATASET_START = '2020-11-01'\nDATASET_END = '2023-10-31'\nOBSERVATION_START = '2020-10-01'\n\nquery = f\"\"\"\nWITH\nfull_range AS (\n    SELECT\n        user_id, UNNEST(generate_series(greatest(registration_date, '{OBSERVATION_START}'), date '{DATASET_END}', INTERVAL 1 DAY))::date AS date\n    FROM (\n        SELECT DISTINCT user_id, registration_date FROM df\n    )\n),\ndau_full AS (\n    SELECT\n        fr.user_id,\n        fr.date,\n        df.date IS NOT NULL AS is_active,\n        registration_date\n    FROM full_range AS fr\n    LEFT JOIN df USING(user_id, date)\n),\nstates AS (\n    SELECT\n        user_id,\n        date,\n        is_active,\n        first_value(registration_date IGNORE NULLS) OVER (PARTITION BY user_id ORDER BY date) AS registration_date,\n        SUM(is_active::int) OVER (PARTITION BY user_id ORDER BY date ROWS BETWEEN 6 PRECEDING and 1 PRECEDING) AS active_days_back_6d,\n        SUM(is_active::int) OVER (PARTITION BY user_id ORDER BY date ROWS BETWEEN 29 PRECEDING and 1 PRECEDING) AS active_days_back_29d,\n        CASE\n            WHEN date = registration_date THEN 'new'\n            WHEN is_active = TRUE AND active_days_back_6d BETWEEN 1 and 6 THEN 'current'\n            WHEN is_active = TRUE AND active_days_back_6d = 0 AND IFNULL(active_days_back_29d, 0) &gt; 0 THEN 'reactivated'\n            WHEN is_active = TRUE AND active_days_back_6d = 0 AND IFNULL(active_days_back_29d, 0) = 0 THEN 'resurrected'\n            WHEN is_active = FALSE AND active_days_back_6d &gt; 0 THEN 'at_risk_wau'\n            WHEN is_active = FALSE AND active_days_back_6d = 0 AND ifnull(active_days_back_29d, 0) &gt; 0 THEN 'at_risk_mau'\n            ELSE 'dormant'\n        END AS state\n    FROM dau_full\n)\nSELECT user_id, date, state FROM states\nWHERE date BETWEEN '{DATASET_START}' AND '{DATASET_END}'\nORDER BY user_id, date\n\"\"\"\nstates = duckdb.sql(query).df()\n\n\nThe query results are kept in the states DataFrame:\n\nstates.head()\n\n\n\n\n\n\n\n\nuser_id\ndate\nstate\n\n\n\n\n0\n00002b68-adba-5a55-92d7-8ea8934c6db3\n2023-06-23\nnew\n\n\n1\n00002b68-adba-5a55-92d7-8ea8934c6db3\n2023-06-24\ncurrent\n\n\n2\n00002b68-adba-5a55-92d7-8ea8934c6db3\n2023-06-25\ncurrent\n\n\n3\n00002b68-adba-5a55-92d7-8ea8934c6db3\n2023-06-26\ncurrent\n\n\n4\n00002b68-adba-5a55-92d7-8ea8934c6db3\n2023-06-27\ncurrent\n\n\n\n\n\n\n\n\n\n3.4 Calculating the transition matrix\nHaving obtained these states, we can calculate state transition frequencies. In the real world, due to the large amount of data, it would be more effective to use a SQL query rather than a Python script. We calculate these frequencies day-wise since we’re going to study how the prediction depends on the period in which transitions are considered further.\n\n\nToggle the code\nquery = f\"\"\"\nSELECT\n    date,\n    state_from,\n    state_to,\n    COUNT(*) AS cnt,\nFROM (\n    SELECT\n        date,\n        state AS state_to,\n        lag(state) OVER (PARTITION BY user_id ORDER BY date) AS state_from\n    FROM states\n)\nWHERE state_from IS NOT NULL\nGROUP BY date, state_from, state_to\nORDER BY date, state_from, state_to;\n\"\"\"\ntransitions = duckdb.sql(query).df()\n\n\nThe result is stored in the transitions DataFrame.\n\ntransitions.head()\n\n\n\n\n\n\n\n\ndate\nstate_from\nstate_to\ncnt\n\n\n\n\n0\n2020-11-02\nat_risk_mau\nat_risk_mau\n273\n\n\n1\n2020-11-02\nat_risk_mau\ndormant\n4\n\n\n2\n2020-11-02\nat_risk_mau\nreactivated\n14\n\n\n3\n2020-11-02\nat_risk_wau\nat_risk_mau\n18\n\n\n4\n2020-11-02\nat_risk_wau\nat_risk_wau\n138\n\n\n\n\n\n\n\nNow, we can calculate the transition matrix \\(M\\). We define the get_transition_matrix function, which accepts the transitions DataFrame and a pair of dates that bounds the transitions to be considered.\n\n\nToggle the code\nstates_order = ['new', 'current', 'reactivated', 'resurrected', 'at_risk_wau', 'at_risk_mau', 'dormant']\n\ndef get_transition_matrix(transitions, date1, date2):\n    if pd.to_datetime(date1) &gt; pd.to_datetime(DATASET_END):\n        date1 = pd.to_datetime(DATASET_END) - pd.Timedelta(days=30)\n\n    probs = transitions\\\n        .loc[lambda _df: _df['date'].between(date1, date2)]\\\n        .groupby(['state_from', 'state_to'], as_index=False)\\\n        ['cnt'].sum()\\\n        .assign(\n            supp=lambda _df: _df.groupby('state_from')['cnt'].transform('sum'),\n            prob=lambda _df: _df['cnt'] / _df['supp']\n        )\n\n    M = probs.pivot(index='state_from', columns='state_to', values='prob')\\\n        .reindex(states_order, axis=0)\\\n        .reindex(states_order, axis=1)\\\n        .fillna(0)\\\n        .astype(float)\n\n    return M\n\n\nAs a baseline, let’s calculate the transition matrix for the whole year from 2021-11-01 to 2022-10-31.\n\nM = get_transition_matrix(transitions, '2022-11-01', '2023-10-31')\nM\n\n\n\n\n\n\n\nstate_to\nnew\ncurrent\nreactivated\nresurrected\nat_risk_wau\nat_risk_mau\ndormant\n\n\nstate_from\n\n\n\n\n\n\n\n\n\n\n\nnew\n0.0\n0.515934\n0.000000\n0.000000\n0.484066\n0.000000\n0.000000\n\n\ncurrent\n0.0\n0.851325\n0.000000\n0.000000\n0.148675\n0.000000\n0.000000\n\n\nreactivated\n0.0\n0.365867\n0.000000\n0.000000\n0.634133\n0.000000\n0.000000\n\n\nresurrected\n0.0\n0.316474\n0.000000\n0.000000\n0.683526\n0.000000\n0.000000\n\n\nat_risk_wau\n0.0\n0.098246\n0.004472\n0.000000\n0.766263\n0.131020\n0.000000\n\n\nat_risk_mau\n0.0\n0.000000\n0.009598\n0.000173\n0.000000\n0.950109\n0.040120\n\n\ndormant\n0.0\n0.000000\n0.000000\n0.000387\n0.000000\n0.000000\n0.999613\n\n\n\n\n\n\n\n\n\n3.5 Getting the initial state counts\nAn initial state is easily retrieved from the states DataFrame by the get_state0 function and the corresponding SQL query. We assign the result to the state0 variable.\n\n\nToggle the code\ndef get_state0(date):\n    query = f\"\"\"\n    SELECT state, count(*) AS cnt\n    FROM states\n    WHERE date = '{date}'\n    GROUP BY state\n    \"\"\"\n\n    state0 = duckdb.sql(query).df()\n    state0 = state0.set_index('state').reindex(states_order)['cnt']\n    \n    return state0\n\n\n\nstate0 = get_state0(DATASET_END)\nstate0\n\nstate\nnew               20\ncurrent          475\nreactivated       15\nresurrected       19\nat_risk_wau      404\nat_risk_mau     1024\ndormant        49523\nName: cnt, dtype: int64\n\n\n\n\n3.6 Predicting DAU\nThe predict_dau function below accepts all the previous variables required for the DAU prediction and makes this prediction for a date range defined by the start_date and end_date arguments.\n\n\nToggle the code\ndef predict_dau(M, state0, start_date, end_date, new_users):\n    \"\"\"\n    Predicts DAU over a given date range.\n\n    Parameters\n    ----------\n    M : pandas.DataFrame\n        Transition matrix representing user state changes.\n    state0 : pandas.Series\n        counts of initial state of users.\n    start_date : str\n        Start date of the prediction period in 'YYYY-MM-DD' format.\n    end_date : str\n        End date of the prediction period in 'YYYY-MM-DD' format.\n    new_users : int or pandas.Series\n        The expected amount of new users for each day between `start_date` and `end_date`.\n        If a Series, it should have dates as the index.\n        If an int, the same number is used for each day.\n        \n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame containing the predicted DAU, WAU, and MAU for each day in the date range,\n        with columns for different user states and tot.\n    \"\"\"\n    \n    dates = pd.date_range(start_date, end_date)\n    dates.name = 'date'\n    dau_pred = []\n    new_dau = state0.copy()\n    for date in dates:\n        new_dau = (M.transpose() @ new_dau).astype(int)\n        if isinstance(new_users, int):\n            new_users_today = new_users\n        else:\n            new_users_today = new_users.astype(int).loc[date] \n        new_dau.loc['new'] = new_users_today\n        dau_pred.append(new_dau.tolist())\n\n    dau_pred = pd.DataFrame(dau_pred, index=dates, columns=states_order)\n    dau_pred['dau'] = dau_pred['new'] + dau_pred['current'] + dau_pred['reactivated'] + dau_pred['resurrected']\n    dau_pred['wau'] = dau_pred['dau'] + dau_pred['at_risk_wau']\n    dau_pred['mau'] = dau_pred['dau'] + dau_pred['at_risk_wau'] + dau_pred['at_risk_mau']\n    \n    return dau_pred\n\n\n\ndau_pred = predict_dau(M, state0, PREDICTION_START, PREDICTION_END, new_users_pred)\ndau_pred\n\n\n\n\n\n\n\n\nnew\ncurrent\nreactivated\nresurrected\nat_risk_wau\nat_risk_mau\ndormant\ndau\nwau\nmau\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-11-01\n29\n465\n11\n19\n412\n1025\n49544\n524\n936\n1961\n\n\n2023-11-02\n25\n461\n11\n19\n418\n1027\n49565\n516\n934\n1961\n\n\n2023-11-03\n21\n456\n11\n19\n420\n1030\n49587\n507\n927\n1957\n\n\n2023-11-04\n22\n450\n11\n19\n419\n1033\n49609\n502\n921\n1954\n\n\n2023-11-05\n34\n445\n11\n19\n418\n1036\n49631\n509\n927\n1963\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2024-12-27\n52\n505\n12\n24\n487\n1129\n61599\n593\n1080\n2209\n\n\n2024-12-28\n56\n516\n13\n24\n497\n1136\n61620\n609\n1106\n2242\n\n\n2024-12-29\n71\n529\n13\n24\n509\n1144\n61641\n637\n1146\n2290\n\n\n2024-12-30\n79\n549\n13\n24\n527\n1153\n61663\n665\n1192\n2345\n\n\n2024-12-31\n74\n572\n13\n24\n548\n1164\n61685\n683\n1231\n2395\n\n\n\n\n427 rows × 10 columns\n\n\n\nBesides the expected dau, wau, and mau columns, the output contains the number of users in each state for each prediction date.\nFinally, we calculate the ground-truth values of DAU, WAU, and MAU (along with the corresponding state decomposition), keep them in the dau_true DataFrame, and plot the predicted and true values altogether.\n\n\nToggle the code\nquery = f\"\"\"\nSELECT date, state, COUNT(*) AS cnt\nFROM states\nGROUP BY date, state\nORDER BY date, state;\n\"\"\"\n\ndau_true = duckdb.sql(query).df()\ndau_true['date'] = pd.to_datetime(dau_true['date'])\ndau_true = dau_true.pivot(index='date', columns='state', values='cnt')\ndau_true['dau'] = dau_true['new'] + dau_true['current'] + dau_true['reactivated'] + dau_true['resurrected']\ndau_true['wau'] = dau_true['dau'] + dau_true['at_risk_wau']\ndau_true['mau'] = dau_true['dau'] + dau_true['at_risk_wau'] + dau_true['at_risk_mau']\n\n\n\ndau_true.head()\n\n\n\n\n\n\n\nstate\nat_risk_mau\nat_risk_wau\ncurrent\ndormant\nnew\nreactivated\nresurrected\ndau\nwau\nmau\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020-11-01\n291.0\n207.0\n293.0\n840.0\n36.0\n14.0\n3.0\n346.0\n553.0\n844.0\n\n\n2020-11-02\n291.0\n208.0\n327.0\n836.0\n53.0\n14.0\n8.0\n402.0\n610.0\n901.0\n\n\n2020-11-03\n296.0\n205.0\n383.0\n840.0\n41.0\n10.0\n3.0\n437.0\n642.0\n938.0\n\n\n2020-11-04\n296.0\n246.0\n375.0\n842.0\n27.0\n13.0\n6.0\n421.0\n667.0\n963.0\n\n\n2020-11-05\n300.0\n275.0\n373.0\n845.0\n33.0\n8.0\n4.0\n418.0\n693.0\n993.0\n\n\n\n\n\n\n\n\n\nToggle the code\npd.concat([dau_true['dau'], dau_pred['dau']])\\\n    .plot(title='DAU, historical & predicted');"
  },
  {
    "objectID": "posts/2024-11-01_dau_prediction/dau_prediction.html#prediction-testing",
    "href": "posts/2024-11-01_dau_prediction/dau_prediction.html#prediction-testing",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "Prediction testing",
    "text": "Prediction testing\n\nstate0 = get_state0('2022-06-30')\nM = get_transition_matrix(transitions, '2022-06-23', '2022-06-30')\ndau_pred = predict_dau(M, state0, '2022-07-01', '2022-10-31', new_users)\n\ndau_true\\\n    .join(dau_pred[['dau']], how='inner', lsuffix='_true', rsuffix='_pred')\\\n    .plot()"
  },
  {
    "objectID": "posts/2024-11-01_dau_prediction/dau_prediction.html#discussion",
    "href": "posts/2024-11-01_dau_prediction/dau_prediction.html#discussion",
    "title": "Modeling DAU with Markov chain (WIP)",
    "section": "5. Discussion",
    "text": "5. Discussion\nIn the section 4 we studied the model performance from the prediction accuracy perspective. Now let’s discuss the model from the practical point of view. We compare it with the baseline model (see the section 4.1).\nBesides poor accuracy, predicting DAU as a time-series makes this approach very stiff. The only thing we can control here is the historical data. In practice, when making plans for the next year we have some certain expectations about the future. For example,\n\nthe marketing team is going to launch some new more effective campaings,\nthe activation team is planning to improve the onboarding process,\nthe product team will release some new features that would engage and retain users more.\n\nOur model can take into account such expectations. For the examples above we can adjust the new users prediction, the new→current and the current→current conversion rates respectively. As a result we can get a prediction that is doesn’t match with the past data but would be more realistic. This model’s property is not just flexible – it’s interpretable. You can easily discuss all these adjustments with the stakeholders, and they can understand how the prediction works.\nAnother advantage of the model is that it doesn’t require to predict whether a user will be active on a certain day. Sometimes binary classifiers are used for this purpose. The downside of this approach is that we need to apply such a classifier to each user including all the dormant users and each day. This is a tremedous computational cost. Unlike this, the Markov model requires only the initial amount of states (state0). Moreover, such classiffiers are often black-box models: they are poorly interpretable and hard to adjust.\nThe Markov model also has some limitations. As we already have seen, it’s sensitive to the new users prediction. It’s easy to totally ruin the prediction by the wrong new users amount. Another problem is that the Markov model is memoryless meaning that it doesn’t take into account the user’s history. For example, it doesn’t distinguish whether a current user is a newbie, experienced, or just reactivated/resurrected. The retention rate of these user types should be certainly different. Also, as we discussed earlier the user bahavior might be of different nature depending on the season, marketing sources, countries, etc. So far our model is not able to capture these differences. However, this might be a subject for further research."
  },
  {
    "objectID": "posts/2024-11-01_dau_prediction/dau_prediction.html#conclusions",
    "href": "posts/2024-11-01_dau_prediction/dau_prediction.html#conclusions",
    "title": "Modeling DAU with Markov chain (WIP)",
    "section": "Conclusions",
    "text": "Conclusions"
  },
  {
    "objectID": "posts/2024-11-01_dau_prediction/dau_prediction.html#introdiction",
    "href": "posts/2024-11-01_dau_prediction/dau_prediction.html#introdiction",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "",
    "text": "Doubtlessly, DAU, WAU, and MAU are critical business metrics. An article “How Duolingo reignited user growth” by Jorge Mazal, former CPO of Duolingo is #1 the Growth section of Lenny’s Newsletter blog. In this article Jorge payed special attention to the methodology Duolingo used to model DAU metrics (see another article “Meaningful metrics: how data sharpened the focus of product teams” by Erin Gustafson). This methodology has multiple strenghts but I focus here on how one can use it for DAU forecasting.\nNew year is coming soon, so many companies are planning their budgets for the next year these days. Cost estimations are often require DAU forecast. In this article I’ll show you how you can get this prediction using the Duolingo’s growth model and share a DAU & MAU “calculator” designed as a Google Spreadsheet calculator."
  },
  {
    "objectID": "posts/2024-11-01_dau_prediction/dau_prediction.html#calculating-the-transition-matrix",
    "href": "posts/2024-11-01_dau_prediction/dau_prediction.html#calculating-the-transition-matrix",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "3.3 Calculating the transition matrix",
    "text": "3.3 Calculating the transition matrix\nNow, having these states obtained, we can calculate state transition frequencies. Again, due to large amount of data this part is better to execute with SQL on the database side, not with Python. For the reasons that will be explained further we calculate these frequencies day-wise. The result is stored in the transitions DataFrame.\n\n\nToggle the code\nquery = f\"\"\"\nSELECT\n    date,\n    state_from,\n    state_to,\n    COUNT(*) AS cnt,\nFROM (\n    SELECT\n        date,\n        state AS state_to,\n        lag(state) OVER (PARTITION BY user_id ORDER BY date) AS state_from\n    FROM states\n)\nWHERE state_from IS NOT NULL\nGROUP BY date, state_from, state_to\nORDER BY date, state_from, state_to;\n\"\"\"\ntransitions = duckdb.sql(query).df()\n\n\n\n\n\n\n\n\n\ndate\nstate_from\nstate_to\ncnt\n\n\n\n\n0\n2021-11-02\nat_risk_mau\nat_risk_mau\n1190\n\n\n1\n2021-11-02\nat_risk_mau\ndormant\n56\n\n\n2\n2021-11-02\nat_risk_mau\nreactivated\n16\n\n\n3\n2021-11-02\nat_risk_wau\nat_risk_mau\n53\n\n\n4\n2021-11-02\nat_risk_wau\nat_risk_wau\n409\n\n\n\n\n\n\n\n\ntransitions.head()\n\n\n\n\n\n\n\n\ndate\nstate_from\nstate_to\ncnt\n\n\n\n\n0\n2021-11-02\nat_risk_mau\nat_risk_mau\n1190\n\n\n1\n2021-11-02\nat_risk_mau\ndormant\n56\n\n\n2\n2021-11-02\nat_risk_mau\nreactivated\n16\n\n\n3\n2021-11-02\nat_risk_wau\nat_risk_mau\n53\n\n\n4\n2021-11-02\nat_risk_wau\nat_risk_wau\n409\n\n\n\n\n\n\n\n\nPREDICTION_START = '2022-11-01'\nPREDICTION_END = '2023-12-31'\n\nstates_order = ['new', 'current', 'reactivated', 'resurrected', 'at_risk_wau', 'at_risk_mau', 'dormant']\n\ndef get_transition_matrix(transitions, date1, date2):\n    probs = transitions\\\n        .loc[lambda _df: _df['date'].between(date1, date2)]\\\n        .groupby(['state_from', 'state_to'], as_index=False)\\\n        ['cnt'].sum()\\\n        .assign(\n            supp=lambda _df: _df.groupby('state_from')['cnt'].transform('sum'),\n            prob=lambda _df: _df['cnt'] / _df['supp']\n        )\n\n    M = probs.pivot(index='state_from', columns='state_to', values='prob')\\\n        .reindex(states_order, axis=0)\\\n        .reindex(states_order, axis=1)\\\n        .fillna(0)\\\n        .astype(float)\n\n    return M\n\n\nPredicting DAU\n\n\nCode\ndef predict_dau(M, state0, date1, date2, new_users):\n    \"\"\"\n    Predicts DAU over a given date range.\n\n    Parameters\n    ----------\n    M : pandas.DataFrame\n        Transition matrix representing user state changes.\n    state0 : pandas.Series\n        counts of initial state of users.\n    date1 : str\n        Start date of the prediction period in 'YYYY-MM-DD' format.\n    date2 : str\n        End date of the prediction period in 'YYYY-MM-DD' format.\n    new_users : int or pandas.Series\n        The expected amount of new users for each day between date1 and date2.\n        If a Series, it should have dates as the index.\n        If an int, the same number is used for each day.\n        \n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame containing the predicted DAU, WAU, and MAU for each day in the date range,\n        with columns for different user states and tot.\n    \"\"\"\n    \n    dates = pd.date_range(date1, date2)\n    dates.name = 'date'\n    dau_pred = []\n    new_dau = state0.copy()\n    for date in dates:\n        new_dau = (M.transpose() @ new_dau).astype(int)\n        if isinstance(new_users, int):\n            new_users_today = new_users\n        else:\n            new_users_today = new_users.astype(int).loc[date] \n        new_dau.loc['new'] = new_users_today\n        dau_pred.append(new_dau.tolist())\n\n    dau_pred = pd.DataFrame(dau_pred, index=dates, columns=states_order)\n    dau_pred['dau'] = dau_pred['new'] + dau_pred['current'] + dau_pred['reactivated'] + dau_pred['resurrected']\n    dau_pred['wau'] = dau_pred['dau'] + dau_pred['at_risk_wau']\n    dau_pred['mau'] = dau_pred['dau'] + dau_pred['at_risk_wau'] + dau_pred['at_risk_mau']\n    \n    return dau_pred\n\n\ndef get_state0(date):\n    query = f\"\"\"\n    SELECT state, count(*) AS cnt\n    FROM states\n    WHERE date = '{date}'\n    GROUP BY state\n    \"\"\"\n\n    state0 = duckdb.sql(query).df()\n    state0 = state0.set_index('state').reindex(states_order)['cnt']\n    \n    return state0\n\n\n\nnew_users_pred\n\ndate\n2022-11-01     49\n2022-11-02     47\n2022-11-03     43\n2022-11-04     39\n2022-11-05     41\n             ... \n2023-12-27    135\n2023-12-28    134\n2023-12-29    132\n2023-12-30    137\n2023-12-31    151\nName: count, Length: 426, dtype: int64\n\n\n\nM = get_transition_matrix(transitions, '2022-10-25', '2022-10-31')\nstate0 = get_state0(DATASET_END)\ndau_pred = predict_dau(M, state0, PREDICTION_START, PREDICTION_END, new_users_pred)\ndau_pred\n\n\n\n\n\n\n\n\nnew\ncurrent\nreactivated\nresurrected\nat_risk_wau\nat_risk_mau\ndormant\ndau\nwau\nmau\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022-11-01\n49\n444\n13\n13\n451\n1269\n35901\n519\n970\n2239\n\n\n2022-11-02\n47\n446\n13\n13\n462\n1267\n35936\n519\n981\n2248\n\n\n2022-11-03\n43\n448\n13\n13\n470\n1267\n35971\n517\n987\n2254\n\n\n2022-11-04\n39\n448\n13\n13\n474\n1268\n36006\n513\n987\n2255\n\n\n2022-11-05\n41\n447\n13\n13\n475\n1269\n36041\n514\n989\n2258\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-12-27\n135\n1083\n31\n24\n1160\n2972\n65468\n1273\n2433\n5405\n\n\n2023-12-28\n134\n1098\n31\n24\n1176\n2983\n65558\n1287\n2463\n5446\n\n\n2023-12-29\n132\n1112\n31\n24\n1190\n2995\n65649\n1299\n2489\n5484\n\n\n2023-12-30\n137\n1124\n31\n24\n1202\n3008\n65740\n1316\n2518\n5526\n\n\n2023-12-31\n151\n1138\n31\n24\n1216\n3022\n65832\n1344\n2560\n5582\n\n\n\n\n426 rows × 10 columns\n\n\n\n\nquery = \"\"\"\nSELECT date, COUNT(*) AS dau\nFROM states\nWHERE state IN ('new', 'current', 'reactivated', 'resurrected')\nGROUP BY date\nORDER BY date\n\"\"\"\n\ndau_true = duckdb.sql(query).df()\\\n    .set_index('date')\npd.concat([dau_true, dau_pred['dau']]).plot(title='DAU, historical & predicted');"
  },
  {
    "objectID": "posts/2024-11-01_dau_prediction/dau_prediction.html#model-evaluation",
    "href": "posts/2024-11-01_dau_prediction/dau_prediction.html#model-evaluation",
    "title": "Modeling DAU with Markov chain (WIP)",
    "section": "4. Model evaluation",
    "text": "4. Model evaluation\n\n4.1 Baseline comparison\nFirst of all, let’s check whether we really need to build a complex model to predict DAU. Wouldn’t it be better to predict DAU as a general time-series using the mentioned prophet library? The function predict_dau_simple implements this. We try to use some tweaks available in the library in order to make the prediction more accurate:\n\nwe use logistic model instead of linear in order to avoid negative values;\nwe add explicitly monthly and yearly seasonality;\nwe remove the outliers.\nwe set explicitly define peak values for January and February as “holidays”.\n\nThe fact that the code turns out to be quite sophisticated indicates that one can’t simply apply prophet to the DAU time-series.\n\n\nToggle the code\ndef predict_dau_prophet(prediction_start, prediction_end, dau_true, show_plot=True):\n    holidays = pd.DataFrame({\n        'holiday': 'january_spike',\n        'ds': pd.date_range('2022-01-01', '2022-01-31', freq='D').tolist() + \\\n              pd.date_range('2023-01-01', '2023-01-31', freq='D').tolist(),\n        'lower_window': 0,\n        'upper_window': 40\n    })\n\n    m = Prophet(growth='logistic', holidays=holidays)\n    m.add_seasonality(name='monthly', period=30.5, fourier_order=3)\n    m.add_seasonality(name='yearly', period=365, fourier_order=3)\n\n    train = dau_true.loc[(dau_true.index &lt; prediction_start) & (dau_true.index &gt;= '2021-08-01')]\n    train_prophet = pd.DataFrame({'ds': train.index, 'y': train.values})\n    train_prophet.loc[train_prophet['ds'].between('2022-06-07', '2022-06-09'), 'y'] = None\n    train_prophet['new_year_peak'] = (train_prophet['ds'] &gt;= '2022-01-01') &\\\n                                     (train_prophet['ds'] &lt;= '2022-02-14')\n    m.add_regressor('new_year_peak')\n    train_prophet['cap'] = dau_true.max() * 1.1\n    train_prophet['floor'] = 0\n\n    m.fit(train_prophet)\n\n    periods = len(pd.date_range(prediction_start, prediction_end))\n    future = m.make_future_dataframe(periods=periods)\n    future['new_year_peak'] = (future['ds'] &gt;= '2022-01-01') & (future['ds'] &lt;= '2022-02-14')\n    future['cap'] = dau_true.max() * 1.1\n    future['floor'] = 0\n    pred = m.predict(future)\n\n    if show_plot:\n        m.plot(pred);\n\n    pred = pred\\\n        .assign(yhat=lambda _df: _df['yhat'].astype(int))\\\n        .rename(columns={'ds': 'date', 'yhat': 'count'})\\\n        .set_index('date')\\\n        .clip(lower=0)\\\n        ['count']\\\n        .loc[lambda s: (s.index &gt;= prediction_start) & (s.index &lt;= prediction_end)]\n\n    return pred\n\n\nHereafter we test a prediction for multiple predicting horizonts: 3, 6, and 12 months. As a result, we get 3 test sets:\n\n3-months horizont: 2023-08-01 - 2023-10-31,\n6-months horizont: 2023-05-01 - 2023-10-31,\n1-year horizont: 2022-11-01 - 2023-10-31.\n\nFor each test set we calculate the MAPE loss function.\n\n\nToggle the code\nfrom sklearn.metrics import mean_absolute_percentage_error\n\nmapes = []\nprediction_end = '2023-10-31'\nprediction_horizont = [3, 6, 12]\n\nfor offset in prediction_horizont:\n    prediction_start = pd.to_datetime(prediction_end) - pd.DateOffset(months=offset - 1)\n    prediction_start = prediction_start.replace(day=1)\n    prediction_end = '2023-10-31'\n    pred = predict_dau_prophet(prediction_start, prediction_end, dau_true['dau'], show_plot=False)\n    mape = mean_absolute_percentage_error(dau_true['dau'].reindex(pred.index), pred)\n    mapes.append(mape)\n\nmapes = pd.DataFrame({'horizont': prediction_horizont, 'MAPE': mapes})\nmapes\n\n\n\n\n\n\n\n\n\nhorizont\nMAPE\n\n\n\n\n0\n3\n0.350167\n\n\n1\n6\n0.185246\n\n\n2\n12\n0.215338\n\n\n\n\n\n\n\nThe MAPE error turns out to be high: 18% - 35%. The fact that the shortest horizont has the highest error means that the model is tuned for the long-term predictions. This is another inconvenience of such an approach: we have to tune the model for each prediction horizont. So this is our baseline. In the next section we’ll compare it with the more advanced models.\n\n\n4.2 General evaluation\nIn this section we evaluate the model implemented in the section 3. So far we set the transition period as 1 year before the prediction start. We’ll study how the prediction depends on the transition period in the next section. As for the new users, we run the model using two options: the real values and the predicted ones. Similarly, we fix the same 3 prediction horizonts and test the model on them.\nThe make_predicion function below implements the described opttions. It accepts prediction_start, prediction_end arguments defining the prediction period for a given horizont, new_users_mode which can be either true or predict, and transition_period.\n\n\nToggle the code\nimport re\n\n\ndef make_prediction(prediction_start, prediction_end, new_users_mode='predict', transition_period='last_30d'):\n    prediction_start_minus_1d = pd.to_datetime(prediction_start) - pd.Timedelta('1d')\n    state0 = get_state0(prediction_start_minus_1d)\n    \n    if new_users_mode == 'predict':\n        new_users_pred = predict_new_users(prediction_start, prediction_end, new_users, show_plot=False)\n    elif new_users_mode == 'true':\n        new_users_pred = new_users.copy()\n\n    if transition_period.startswith('last_'):\n        shift = int(re.search(r'last_(\\d+)d', transition_period).group(1))\n        transitions_start = pd.to_datetime(prediction_start) - pd.Timedelta(shift, 'd')\n        M = get_transition_matrix(transitions, transitions_start, prediction_start_minus_1d)\n        dau_pred = predict_dau(M, state0, prediction_start, prediction_end, new_users_pred)\n    else:\n        transitions_start = pd.to_datetime(prediction_start) - pd.Timedelta(240, 'd')\n        M_base = get_transition_matrix(transitions, transitions_start, prediction_start_minus_1d)\n        dau_pred = pd.DataFrame()\n\n        month_starts = pd.date_range(prediction_start, prediction_end, freq='1MS')\n        N = len(month_starts)\n\n        for i, prediction_month_start in enumerate(month_starts):\n            prediction_month_end = pd.offsets.MonthEnd().rollforward(prediction_month_start)\n            transitions_month_start = prediction_month_start - pd.Timedelta('365D')\n            transitions_month_end = prediction_month_end - pd.Timedelta('365D')\n\n            M_seasonal = get_transition_matrix(transitions, transitions_month_start, transitions_month_end)\n            if transition_period == 'smoothing':\n                i = min(i, 12)\n                M = M_seasonal * i / (N - 1)  + (1 - i / (N - 1)) * M_base\n            elif transition_period.startswith('seasonal_'):\n                seasonal_coef = float(re.search(r'seasonal_(0\\.\\d+)', transition_period).group(1))\n                M = seasonal_coef * M_seasonal + (1 - seasonal_coef) * M_base\n            \n            dau_tmp = predict_dau(M, state0, prediction_month_start, prediction_month_end, new_users_pred)\n            dau_pred = pd.concat([dau_pred, dau_tmp])\n\n            state0 = dau_tmp.loc[prediction_month_end][states_order]\n\n    return dau_pred\n\ndef prediction_details(dau_true, dau_pred, show_plot=True, ax=None):\n    y_true = dau_true.reindex(dau_pred.index)['dau']\n    y_pred = dau_pred['dau']\n    mape = mean_absolute_percentage_error(y_true, y_pred) \n\n    if show_plot:\n        prediction_start = str(y_true.index.min().date())\n        prediction_end = str(y_true.index.max().date())\n        if ax is None:\n            y_true.plot(label='DAU true')\n            y_pred.plot(label='DAU pred')\n            plt.title(f'DAU prediction, {prediction_start} - {prediction_end}')\n            plt.legend()\n        else:\n            y_true.plot(label='DAU true', ax=ax)\n            y_pred.plot(label='DAU pred', ax=ax)\n            ax.set_title(f'DAU prediction, {prediction_start} - {prediction_end}')\n            ax.legend()\n    return mape\n\n\nThe charts on the left relate to the new_users_mode = 'true' option, while the right ones relate to the new_users_mode = 'predict' option.\n\n\nToggle the code\nfig, axs = plt.subplots(3, 2, figsize=(15, 6))\nmapes = []\nprediction_end = '2023-10-31'\nprediction_horizont = [3, 6, 12]\n\nfor i, offset in enumerate(prediction_horizont):\n    prediction_start = pd.to_datetime(prediction_end) - pd.DateOffset(months=offset - 1)\n    prediction_start = prediction_start.replace(day=1)\n    args = {\n        'prediction_start': prediction_start,\n        'prediction_end': prediction_end,\n        'transition_period': 'last_365d'\n    }\n    for j, new_users_mode in enumerate(['predict', 'true']):\n        args['new_users_mode'] = new_users_mode\n        dau_pred = make_prediction(**args)\n        mape = prediction_details(dau_true, dau_pred, ax=axs[i, j])\n        mapes.append([offset, new_users_mode, mape])\n\nmapes = pd.DataFrame(mapes, columns=['horizont', 'new_users', 'MAPE'])\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nmapes.pivot(index='horizont', columns='new_users', values='MAPE')\n\n\n\n\n\n\n\nnew_users\npredict\ntrue\n\n\nhorizont\n\n\n\n\n\n\n3\n0.079538\n0.103631\n\n\n6\n0.125035\n0.084652\n\n\n12\n0.651502\n0.086885\n\n\n\n\n\n\n\nWe notice multiple things.\n\nIn general, the model demonstrates much better results than the baseline. Indeed, the baseline is based on the historical DAU data only, while the model uses the user states information.\nHowever, for the 1-year horizont and new_users_mode='predict' the MAPE error is huge: 65%. This is 3 times higher than the corresponding baseline error (21%). On the other hand, new_users_mode='true' option gives a much better result: 8%. It means that the new users prediction has a huge impact on the model, especially for long-term predictions. For the shorter periods the difference is less dramatic. The major difference for such a difference is that 1-year period includes Christmas with its extreme values. As a result, i) it’s hard to predict such high new user values, ii) the period heavily impacts user behavior, the transition matrix and, consequently, DAU values. Hence, we strongly recommend to implement the new user prediction carefully. The baseline model was specially tuned for this Christmas period, so it’s not surprising that it outperforms the Markov model.\nWhen the new users prediction is accurate, the model captures trends well. It means that using last 365 days for the transition matrix calculation is a reasonable choice.\nInterestingly, the true new users data provides worse results for the 3-months prediction. This is nothing but a coincidence. The wrong new users prediction in October 2023 reversed the predicted DAU trend and made MAPE a bit lower.\n\nNow, let’s decompose the prediction error and see which states contribure the most. Since we already know that the new users prediction has a huge impact on the model, we focus on a shorter prediction period in order to study the other factors.\n\n\nToggle the code\ndau_component_cols = ['new', 'current', 'reactivated', 'resurrected']\n\ndau_pred = make_prediction('2023-08-01', '2023-10-31', new_users_mode='predict', transition_period='last_365d')\nfigure, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\ndau_pred[dau_component_cols]\\\n    .subtract(dau_true[dau_component_cols])\\\n    .reindex(dau_pred.index)\\\n    .plot(title='Prediction error by state', ax=ax1)\n\ndau_pred[['current']]\\\n    .subtract(dau_true[['current']])\\\n    .div(dau_true[['current']])\\\n    .reindex(dau_pred.index)\\\n    .plot(title='Relative prediction error (current state)', ax=ax2);\n\n\n\n\n\n\n\n\n\nFrom the left chart we notice that the error is basically contributed by the current state. It’s not surprising since this state contributes to DAU the most. The error for the new, reactivated, and resurrected states is quite low. Another interesting thing is that this error is mostly negative. Perhaps it mean that the new users who appeared in the prediction period are more engaged that the users from the past.\nAs for the relative error on the right chart, it makes sense to analyze it for the current state only. This is because the daily amount of the reactivated and resurrected states are low so the relative error is high and noisy. The relative error for the current state varies between -20% and 5% which is quite high. And since the current state amount is mostly regulated by the current -&gt; current conversion rate (it’s roughly 0.8), this error is explained by the transition matrix inaccuracy.\n\n\n4.3 Transitions period impact\nIn the previous section we kept the transitions period fixed – 1 year before a prediction start. Now we’re going to study how long this period should be to get more accurate prediction. We consider the same prediction horizonts of 3, 6, and 12 months. In order to mitigate the noise from the new users prediction, we use the real values of the new users amount.\n\n\nToggle the code\nresult = []\n\nfor prediction_offset in prediction_horizont:\n    prediction_start = pd.to_datetime(prediction_end) - pd.DateOffset(months=prediction_offset - 1)\n    prediction_start = prediction_start.replace(day=1)\n\n    for transition_offset in range(1, 13):\n        dau_pred = make_prediction(\n            prediction_start, prediction_end, new_users_mode='true',\n            transition_period=f'last_{transition_offset*30}d'\n        )\n        mape = prediction_details(dau_true, dau_pred, show_plot=False)\n        result.append([prediction_offset, transition_offset, mape])\nresult = pd.DataFrame(result, columns=['prediction_period', 'transition_period', 'mape'])\n\nresult.pivot(index='transition_period', columns='prediction_period', values='mape')\\\n    .plot(title='MAPE by prediction and transition period');\n\n\n\n\n\n\n\n\n\nIt turns out that the optimal transitions period depends on the prediction horizont. Shorter horizonts require shorter transitions periods: the minimal MAPE error is achieved at 1, 4, and 8 transition periods for the 3, 6, and 12 months correspondingly. It seems this is because the longer horizonts contain some seasonal effects that could be captured only by the longer transitions periods.\n\n\n4.4 Obsolence and seasonality\nNevertheless, fixing a single transition matrix for predicting the whole year ahead doesn’t seem to be a good idea: such a model would be too rigid. Usually, the user behavior varies depending on a season. For example, users who appear after Christmas might have some shifts in behavior. Another typical situation is when users change their behavior in summer. In this section, we’ll try to take into account these seasonal effects.\nSo we want to predict DAU for the 1 year ahead starting from November 2022. Instead of using a single transition matrix \\(M_{base}\\) which is calculated for the last 8 months before the prediction start (as an optimal transitions period derived from the previous section, labeled as last_240d option), we’ll consider a mixture of this matrix and a seasonal one \\(M_{seasonal}\\). The latter is calculated on monthly basis lagging 1 year behind. For example, to predict DAU for November 2022 we define \\(M_{seasonal}\\) as the transition matrix for November 2021. Then we shift the prediction horizon to December 2022 and calculate \\(M_{seasonal}\\) for December 2021, etc.\nIn order to mix \\(M_{base}\\) and \\(M_{seasonal}\\) we define the following two options.\n\nseasonal_0.3: \\(M = 0.3 \\cdot M_{seasonal} + 0.7 \\cdot M_{base}\\). 0.3 weight was chosen as a local minimum after some experiments.\nsmoothing: \\(M = \\frac{i}{N - 1} M_{seasonal} + (1 - \\frac{i}{N - 1}) M_{base}\\) where \\(N\\) is the number of months within the predicting period, \\(i = 0, \\ldots, N - 1\\) – the month index. The idea of this configuration is to gradually switch from the most recent transition matrix \\(M_{base}\\) to seasonal ones as the prediction month moves forward from the prediction start.\n\n\n\nToggle the code\nresult = pd.DataFrame()\nfor transition_period in ['last_240d', 'seasonal_0.3', 'smoothing']:\n    result[transition_period] = make_prediction('2022-11-01', '2023-10-31', 'true', transition_period)['dau']\nresult['true'] = dau_true['dau']\nresult['true'] = result['true'].astype(int)\nresult.plot(title='DAU prediction by different transition matrices');\n\n\n\n\n\n\n\n\n\n\n\nToggle the code\nmape = pd.DataFrame()\nfor col in result.columns:\n    if col != 'true':\n        mape.loc[col, 'mape'] = mean_absolute_percentage_error(result['true'], result[col])\nmape\n\n\n\n\n\n\n\n\n\nmape\n\n\n\n\nlast_240d\n0.080804\n\n\nseasonal_0.3\n0.077545\n\n\nsmoothing\n0.097802\n\n\n\n\n\n\n\nAccording to the MAPE errors, seasonal_0.3 configuration provides the best results. Interestingly, smoothing approach has appeared to be even worse than the last_240d. From the diagram above we see that all three models start to underestimate the DAU values from July 2023, especially the smoothing model. It seems that the new users who started appearing from July 2023 are more engaged than the users from 2022. Probably, the app was improved sufficiently or the marketing team did a great job. As a result, the smoothing model that much relies on the outdated transitions data from July 2022 - October 2022 fails more than the other models.\n\n\n4.5 Final solution\nTo sum things up, let’s make a final prediction for the 2024 year. We use the seasonal_0.3 configuration and the predicted values for new users.\n\n\nToggle the code\ndau_pred = make_prediction(PREDICTION_START, PREDICTION_END, new_users_mode='predict', transition_period='seasonal_0.3')\ndau_true['dau'].plot(label='true')\ndau_pred['dau'].plot(label='seasonal_0.3')\nplt.title('DAU, historical & predicted')\nplt.legend();"
  },
  {
    "objectID": "posts/2024-11-01_dau_prediction/dau_prediction.html#introduction",
    "href": "posts/2024-11-01_dau_prediction/dau_prediction.html#introduction",
    "title": "Modeling DAU with Markov chain (WIP)",
    "section": "",
    "text": "Doubtlessly, DAU, WAU, and MAU are critical business metrics. An article “How Duolingo reignited user growth” by Jorge Mazal, former CPO of Duolingo is #1 the Growth section of Lenny’s Newsletter blog. In this article Jorge payed special attention to the methodology Duolingo used to model DAU metrics (see another article “Meaningful metrics: how data sharpened the focus of product teams” by Erin Gustafson). This methodology has multiple strenghts but I focus here on how one can use it for DAU forecasting.\nNew year is coming soon, so many companies are planning their budgets for the next year these days. Cost estimations are often require DAU forecast. In this article I’ll show you how you can get this prediction using the Duolingo’s growth model and share a DAU & MAU “calculator” designed as a Google Spreadsheet calculator."
  },
  {
    "objectID": "posts/2024-11-02_dau_prediction/dau_prediction.html",
    "href": "posts/2024-11-02_dau_prediction/dau_prediction.html",
    "title": "Modeling DAU with Markov chain (WIP)",
    "section": "",
    "text": "Doubtlessly, DAU, WAU, and MAU are critical business metrics. An article “How Duolingo reignited user growth” by Jorge Mazal, former CPO of Duolingo is #1 in the Growth section of Lenny’s Newsletter blog. In this article Jorge payed special attention to the methodology Duolingo used to model DAU metrics (see another article “Meaningful metrics: how data sharpened the focus of product teams” by Erin Gustafson). This methodology has multiple strenghts but I’d like to focus on how one can use this approach for DAU forecasting.\nNew year is coming soon, so many companies are planning their budgets for the next year these days. Cost estimations are often require DAU forecast. In this article I’ll show how you can get this prediction using the Duolingo’s growth model. I’ll explain why this approach is better compared to standard time-series forecasting methods and how you can adjust the prediction according to your teams’ plans (e.g. marketing, activation, product teams).\nThe article text goes along with the code and a simulated dataset is attached so the research is fully reproducible. The Jupyter notebook version is available here.\nFor those who read this article to the end I’ll share a DAU “calculator” designed in Google Spreadsheet format."
  },
  {
    "objectID": "posts/2024-11-02_dau_prediction/dau_prediction.html#introduction",
    "href": "posts/2024-11-02_dau_prediction/dau_prediction.html#introduction",
    "title": "Modeling DAU with Markov chain (WIP)",
    "section": "",
    "text": "Doubtlessly, DAU, WAU, and MAU are critical business metrics. An article “How Duolingo reignited user growth” by Jorge Mazal, former CPO of Duolingo is #1 in the Growth section of Lenny’s Newsletter blog. In this article Jorge payed special attention to the methodology Duolingo used to model DAU metrics (see another article “Meaningful metrics: how data sharpened the focus of product teams” by Erin Gustafson). This methodology has multiple strenghts but I’d like to focus on how one can use this approach for DAU forecasting.\nNew year is coming soon, so many companies are planning their budgets for the next year these days. Cost estimations are often require DAU forecast. In this article I’ll show how you can get this prediction using the Duolingo’s growth model. I’ll explain why this approach is better compared to standard time-series forecasting methods and how you can adjust the prediction according to your teams’ plans (e.g. marketing, activation, product teams).\nThe article text goes along with the code and a simulated dataset is attached so the research is fully reproducible. The Jupyter notebook version is available here.\nFor those who read this article to the end I’ll share a DAU “calculator” designed in Google Spreadsheet format."
  },
  {
    "objectID": "posts/2024-11-02_dau_prediction/dau_prediction.html#methodology",
    "href": "posts/2024-11-02_dau_prediction/dau_prediction.html#methodology",
    "title": "Modeling DAU with Markov chain (WIP)",
    "section": "2. Methodology",
    "text": "2. Methodology\nA quick recap on how the Duolingo’s growth model works. At day \\(d\\) (\\(d=1,2,\\ldots,\\)) of a user’s lifetime the user can be in one of the following 7 (mutually-exclusive) states:\n\n\n\n\nstate\n\n\nd = 1\n\n\nactivetoday\n\n\nactive in[d-6, d-1]\n\n\nactive in[d-29, d-7]\n\n\nactivebefore d-30\n\n\n\n\n\nnew\n\n\n✅\n\n\n?\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\ncurrent\n\n\n❌\n\n\n✅\n\n\n✅\n\n\n❓\n\n\n❓\n\n\n\n\nreactivated\n\n\n❌\n\n\n✅\n\n\n❌\n\n\n✅\n\n\n?\n\n\n\n\nresurrected\n\n\n❌\n\n\n✅\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n\n\nat_risk_wau\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n?\n\n\n?\n\n\n\n\nat_risk_mau\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n?\n\n\n\n\ndormant\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n\nHaving these states defined (as set \\(S\\)), we can consider a user’s lifetime trajectory as a Markov chain. Let \\(M\\) be a transition matrix associated with this Markov chain: \\(m_{i, j} = P(s_j | s_i)\\) are the probabilities that a user moves to state \\(s_j\\) right after being at state \\(s_i\\), \\(s_i, s_j \\in S\\). The matrix values are easily fetched from the historical data.\nIf we assume that the user behavior is stationary, the matrix \\(M\\) fully describes states of the all users in the future. Suppose that vector \\(u_0\\) of length 7 contains the counts of users being in certain states at some calendar day denoted as 0. Thus, according to the Markov model, in the next day \\(u_1\\) we expect to have the following amount of users:\n\\[\n\\underbrace{\n\\begin{pmatrix}  \\#New_1 \\\\ \\#Current_1 \\\\ \\#Reactivated_1 \\\\ \\#Resurrected_1 \\\\ \\#AtRiskWau_1 \\\\ \\#AtRiskMau_1 \\\\ \\#Dormant_1 \\end{pmatrix}\n}_{u_1} = M^T \\cdot\n\\underbrace{\n\\begin{pmatrix}  \\#New_0 \\\\ \\#Current_0 \\\\ \\#Reactivated_0 \\\\ \\#Resurrected_0 \\\\ \\#AtRiskWau_0 \\\\ \\#AtRiskMau_0 \\\\ \\#Dormant_0 \\end{pmatrix}\n}_{u_0}\n\\]\nApplying this formula recursevely, we derive the amount of the users at any arbitrary day \\(t &gt; 0\\) in the future. The only thing we need to provide despite of the initial distribution \\(u_0\\) is to the amount of new users that would appear in the product each day in the future. We’ll get it by using historical data on new users appeared in the past and appyling the prophet library.\nNow, having \\(u_t\\) calculated, we can calculate DAU values at day t: \\[DAU_t = \\#New_t + \\#Current_t + \\#Reactivated_t +\\#Resurrected_t.\\]\nAdditionally, we can easily calculate WAU and MAU metrics: \\[WAU_t = DAU_t +\\#AtRiskWau_t,\\] \\[MAU_t = DAU_t +\\#AtRiskWau_t + \\#AtRiskMau_t.\\]\nFinally, the algorithm looks like this:\n\nFor each prediction day \\(t=1, ..., T\\) calculate the expected amount of new users \\(\\#New_1, \\ldots, \\#New_T\\).\nFor each lifetime day of each user define on of the 7 states.\nCalculate the transition matrix \\(M\\).\nCalculate initial counts \\(u_0\\) corresponding to \\(t=0\\) day.\nCalculate recursively \\(u_{t+1} = M^T u_t\\).\nCalculate DAU, WAU, MAU for each prediction day \\(t=1, ..., T\\)."
  },
  {
    "objectID": "posts/2024-11-02_dau_prediction/dau_prediction.html#implementation",
    "href": "posts/2024-11-02_dau_prediction/dau_prediction.html#implementation",
    "title": "Modeling DAU with Markov chain (WIP)",
    "section": "3. Implementation",
    "text": "3. Implementation\n\n3.1 Dataset\nWe use a simulated dataset based on historical data of a SAAS app. The data is stored in the dau_data.csv.gz file and contains three columns: user_id, date, and registration_date. Each record indicates a day when a user was active.\nThe data includes activity indicators for all users from 2020-11-01 to 2023-10-31. An additional month, October 2020, is included to calculate user states correctly (at_risk_mau and dormant states require data from one month prior).\n\nimport pandas as pd\n\ndf = pd.read_csv('dau_data.csv.gz', compression='gzip')\ndf['date'] = pd.to_datetime(df['date'])\ndf['registration_date'] = pd.to_datetime(df['registration_date'])\n\nprint(f'Shape: {df.shape}')\nprint(f'Total users: {df['user_id'].nunique()}')\nprint(f'Data range: [{df['date'].min()}, {df['date'].max()}]')\ndf.head()\n\nShape: (667236, 3)\nTotal users: 51480\nData range: [2020-10-01 00:00:00, 2023-10-31 00:00:00]\n\n\n\n\n\n\n\n\n\nuser_id\ndate\nregistration_date\n\n\n\n\n0\nd8c465ab-e9fd-5edd-9e4e-c77094700cb5\n2020-10-01\n2020-08-25\n\n\n1\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-01\n2020-08-25\n\n\n2\nbfeac474-5b66-566f-8654-262bb79c873e\n2020-10-01\n2020-05-31\n\n\n3\nd32fcac5-122c-5463-8aea-01b39b9ad0bb\n2020-10-01\n2020-09-30\n\n\n4\nc1ece677-e643-5bb3-8701-f1c59a0bf4cd\n2020-10-01\n2020-09-05\n\n\n\n\n\n\n\nThis is how the DAU time-series looks like up to 2023-10-31.\n\ndf.groupby('date').size()\\\n    .plot(title='DAU, historical')\n\n\n\n\n\n\n\n\nSuppose that today is 2022-10-31 and we want to predict the DAU metric for the next 2023 year. We define a couple of constants PREDICTION_START and PREDICTION_END which define the prediction period.\n\nPREDICTION_START = '2023-11-01'\nPREDICTION_END = '2024-12-31'\n\n\n\n3.2 Predicting new users amount\nLet’s start from the new users prediction. We use the prophet library as one of the easiest ways to predict time-series data. The new_users Series contains such data. We extract it from the original df dataset selecting the rows where the registration date is equal to the date.\n\n\nToggle the code\nnew_users = df[df['date'] == df['registration_date']]\\\n    .assign(date=pd.to_datetime(df['date']))\\\n    .groupby('date').size()\n\n\n\nnew_users.head()\n\ndate\n2020-10-01    4\n2020-10-02    4\n2020-10-03    3\n2020-10-04    4\n2020-10-05    8\ndtype: int64\n\n\nprophet requires a time-series as a DataFrame containing two columns ds and y, so we reformat the new_users Series to the new_users_prophet DataFrame. Another thing we need to prepare is to create the future variable containing certain days for prediction: from PREDICTION_START to PREDICTION_END. The plot illustrates predictions for both past and future dates.\n\n\nToggle the code\nimport logging\nimport matplotlib.pyplot as plt\nfrom prophet import Prophet\n\n# suppress prophet logs\nlogging.getLogger(\"prophet\").setLevel(logging.WARNING)\nlogging.getLogger(\"cmdstanpy\").disabled=True\n\ndef predict_new_users(prediction_start, prediction_end, new_users_train, show_plot=True):\n    m = Prophet()\n    new_users_train = new_users_train.loc[new_users_train.index &lt; prediction_start]\n    new_users_prophet = pd.DataFrame({'ds': new_users_train.index, 'y': new_users_train.values})\n    m.fit(new_users_prophet)\n\n    periods = len(pd.date_range(prediction_start, prediction_end))\n    future = m.make_future_dataframe(periods=periods)\n    new_users_pred = m.predict(future)\n    if show_plot:\n        m.plot(new_users_pred)\n        plt.title('New users prediction');\n\n    new_users_pred = new_users_pred\\\n        .assign(yhat=lambda _df: _df['yhat'].astype(int))\\\n        .rename(columns={'ds': 'date', 'yhat': 'count'})\\\n        .set_index('date')\\\n        .clip(lower=0)\\\n        ['count']\n\n    return new_users_pred\n\n\n/Users/v.kukushkin/Documents/private/wowone.github.io/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nnew_users_pred = predict_new_users(PREDICTION_START, PREDICTION_END, new_users)\n\n\n\n\n\n\n\n\nThe new_users_pred Series keeps the predicted users amount.\n\nnew_users_pred.tail(5)\n\ndate\n2024-12-27    52\n2024-12-28    56\n2024-12-29    71\n2024-12-30    79\n2024-12-31    74\nName: count, dtype: int64\n\n\n\n\n3.3 Getting the states\nIn practice, the most calculations are reasonable to execute as SQL queries to a database where the data is stored. Hereafter, we will simulate such quering with the duckdb library.\nWe want to assign one of the 7 states to each day of a user’s lifetime within the app. According to the definition, for each day, we need to consider at least the past 30 days. This is where SQL window functions come in. However, since the df data contains only records of active days, we need to explicitly extend it to include the days when a user was not active. In other words, instead of this list of records:\nuser_id    date          registration_date\n1234567    2023-01-01    2023-01-01\n1234567    2023-01-03    2023-01-01\nwe’d like to get a list like this:\nuser_id    date          is_active    registration_date\n1234567    2023-01-01    TRUE         2023-01-01\n1234567    2023-01-02    FALSE        2023-01-01\n1234567    2023-01-03    TRUE         2023-01-01\n1234567    2023-01-04    FALSE        2023-01-01\n1234567    2023-01-05    FALSE        2023-01-01\n...        ...           ...          ...\n1234567    2023-10-31    FALSE        2023-01-01\nFor readability purposes we split the following SQL query into multiple subqueries.\n\nfull_range: Create a full sequence of dates for each user.\ndau_full: Get the full list of both active and inactive records.\nstates: Assign one of the 7 states for each day of a user’s lifetime.\n\n\n\nToggle the code\nimport duckdb\n\nDATASET_START = '2020-11-01'\nDATASET_END = '2023-10-31'\nOBSERVATION_START = '2020-10-01'\n\nquery = f\"\"\"\nWITH\nfull_range AS (\n    SELECT\n        user_id, UNNEST(generate_series(greatest(registration_date, '{OBSERVATION_START}'), date '{DATASET_END}', INTERVAL 1 DAY))::date AS date\n    FROM (\n        SELECT DISTINCT user_id, registration_date FROM df\n    )\n),\ndau_full AS (\n    SELECT\n        fr.user_id,\n        fr.date,\n        df.date IS NOT NULL AS is_active,\n        registration_date\n    FROM full_range AS fr\n    LEFT JOIN df USING(user_id, date)\n),\nstates AS (\n    SELECT\n        user_id,\n        date,\n        is_active,\n        first_value(registration_date IGNORE NULLS) OVER (PARTITION BY user_id ORDER BY date) AS registration_date,\n        SUM(is_active::int) OVER (PARTITION BY user_id ORDER BY date ROWS BETWEEN 6 PRECEDING and 1 PRECEDING) AS active_days_back_6d,\n        SUM(is_active::int) OVER (PARTITION BY user_id ORDER BY date ROWS BETWEEN 29 PRECEDING and 1 PRECEDING) AS active_days_back_29d,\n        CASE\n            WHEN date = registration_date THEN 'new'\n            WHEN is_active = TRUE AND active_days_back_6d BETWEEN 1 and 6 THEN 'current'\n            WHEN is_active = TRUE AND active_days_back_6d = 0 AND IFNULL(active_days_back_29d, 0) &gt; 0 THEN 'reactivated'\n            WHEN is_active = TRUE AND active_days_back_6d = 0 AND IFNULL(active_days_back_29d, 0) = 0 THEN 'resurrected'\n            WHEN is_active = FALSE AND active_days_back_6d &gt; 0 THEN 'at_risk_wau'\n            WHEN is_active = FALSE AND active_days_back_6d = 0 AND ifnull(active_days_back_29d, 0) &gt; 0 THEN 'at_risk_mau'\n            ELSE 'dormant'\n        END AS state\n    FROM dau_full\n)\nSELECT user_id, date, state FROM states\nWHERE date BETWEEN '{DATASET_START}' AND '{DATASET_END}'\nORDER BY user_id, date\n\"\"\"\nstates = duckdb.sql(query).df()\n\n\nThe query results are kept in the states DataFrame:\n\nstates.head()\n\n\n\n\n\n\n\n\nuser_id\ndate\nstate\n\n\n\n\n0\n00002b68-adba-5a55-92d7-8ea8934c6db3\n2023-06-23\nnew\n\n\n1\n00002b68-adba-5a55-92d7-8ea8934c6db3\n2023-06-24\ncurrent\n\n\n2\n00002b68-adba-5a55-92d7-8ea8934c6db3\n2023-06-25\ncurrent\n\n\n3\n00002b68-adba-5a55-92d7-8ea8934c6db3\n2023-06-26\ncurrent\n\n\n4\n00002b68-adba-5a55-92d7-8ea8934c6db3\n2023-06-27\ncurrent\n\n\n\n\n\n\n\n\n\n3.4 Calculating the transition matrix\nHaving obtained these states, we can calculate state transition frequencies. In the real world, due to the large amount of data, it would be more effective to use a SQL query rather than a Python script. We calculate these frequencies day-wise since we’re going to study how the prediction depends on the period in which transitions are considered further.\n\n\nToggle the code\nquery = f\"\"\"\nSELECT\n    date,\n    state_from,\n    state_to,\n    COUNT(*) AS cnt,\nFROM (\n    SELECT\n        date,\n        state AS state_to,\n        lag(state) OVER (PARTITION BY user_id ORDER BY date) AS state_from\n    FROM states\n)\nWHERE state_from IS NOT NULL\nGROUP BY date, state_from, state_to\nORDER BY date, state_from, state_to;\n\"\"\"\ntransitions = duckdb.sql(query).df()\n\n\nThe result is stored in the transitions DataFrame.\n\ntransitions.head()\n\n\n\n\n\n\n\n\ndate\nstate_from\nstate_to\ncnt\n\n\n\n\n0\n2020-11-02\nat_risk_mau\nat_risk_mau\n273\n\n\n1\n2020-11-02\nat_risk_mau\ndormant\n4\n\n\n2\n2020-11-02\nat_risk_mau\nreactivated\n14\n\n\n3\n2020-11-02\nat_risk_wau\nat_risk_mau\n18\n\n\n4\n2020-11-02\nat_risk_wau\nat_risk_wau\n138\n\n\n\n\n\n\n\nNow, we can calculate the transition matrix \\(M\\). We define the get_transition_matrix function, which accepts the transitions DataFrame and a pair of dates that bounds the transitions to be considered.\n\n\nToggle the code\nstates_order = ['new', 'current', 'reactivated', 'resurrected', 'at_risk_wau', 'at_risk_mau', 'dormant']\n\ndef get_transition_matrix(transitions, date1, date2):\n    if pd.to_datetime(date1) &gt; pd.to_datetime(DATASET_END):\n        date1 = pd.to_datetime(DATASET_END) - pd.Timedelta(days=30)\n\n    probs = transitions\\\n        .loc[lambda _df: _df['date'].between(date1, date2)]\\\n        .groupby(['state_from', 'state_to'], as_index=False)\\\n        ['cnt'].sum()\\\n        .assign(\n            supp=lambda _df: _df.groupby('state_from')['cnt'].transform('sum'),\n            prob=lambda _df: _df['cnt'] / _df['supp']\n        )\n\n    M = probs.pivot(index='state_from', columns='state_to', values='prob')\\\n        .reindex(states_order, axis=0)\\\n        .reindex(states_order, axis=1)\\\n        .fillna(0)\\\n        .astype(float)\n\n    return M\n\n\nAs a baseline, let’s calculate the transition matrix for the whole year from 2021-11-01 to 2022-10-31.\n\nM = get_transition_matrix(transitions, '2022-11-01', '2023-10-31')\nM\n\n\n\n\n\n\n\nstate_to\nnew\ncurrent\nreactivated\nresurrected\nat_risk_wau\nat_risk_mau\ndormant\n\n\nstate_from\n\n\n\n\n\n\n\n\n\n\n\nnew\n0.0\n0.515934\n0.000000\n0.000000\n0.484066\n0.000000\n0.000000\n\n\ncurrent\n0.0\n0.851325\n0.000000\n0.000000\n0.148675\n0.000000\n0.000000\n\n\nreactivated\n0.0\n0.365867\n0.000000\n0.000000\n0.634133\n0.000000\n0.000000\n\n\nresurrected\n0.0\n0.316474\n0.000000\n0.000000\n0.683526\n0.000000\n0.000000\n\n\nat_risk_wau\n0.0\n0.098246\n0.004472\n0.000000\n0.766263\n0.131020\n0.000000\n\n\nat_risk_mau\n0.0\n0.000000\n0.009598\n0.000173\n0.000000\n0.950109\n0.040120\n\n\ndormant\n0.0\n0.000000\n0.000000\n0.000387\n0.000000\n0.000000\n0.999613\n\n\n\n\n\n\n\n\n\n3.5 Getting the initial state counts\nAn initial state is easily retrieved from the states DataFrame by the get_state0 function and the corresponding SQL query. We assign the result to the state0 variable.\n\n\nToggle the code\ndef get_state0(date):\n    query = f\"\"\"\n    SELECT state, count(*) AS cnt\n    FROM states\n    WHERE date = '{date}'\n    GROUP BY state\n    \"\"\"\n\n    state0 = duckdb.sql(query).df()\n    state0 = state0.set_index('state').reindex(states_order)['cnt']\n    \n    return state0\n\n\n\nstate0 = get_state0(DATASET_END)\nstate0\n\nstate\nnew               20\ncurrent          475\nreactivated       15\nresurrected       19\nat_risk_wau      404\nat_risk_mau     1024\ndormant        49523\nName: cnt, dtype: int64\n\n\n\n\n3.6 Predicting DAU\nThe predict_dau function below accepts all the previous variables required for the DAU prediction and makes this prediction for a date range defined by the start_date and end_date arguments.\n\n\nToggle the code\ndef predict_dau(M, state0, start_date, end_date, new_users):\n    \"\"\"\n    Predicts DAU over a given date range.\n\n    Parameters\n    ----------\n    M : pandas.DataFrame\n        Transition matrix representing user state changes.\n    state0 : pandas.Series\n        counts of initial state of users.\n    start_date : str\n        Start date of the prediction period in 'YYYY-MM-DD' format.\n    end_date : str\n        End date of the prediction period in 'YYYY-MM-DD' format.\n    new_users : int or pandas.Series\n        The expected amount of new users for each day between `start_date` and `end_date`.\n        If a Series, it should have dates as the index.\n        If an int, the same number is used for each day.\n        \n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame containing the predicted DAU, WAU, and MAU for each day in the date range,\n        with columns for different user states and tot.\n    \"\"\"\n    \n    dates = pd.date_range(start_date, end_date)\n    dates.name = 'date'\n    dau_pred = []\n    new_dau = state0.copy()\n    for date in dates:\n        new_dau = (M.transpose() @ new_dau).astype(int)\n        if isinstance(new_users, int):\n            new_users_today = new_users\n        else:\n            new_users_today = new_users.astype(int).loc[date] \n        new_dau.loc['new'] = new_users_today\n        dau_pred.append(new_dau.tolist())\n\n    dau_pred = pd.DataFrame(dau_pred, index=dates, columns=states_order)\n    dau_pred['dau'] = dau_pred['new'] + dau_pred['current'] + dau_pred['reactivated'] + dau_pred['resurrected']\n    dau_pred['wau'] = dau_pred['dau'] + dau_pred['at_risk_wau']\n    dau_pred['mau'] = dau_pred['dau'] + dau_pred['at_risk_wau'] + dau_pred['at_risk_mau']\n    \n    return dau_pred\n\n\n\ndau_pred = predict_dau(M, state0, PREDICTION_START, PREDICTION_END, new_users_pred)\ndau_pred\n\n\n\n\n\n\n\n\nnew\ncurrent\nreactivated\nresurrected\nat_risk_wau\nat_risk_mau\ndormant\ndau\nwau\nmau\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-11-01\n29\n465\n11\n19\n412\n1025\n49544\n524\n936\n1961\n\n\n2023-11-02\n25\n461\n11\n19\n418\n1027\n49565\n516\n934\n1961\n\n\n2023-11-03\n21\n456\n11\n19\n420\n1030\n49587\n507\n927\n1957\n\n\n2023-11-04\n22\n450\n11\n19\n419\n1033\n49609\n502\n921\n1954\n\n\n2023-11-05\n34\n445\n11\n19\n418\n1036\n49631\n509\n927\n1963\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2024-12-27\n52\n505\n12\n24\n487\n1129\n61599\n593\n1080\n2209\n\n\n2024-12-28\n56\n516\n13\n24\n497\n1136\n61620\n609\n1106\n2242\n\n\n2024-12-29\n71\n529\n13\n24\n509\n1144\n61641\n637\n1146\n2290\n\n\n2024-12-30\n79\n549\n13\n24\n527\n1153\n61663\n665\n1192\n2345\n\n\n2024-12-31\n74\n572\n13\n24\n548\n1164\n61685\n683\n1231\n2395\n\n\n\n\n427 rows × 10 columns\n\n\n\nBesides the expected dau, wau, and mau columns, the output contains the number of users in each state for each prediction date.\nFinally, we calculate the ground-truth values of DAU, WAU, and MAU (along with the corresponding state decomposition), keep them in the dau_true DataFrame, and plot the predicted and true values altogether.\n\n\nToggle the code\nquery = f\"\"\"\nSELECT date, state, COUNT(*) AS cnt\nFROM states\nGROUP BY date, state\nORDER BY date, state;\n\"\"\"\n\ndau_true = duckdb.sql(query).df()\ndau_true['date'] = pd.to_datetime(dau_true['date'])\ndau_true = dau_true.pivot(index='date', columns='state', values='cnt')\ndau_true['dau'] = dau_true['new'] + dau_true['current'] + dau_true['reactivated'] + dau_true['resurrected']\ndau_true['wau'] = dau_true['dau'] + dau_true['at_risk_wau']\ndau_true['mau'] = dau_true['dau'] + dau_true['at_risk_wau'] + dau_true['at_risk_mau']\n\n\n\ndau_true.head()\n\n\n\n\n\n\n\nstate\nat_risk_mau\nat_risk_wau\ncurrent\ndormant\nnew\nreactivated\nresurrected\ndau\nwau\nmau\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020-11-01\n291.0\n207.0\n293.0\n840.0\n36.0\n14.0\n3.0\n346.0\n553.0\n844.0\n\n\n2020-11-02\n291.0\n208.0\n327.0\n836.0\n53.0\n14.0\n8.0\n402.0\n610.0\n901.0\n\n\n2020-11-03\n296.0\n205.0\n383.0\n840.0\n41.0\n10.0\n3.0\n437.0\n642.0\n938.0\n\n\n2020-11-04\n296.0\n246.0\n375.0\n842.0\n27.0\n13.0\n6.0\n421.0\n667.0\n963.0\n\n\n2020-11-05\n300.0\n275.0\n373.0\n845.0\n33.0\n8.0\n4.0\n418.0\n693.0\n993.0\n\n\n\n\n\n\n\n\n\nToggle the code\npd.concat([dau_true['dau'], dau_pred['dau']])\\\n    .plot(title='DAU, historical & predicted');"
  },
  {
    "objectID": "posts/2024-11-02_dau_prediction/dau_prediction.html#model-evaluation",
    "href": "posts/2024-11-02_dau_prediction/dau_prediction.html#model-evaluation",
    "title": "Modeling DAU with Markov chain (WIP)",
    "section": "4. Model evaluation",
    "text": "4. Model evaluation\n\n4.1 Baseline comparison\nFirst of all, let’s check whether we really need to build a complex model to predict DAU. Wouldn’t it be better to predict DAU as a general time-series using the mentioned prophet library? The function predict_dau_simple implements this. We try to use some tweaks available in the library in order to make the prediction more accurate:\n\nwe use logistic model instead of linear in order to avoid negative values;\nwe add explicitly monthly and yearly seasonality;\nwe remove the outliers.\nwe set explicitly define peak values for January and February as “holidays”.\n\nThe fact that the code turns out to be quite sophisticated indicates that one can’t simply apply prophet to the DAU time-series.\n\n\nToggle the code\ndef predict_dau_prophet(prediction_start, prediction_end, dau_true, show_plot=True):\n    holidays = pd.DataFrame({\n        'holiday': 'january_spike',\n        'ds': pd.date_range('2022-01-01', '2022-01-31', freq='D').tolist() + \\\n              pd.date_range('2023-01-01', '2023-01-31', freq='D').tolist(),\n        'lower_window': 0,\n        'upper_window': 40\n    })\n\n    m = Prophet(growth='logistic', holidays=holidays)\n    m.add_seasonality(name='monthly', period=30.5, fourier_order=3)\n    m.add_seasonality(name='yearly', period=365, fourier_order=3)\n\n    train = dau_true.loc[(dau_true.index &lt; prediction_start) & (dau_true.index &gt;= '2021-08-01')]\n    train_prophet = pd.DataFrame({'ds': train.index, 'y': train.values})\n    train_prophet.loc[train_prophet['ds'].between('2022-06-07', '2022-06-09'), 'y'] = None\n    train_prophet['new_year_peak'] = (train_prophet['ds'] &gt;= '2022-01-01') &\\\n                                     (train_prophet['ds'] &lt;= '2022-02-14')\n    m.add_regressor('new_year_peak')\n    train_prophet['cap'] = dau_true.max() * 1.1\n    train_prophet['floor'] = 0\n\n    m.fit(train_prophet)\n\n    periods = len(pd.date_range(prediction_start, prediction_end))\n    future = m.make_future_dataframe(periods=periods)\n    future['new_year_peak'] = (future['ds'] &gt;= '2022-01-01') & (future['ds'] &lt;= '2022-02-14')\n    future['cap'] = dau_true.max() * 1.1\n    future['floor'] = 0\n    pred = m.predict(future)\n\n    if show_plot:\n        m.plot(pred);\n\n    pred = pred\\\n        .assign(yhat=lambda _df: _df['yhat'].astype(int))\\\n        .rename(columns={'ds': 'date', 'yhat': 'count'})\\\n        .set_index('date')\\\n        .clip(lower=0)\\\n        ['count']\\\n        .loc[lambda s: (s.index &gt;= prediction_start) & (s.index &lt;= prediction_end)]\n\n    return pred\n\n\nHereafter we test a prediction for multiple predicting horizonts: 3, 6, and 12 months. As a result, we get 3 test sets:\n\n3-months horizont: 2023-08-01 - 2023-10-31,\n6-months horizont: 2023-05-01 - 2023-10-31,\n1-year horizont: 2022-11-01 - 2023-10-31.\n\nFor each test set we calculate the MAPE loss function.\n\n\nToggle the code\nfrom sklearn.metrics import mean_absolute_percentage_error\n\nmapes = []\nprediction_end = '2023-10-31'\nprediction_horizont = [3, 6, 12]\n\nfor offset in prediction_horizont:\n    prediction_start = pd.to_datetime(prediction_end) - pd.DateOffset(months=offset - 1)\n    prediction_start = prediction_start.replace(day=1)\n    prediction_end = '2023-10-31'\n    pred = predict_dau_prophet(prediction_start, prediction_end, dau_true['dau'], show_plot=False)\n    mape = mean_absolute_percentage_error(dau_true['dau'].reindex(pred.index), pred)\n    mapes.append(mape)\n\nmapes = pd.DataFrame({'horizont': prediction_horizont, 'MAPE': mapes})\nmapes\n\n\n\n\n\n\n\n\n\nhorizont\nMAPE\n\n\n\n\n0\n3\n0.350167\n\n\n1\n6\n0.185246\n\n\n2\n12\n0.215338\n\n\n\n\n\n\n\nThe MAPE error turns out to be high: 18% - 35%. The fact that the shortest horizont has the highest error means that the model is tuned for the long-term predictions. This is another inconvenience of such an approach: we have to tune the model for each prediction horizont. So this is our baseline. In the next section we’ll compare it with the more advanced models.\n\n\n4.2 General evaluation\nIn this section we evaluate the model implemented in the section 3. So far we set the transition period as 1 year before the prediction start. We’ll study how the prediction depends on the transition period in the next section. As for the new users, we run the model using two options: the real values and the predicted ones. Similarly, we fix the same 3 prediction horizonts and test the model on them.\nThe make_predicion function below implements the described opttions. It accepts prediction_start, prediction_end arguments defining the prediction period for a given horizont, new_users_mode which can be either true or predict, and transition_period.\n\n\nToggle the code\nimport re\n\n\ndef make_prediction(prediction_start, prediction_end, new_users_mode='predict', transition_period='last_30d'):\n    prediction_start_minus_1d = pd.to_datetime(prediction_start) - pd.Timedelta('1d')\n    state0 = get_state0(prediction_start_minus_1d)\n    \n    if new_users_mode == 'predict':\n        new_users_pred = predict_new_users(prediction_start, prediction_end, new_users, show_plot=False)\n    elif new_users_mode == 'true':\n        new_users_pred = new_users.copy()\n\n    if transition_period.startswith('last_'):\n        shift = int(re.search(r'last_(\\d+)d', transition_period).group(1))\n        transitions_start = pd.to_datetime(prediction_start) - pd.Timedelta(shift, 'd')\n        M = get_transition_matrix(transitions, transitions_start, prediction_start_minus_1d)\n        dau_pred = predict_dau(M, state0, prediction_start, prediction_end, new_users_pred)\n    else:\n        transitions_start = pd.to_datetime(prediction_start) - pd.Timedelta(240, 'd')\n        M_base = get_transition_matrix(transitions, transitions_start, prediction_start_minus_1d)\n        dau_pred = pd.DataFrame()\n\n        month_starts = pd.date_range(prediction_start, prediction_end, freq='1MS')\n        N = len(month_starts)\n\n        for i, prediction_month_start in enumerate(month_starts):\n            prediction_month_end = pd.offsets.MonthEnd().rollforward(prediction_month_start)\n            transitions_month_start = prediction_month_start - pd.Timedelta('365D')\n            transitions_month_end = prediction_month_end - pd.Timedelta('365D')\n\n            M_seasonal = get_transition_matrix(transitions, transitions_month_start, transitions_month_end)\n            if transition_period == 'smoothing':\n                i = min(i, 12)\n                M = M_seasonal * i / (N - 1)  + (1 - i / (N - 1)) * M_base\n            elif transition_period.startswith('seasonal_'):\n                seasonal_coef = float(re.search(r'seasonal_(0\\.\\d+)', transition_period).group(1))\n                M = seasonal_coef * M_seasonal + (1 - seasonal_coef) * M_base\n            \n            dau_tmp = predict_dau(M, state0, prediction_month_start, prediction_month_end, new_users_pred)\n            dau_pred = pd.concat([dau_pred, dau_tmp])\n\n            state0 = dau_tmp.loc[prediction_month_end][states_order]\n\n    return dau_pred\n\ndef prediction_details(dau_true, dau_pred, show_plot=True, ax=None):\n    y_true = dau_true.reindex(dau_pred.index)['dau']\n    y_pred = dau_pred['dau']\n    mape = mean_absolute_percentage_error(y_true, y_pred) \n\n    if show_plot:\n        prediction_start = str(y_true.index.min().date())\n        prediction_end = str(y_true.index.max().date())\n        if ax is None:\n            y_true.plot(label='DAU true')\n            y_pred.plot(label='DAU pred')\n            plt.title(f'DAU prediction, {prediction_start} - {prediction_end}')\n            plt.legend()\n        else:\n            y_true.plot(label='DAU true', ax=ax)\n            y_pred.plot(label='DAU pred', ax=ax)\n            ax.set_title(f'DAU prediction, {prediction_start} - {prediction_end}')\n            ax.legend()\n    return mape\n\n\nThe charts on the left relate to the new_users_mode = 'true' option, while the right ones relate to the new_users_mode = 'predict' option.\n\n\nToggle the code\nfig, axs = plt.subplots(3, 2, figsize=(15, 6))\nmapes = []\nprediction_end = '2023-10-31'\nprediction_horizont = [3, 6, 12]\n\nfor i, offset in enumerate(prediction_horizont):\n    prediction_start = pd.to_datetime(prediction_end) - pd.DateOffset(months=offset - 1)\n    prediction_start = prediction_start.replace(day=1)\n    args = {\n        'prediction_start': prediction_start,\n        'prediction_end': prediction_end,\n        'transition_period': 'last_365d'\n    }\n    for j, new_users_mode in enumerate(['predict', 'true']):\n        args['new_users_mode'] = new_users_mode\n        dau_pred = make_prediction(**args)\n        mape = prediction_details(dau_true, dau_pred, ax=axs[i, j])\n        mapes.append([offset, new_users_mode, mape])\n\nmapes = pd.DataFrame(mapes, columns=['horizont', 'new_users', 'MAPE'])\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nmapes.pivot(index='horizont', columns='new_users', values='MAPE')\n\n\n\n\n\n\n\nnew_users\npredict\ntrue\n\n\nhorizont\n\n\n\n\n\n\n3\n0.079538\n0.103631\n\n\n6\n0.125035\n0.084652\n\n\n12\n0.651502\n0.086885\n\n\n\n\n\n\n\nWe notice multiple things.\n\nIn general, the model demonstrates much better results than the baseline. Indeed, the baseline is based on the historical DAU data only, while the model uses the user states information.\nHowever, for the 1-year horizont and new_users_mode='predict' the MAPE error is huge: 65%. This is 3 times higher than the corresponding baseline error (21%). On the other hand, new_users_mode='true' option gives a much better result: 8%. It means that the new users prediction has a huge impact on the model, especially for long-term predictions. For the shorter periods the difference is less dramatic. The major difference for such a difference is that 1-year period includes Christmas with its extreme values. As a result, i) it’s hard to predict such high new user values, ii) the period heavily impacts user behavior, the transition matrix and, consequently, DAU values. Hence, we strongly recommend to implement the new user prediction carefully. The baseline model was specially tuned for this Christmas period, so it’s not surprising that it outperforms the Markov model.\nWhen the new users prediction is accurate, the model captures trends well. It means that using last 365 days for the transition matrix calculation is a reasonable choice.\nInterestingly, the true new users data provides worse results for the 3-months prediction. This is nothing but a coincidence. The wrong new users prediction in October 2023 reversed the predicted DAU trend and made MAPE a bit lower.\n\nNow, let’s decompose the prediction error and see which states contribure the most. Since we already know that the new users prediction has a huge impact on the model, we focus on a shorter prediction period in order to study the other factors.\n\n\nToggle the code\ndau_component_cols = ['new', 'current', 'reactivated', 'resurrected']\n\ndau_pred = make_prediction('2023-08-01', '2023-10-31', new_users_mode='predict', transition_period='last_365d')\nfigure, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\ndau_pred[dau_component_cols]\\\n    .subtract(dau_true[dau_component_cols])\\\n    .reindex(dau_pred.index)\\\n    .plot(title='Prediction error by state', ax=ax1)\n\ndau_pred[['current']]\\\n    .subtract(dau_true[['current']])\\\n    .div(dau_true[['current']])\\\n    .reindex(dau_pred.index)\\\n    .plot(title='Relative prediction error (current state)', ax=ax2);\n\n\n\n\n\n\n\n\n\nFrom the left chart we notice that the error is basically contributed by the current state. It’s not surprising since this state contributes to DAU the most. The error for the new, reactivated, and resurrected states is quite low. Another interesting thing is that this error is mostly negative. Perhaps it mean that the new users who appeared in the prediction period are more engaged that the users from the past.\nAs for the relative error on the right chart, it makes sense to analyze it for the current state only. This is because the daily amount of the reactivated and resurrected states are low so the relative error is high and noisy. The relative error for the current state varies between -20% and 5% which is quite high. And since the current state amount is mostly regulated by the current -&gt; current conversion rate (it’s roughly 0.8), this error is explained by the transition matrix inaccuracy.\n\n\n4.3 Transitions period impact\nIn the previous section we kept the transitions period fixed – 1 year before a prediction start. Now we’re going to study how long this period should be to get more accurate prediction. We consider the same prediction horizonts of 3, 6, and 12 months. In order to mitigate the noise from the new users prediction, we use the real values of the new users amount.\n\n\nToggle the code\nresult = []\n\nfor prediction_offset in prediction_horizont:\n    prediction_start = pd.to_datetime(prediction_end) - pd.DateOffset(months=prediction_offset - 1)\n    prediction_start = prediction_start.replace(day=1)\n\n    for transition_offset in range(1, 13):\n        dau_pred = make_prediction(\n            prediction_start, prediction_end, new_users_mode='true',\n            transition_period=f'last_{transition_offset*30}d'\n        )\n        mape = prediction_details(dau_true, dau_pred, show_plot=False)\n        result.append([prediction_offset, transition_offset, mape])\nresult = pd.DataFrame(result, columns=['prediction_period', 'transition_period', 'mape'])\n\nresult.pivot(index='transition_period', columns='prediction_period', values='mape')\\\n    .plot(title='MAPE by prediction and transition period');\n\n\n\n\n\n\n\n\n\nIt turns out that the optimal transitions period depends on the prediction horizont. Shorter horizonts require shorter transitions periods: the minimal MAPE error is achieved at 1, 4, and 8 transition periods for the 3, 6, and 12 months correspondingly. It seems this is because the longer horizonts contain some seasonal effects that could be captured only by the longer transitions periods.\n\n\n4.4 Obsolence and seasonality\nNevertheless, fixing a single transition matrix for predicting the whole year ahead doesn’t seem to be a good idea: such a model would be too rigid. Usually, the user behavior varies depending on a season. For example, users who appear after Christmas might have some shifts in behavior. Another typical situation is when users change their behavior in summer. In this section, we’ll try to take into account these seasonal effects.\nSo we want to predict DAU for the 1 year ahead starting from November 2022. Instead of using a single transition matrix \\(M_{base}\\) which is calculated for the last 8 months before the prediction start (as an optimal transitions period derived from the previous section, labeled as last_240d option), we’ll consider a mixture of this matrix and a seasonal one \\(M_{seasonal}\\). The latter is calculated on monthly basis lagging 1 year behind. For example, to predict DAU for November 2022 we define \\(M_{seasonal}\\) as the transition matrix for November 2021. Then we shift the prediction horizon to December 2022 and calculate \\(M_{seasonal}\\) for December 2021, etc.\nIn order to mix \\(M_{base}\\) and \\(M_{seasonal}\\) we define the following two options.\n\nseasonal_0.3: \\(M = 0.3 \\cdot M_{seasonal} + 0.7 \\cdot M_{base}\\). 0.3 weight was chosen as a local minimum after some experiments.\nsmoothing: \\(M = \\frac{i}{N - 1} M_{seasonal} + (1 - \\frac{i}{N - 1}) M_{base}\\) where \\(N\\) is the number of months within the predicting period, \\(i = 0, \\ldots, N - 1\\) – the month index. The idea of this configuration is to gradually switch from the most recent transition matrix \\(M_{base}\\) to seasonal ones as the prediction month moves forward from the prediction start.\n\n\n\nToggle the code\nresult = pd.DataFrame()\nfor transition_period in ['last_240d', 'seasonal_0.3', 'smoothing']:\n    result[transition_period] = make_prediction('2022-11-01', '2023-10-31', 'true', transition_period)['dau']\nresult['true'] = dau_true['dau']\nresult['true'] = result['true'].astype(int)\nresult.plot(title='DAU prediction by different transition matrices');\n\n\n\n\n\n\n\n\n\n\n\nToggle the code\nmape = pd.DataFrame()\nfor col in result.columns:\n    if col != 'true':\n        mape.loc[col, 'mape'] = mean_absolute_percentage_error(result['true'], result[col])\nmape\n\n\n\n\n\n\n\n\n\nmape\n\n\n\n\nlast_240d\n0.080804\n\n\nseasonal_0.3\n0.077545\n\n\nsmoothing\n0.097802\n\n\n\n\n\n\n\nAccording to the MAPE errors, seasonal_0.3 configuration provides the best results. Interestingly, smoothing approach has appeared to be even worse than the last_240d. From the diagram above we see that all three models start to underestimate the DAU values from July 2023, especially the smoothing model. It seems that the new users who started appearing from July 2023 are more engaged than the users from 2022. Probably, the app was improved sufficiently or the marketing team did a great job. As a result, the smoothing model that much relies on the outdated transitions data from July 2022 - October 2022 fails more than the other models.\n\n\n4.5 Final solution\nTo sum things up, let’s make a final prediction for the 2024 year. We use the seasonal_0.3 configuration and the predicted values for new users.\n\n\nToggle the code\ndau_pred = make_prediction(PREDICTION_START, PREDICTION_END, new_users_mode='predict', transition_period='seasonal_0.3')\ndau_true['dau'].plot(label='true')\ndau_pred['dau'].plot(label='seasonal_0.3')\nplt.title('DAU, historical & predicted')\nplt.legend();"
  },
  {
    "objectID": "posts/2024-11-02_dau_prediction/dau_prediction.html#discussion",
    "href": "posts/2024-11-02_dau_prediction/dau_prediction.html#discussion",
    "title": "Modeling DAU with Markov chain (WIP)",
    "section": "5. Discussion",
    "text": "5. Discussion\nIn the section 4 we studied the model performance from the prediction accuracy perspective. Now let’s discuss the model from the practical point of view. We compare it with the baseline model (see the section 4.1).\nBesides poor accuracy, predicting DAU as a time-series makes this approach very stiff. The only thing we can control here is the historical data. In practice, when making plans for the next year we have some certain expectations about the future. For example,\n\nthe marketing team is going to launch some new more effective campaings,\nthe activation team is planning to improve the onboarding process,\nthe product team will release some new features that would engage and retain users more.\n\nOur model can take into account such expectations. For the examples above we can adjust the new users prediction, the new→current and the current→current conversion rates respectively. As a result we can get a prediction that is doesn’t match with the past data but would be more realistic. This model’s property is not just flexible – it’s interpretable. You can easily discuss all these adjustments with the stakeholders, and they can understand how the prediction works.\nAnother advantage of the model is that it doesn’t require to predict whether a user will be active on a certain day. Sometimes binary classifiers are used for this purpose. The downside of this approach is that we need to apply such a classifier to each user including all the dormant users and each day. This is a tremedous computational cost. Unlike this, the Markov model requires only the initial amount of states (state0). Moreover, such classiffiers are often black-box models: they are poorly interpretable and hard to adjust.\nThe Markov model also has some limitations. As we already have seen, it’s sensitive to the new users prediction. It’s easy to totally ruin the prediction by the wrong new users amount. Another problem is that the Markov model is memoryless meaning that it doesn’t take into account the user’s history. For example, it doesn’t distinguish whether a current user is a newbie, experienced, or just reactivated/resurrected. The retention rate of these user types should be certainly different. Also, as we discussed earlier the user bahavior might be of different nature depending on the season, marketing sources, countries, etc. So far our model is not able to capture these differences. However, this might be a subject for further research."
  },
  {
    "objectID": "posts/2024-12-02_dau_prediction/dau_prediction.html",
    "href": "posts/2024-12-02_dau_prediction/dau_prediction.html",
    "title": "Modeling DAU with Markov chain",
    "section": "",
    "text": "Doubtlessly, DAU, WAU, and MAU are critical business metrics. An article “How Duolingo reignited user growth” by Jorge Mazal, former CPO of Duolingo, is #1 in the Growth section of Lenny’s Newsletter blog. In this article, Jorge paid special attention to the methodology Duolingo used to model the DAU metric (see another article “Meaningful metrics: how data sharpened the focus of product teams” by Erin Gustafson). This methodology has multiple strengths, but I’d like to focus on how one can use this approach for DAU forecasting.\nThe new year is coming soon, so many companies are planning their budgets for the next year these days. Cost estimations often require DAU forecasts. In this article, I’ll show how you can get this prediction using Duolingo’s growth model. I’ll explain why this approach is better compared to standard time-series forecasting methods and how you can adjust the prediction according to your teams’ plans (e.g., marketing, activation, product teams).\nThe article text goes along with the code, and a simulated dataset is attached so the research is fully reproducible. The Jupyter notebook version is available here. In the end, I’ll share a DAU “calculator” designed in Google Spreadsheet format.\nI’ll be narrating on behalf of the collective “we”. I’m used to it because I usually write articles with co-authors."
  },
  {
    "objectID": "posts/2024-12-02_dau_prediction/dau_prediction.html#introduction",
    "href": "posts/2024-12-02_dau_prediction/dau_prediction.html#introduction",
    "title": "Modeling DAU with Markov chain",
    "section": "",
    "text": "Doubtlessly, DAU, WAU, and MAU are critical business metrics. An article “How Duolingo reignited user growth” by Jorge Mazal, former CPO of Duolingo, is #1 in the Growth section of Lenny’s Newsletter blog. In this article, Jorge paid special attention to the methodology Duolingo used to model the DAU metric (see another article “Meaningful metrics: how data sharpened the focus of product teams” by Erin Gustafson). This methodology has multiple strengths, but I’d like to focus on how one can use this approach for DAU forecasting.\nThe new year is coming soon, so many companies are planning their budgets for the next year these days. Cost estimations often require DAU forecasts. In this article, I’ll show how you can get this prediction using Duolingo’s growth model. I’ll explain why this approach is better compared to standard time-series forecasting methods and how you can adjust the prediction according to your teams’ plans (e.g., marketing, activation, product teams).\nThe article text goes along with the code, and a simulated dataset is attached so the research is fully reproducible. The Jupyter notebook version is available here. In the end, I’ll share a DAU “calculator” designed in Google Spreadsheet format.\nI’ll be narrating on behalf of the collective “we”. I’m used to it because I usually write articles with co-authors."
  },
  {
    "objectID": "posts/2024-12-02_dau_prediction/dau_prediction.html#methodology",
    "href": "posts/2024-12-02_dau_prediction/dau_prediction.html#methodology",
    "title": "Modeling DAU with Markov chain",
    "section": "2 Methodology",
    "text": "2 Methodology\nA quick recap on how the Duolingo’s growth model works. At day \\(d\\) (\\(d=1,2,\\ldots,\\)) of a user’s lifetime, the user can be in one of the following 7 (mutually-exclusive) states: new, current, reactivated, resurrected, at_risk_wau, at_risk_mau, dormant. The states are defined according to indicators of whether a user was active today, in the last 7 days, or in the last 30 days. The definition summary is given in the table below:\n\n\n\n\nstate\n\n\nd = 1\n\n\nactivetoday\n\n\nactive in[d-6, d-1]\n\n\nactive in[d-29, d-7]\n\n\nactivebefore d-30\n\n\n\n\n\nnew\n\n\n✅\n\n\n?\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n\n\ncurrent\n\n\n❌\n\n\n✅\n\n\n✅\n\n\n❓\n\n\n❓\n\n\n\n\nreactivated\n\n\n❌\n\n\n✅\n\n\n❌\n\n\n✅\n\n\n?\n\n\n\n\nresurrected\n\n\n❌\n\n\n✅\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n\n\nat_risk_wau\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n?\n\n\n?\n\n\n\n\nat_risk_mau\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n?\n\n\n\n\ndormant\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n\nHaving these states defined (as a set \\(S\\)), we can consider user behavior as a Markov chain. Here’s an example of a user’s trajectory: new→current→current→at_risk_wau→…→at_risk_mau→…→dormant. Let \\(M\\) be a transition matrix associated with this Markov process: \\(m_{i, j} = P(s_j | s_i)\\) are the probabilities that a user moves to state \\(s_j\\) right after being at state \\(s_i\\), where \\(s_i, s_j \\in S\\). Such a matrix is inferred from the historical data.\nIf we assume that user behavior is stationary (independent of time), the matrix \\(M\\) fully describes the states of all users in the future. Suppose that the vector \\(u_0\\) of length 7 contains the counts of users in certain states on a given day, denoted as day 0. According to the Markov model, on the next day 1, we expect to have the following number of users states \\(u_1\\):\n\\[\n\\underbrace{\n\\begin{pmatrix}  \\#New_1 \\\\ \\#Current_1 \\\\ \\#Reactivated_1 \\\\ \\#Resurrected_1 \\\\ \\#AtRiskWau_1 \\\\ \\#AtRiskMau_1 \\\\ \\#Dormant_1 \\end{pmatrix}\n}_{u_1} = M^T \\cdot\n\\underbrace{\n\\begin{pmatrix}  \\#New_0 \\\\ \\#Current_0 \\\\ \\#Reactivated_0 \\\\ \\#Resurrected_0 \\\\ \\#AtRiskWau_0 \\\\ \\#AtRiskMau_0 \\\\ \\#Dormant_0 \\end{pmatrix}\n}_{u_0}\n\\]\nApplying this formula recursively, we derive the number of users in certain states on any arbitrary day \\(t &gt; 0\\) in the future. Besides the initial distribution \\(u_0\\), we need to provide the number of new users that will appear in the product each day in the future. We’ll address this problem as a general time-series forecasting.\nNow, having \\(u_t\\) calculated, we can determine DAU values on day \\(t\\): \\[\\begin{equation} DAU_t = \\#New_t + \\#Current_t + \\#Reactivated_t + \\#Resurrected_t \\end{equation}.\\]\nAdditionally, we can easily calculate WAU and MAU metrics: \\[WAU_t = DAU_t + \\#AtRiskWau_t,\\] \\[MAU_t = DAU_t + \\#AtRiskWau_t + \\#AtRiskMau_t.\\]\nFinally, here’s the algorithm outline:\n\nFor each prediction day \\(t=1, \\ldots, T\\), calculate the expected number of new users \\(\\#New_1, \\ldots, \\#New_T\\).\nFor each lifetime day of each user, assign one of the 7 states.\nCalculate the transition matrix \\(M\\) from the historical data.\nCalculate initial state counts \\(u_0\\) corresponding to day \\(t=0\\).\nRecursively calculate \\(u_{t+1} = M^T u_t\\).\nCalculate DAU, WAU, and MAU for each prediction day \\(t=1, \\ldots, T\\)."
  },
  {
    "objectID": "posts/2024-12-02_dau_prediction/dau_prediction.html#implementation",
    "href": "posts/2024-12-02_dau_prediction/dau_prediction.html#implementation",
    "title": "Modeling DAU with Markov chain",
    "section": "3 Implementation",
    "text": "3 Implementation\nThis section is devoted to technical aspects of the implementation. If you’re interested in studying the model properties rather than code, you may skip this section and go to the Section 4.\n\n3.1 Dataset\nWe use a simulated dataset based on historical data of a SaaS app. The data is stored in the dau_data.csv.gz file and contains three columns: user_id, date, and registration_date. Each record indicates a day when a user was active. The dataset includes activity indicators for 51480 users from 2020-11-01 to 2023-10-31. Additionally, data from October 2020 is included to calculate user states properly, as the at_risk_mau and dormant states require data from one month prior.\n\nimport pandas as pd\n\ndf = pd.read_csv('dau_data.csv.gz', compression='gzip')\ndf['date'] = pd.to_datetime(df['date'])\ndf['registration_date'] = pd.to_datetime(df['registration_date'])\n\nprint(f'Shape: {df.shape}')\nprint(f'Total users: {df['user_id'].nunique()}')\nprint(f'Data range: [{df['date'].min()}, {df['date'].max()}]')\ndf.head()\n\nShape: (667236, 3)\nTotal users: 51480\nData range: [2020-10-01 00:00:00, 2023-10-31 00:00:00]\n\n\n\n\n\n\n\n\n\nuser_id\ndate\nregistration_date\n\n\n\n\n0\nd8c465ab-e9fd-5edd-9e4e-c77094700cb5\n2020-10-01\n2020-08-25\n\n\n1\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-01\n2020-08-25\n\n\n2\nbfeac474-5b66-566f-8654-262bb79c873e\n2020-10-01\n2020-05-31\n\n\n3\nd32fcac5-122c-5463-8aea-01b39b9ad0bb\n2020-10-01\n2020-09-30\n\n\n4\nc1ece677-e643-5bb3-8701-f1c59a0bf4cd\n2020-10-01\n2020-09-05\n\n\n\n\n\n\n\nThis is how the DAU time-series looks like.\n\ndf.groupby('date').size()\\\n    .plot(title='DAU, historical')\n\n\n\n\n\n\n\n\nSuppose that today is 2023-10-31 and we want to predict the DAU metric for the next 2024 year. We define a couple of global constants PREDICTION_START and PREDICTION_END which encompass the prediction period.\n\nPREDICTION_START = '2023-11-01'\nPREDICTION_END = '2024-12-31'\n\n\n\n3.2 Predicting new users amount\nLet’s start from the new users prediction. We use the prophet library as one of the easiest ways to forecast time-series data. The new_users Series contains such data. We extract it from the original df dataset selecting the rows where the registration date is equal to the date.\n\n\nToggle the code\nnew_users = df[df['date'] == df['registration_date']]\\\n    .groupby('date').size()\n\n\n\nnew_users.head()\n\ndate\n2020-10-01    4\n2020-10-02    4\n2020-10-03    3\n2020-10-04    4\n2020-10-05    8\ndtype: int64\n\n\nprophet requires a time-series as a DataFrame containing two columns ds and y, so we reformat the new_users Series to the new_users_prophet DataFrame. Another thing we need to prepare is to create the future variable containing certain days for prediction: from prediction_start to prediction_end. This logic is implemented in the predict_new_users function. The plot below illustrates predictions for both past and future periods.\n\n\nToggle the code\nimport logging\nimport matplotlib.pyplot as plt\nfrom prophet import Prophet\n\n# suppress prophet logs\nlogging.getLogger('prophet').setLevel(logging.WARNING)\nlogging.getLogger('cmdstanpy').disabled=True\n\ndef predict_new_users(prediction_start, prediction_end, new_users_train, show_plot=True):\n    \"\"\"\n    Forecasts a time-seires for new users\n\n    Parameters\n    ----------\n    prediction_start : str\n        Date in YYYY-MM-DD format.\n    prediction_end : str\n        Date in YYYY-MM-DD format.\n    new_users_train : pandas.Series\n        Historical data for the time-series preceding the prediction period.\n    show_plot : boolean, default=True\n        If True, a chart with the train and predicted time-series values is displayed.\n    Returns\n    -------\n    pandas.Series\n        Series containing the predicted values.\n    \"\"\"\n    m = Prophet()\n\n    new_users_train = new_users_train.loc[new_users_train.index &lt; prediction_start]\n    new_users_prophet = pd.DataFrame({'ds': new_users_train.index, 'y': new_users_train.values})\n\n    m.fit(new_users_prophet)\n\n    periods = len(pd.date_range(prediction_start, prediction_end))\n    future = m.make_future_dataframe(periods=periods)\n    new_users_pred = m.predict(future)\n    if show_plot:\n        m.plot(new_users_pred)\n        plt.title('New users prediction');\n\n    new_users_pred = new_users_pred\\\n        .assign(yhat=lambda _df: _df['yhat'].astype(int))\\\n        .rename(columns={'ds': 'date', 'yhat': 'count'})\\\n        .set_index('date')\\\n        .clip(lower=0)\\\n        ['count']\n\n    return new_users_pred\n\n\n/Users/v.kukushkin/Documents/private/wowone.github.io/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\nnew_users_pred = predict_new_users(PREDICTION_START, PREDICTION_END, new_users)\n\n\n\n\n\n\n\n\nThe new_users_pred Series stores the predicted users amount.\n\nnew_users_pred.tail(5)\n\ndate\n2024-12-27    52\n2024-12-28    56\n2024-12-29    71\n2024-12-30    79\n2024-12-31    74\nName: count, dtype: int64\n\n\n\n\n3.3 Getting the states\nIn practice, the most calculations are reasonable to execute as SQL queries to a database where the data is stored. Hereafter, we will simulate such querying using the duckdb library.\nWe want to assign one of the 7 states to each day of a user’s lifetime within the app. According to the definition, for each day, we need to consider at least the past 30 days. This is where SQL window functions come in. However, since the df data contains only records of active days, we need to explicitly extend them and include the days when a user was not active. In other words, instead of this list of records:\nuser_id    date          registration_date\n1234567    2023-01-01    2023-01-01\n1234567    2023-01-03    2023-01-01\nwe’d like to get a list like this:\nuser_id    date          is_active    registration_date\n1234567    2023-01-01    TRUE         2023-01-01\n1234567    2023-01-02    FALSE        2023-01-01\n1234567    2023-01-03    TRUE         2023-01-01\n1234567    2023-01-04    FALSE        2023-01-01\n1234567    2023-01-05    FALSE        2023-01-01\n...        ...           ...          ...\n1234567    2023-10-31    FALSE        2023-01-01\nFor readability purposes we split the following SQL query into multiple subqueries.\n\nfull_range: Create a full sequence of dates for each user.\ndau_full: Get the full list of both active and inactive records.\nstates: Assign one of the 7 states for each day of a user’s lifetime.\n\n\n\nToggle the code\nimport duckdb\n\nDATASET_START = '2020-11-01'\nDATASET_END = '2023-10-31'\nOBSERVATION_START = '2020-10-01'\n\nquery = f\"\"\"\nWITH\nfull_range AS (\n    SELECT\n        user_id, UNNEST(generate_series(greatest(registration_date, '{OBSERVATION_START}'), date '{DATASET_END}', INTERVAL 1 DAY))::date AS date\n    FROM (\n        SELECT DISTINCT user_id, registration_date FROM df\n    )\n),\ndau_full AS (\n    SELECT\n        fr.user_id,\n        fr.date,\n        df.date IS NOT NULL AS is_active,\n        registration_date\n    FROM full_range AS fr\n    LEFT JOIN df USING(user_id, date)\n),\nstates AS (\n    SELECT\n        user_id,\n        date,\n        is_active,\n        first_value(registration_date IGNORE NULLS) OVER (PARTITION BY user_id ORDER BY date) AS registration_date,\n        SUM(is_active::int) OVER (PARTITION BY user_id ORDER BY date ROWS BETWEEN 6 PRECEDING and 1 PRECEDING) AS active_days_back_6d,\n        SUM(is_active::int) OVER (PARTITION BY user_id ORDER BY date ROWS BETWEEN 29 PRECEDING and 1 PRECEDING) AS active_days_back_29d,\n        CASE\n            WHEN date = registration_date THEN 'new'\n            WHEN is_active = TRUE AND active_days_back_6d BETWEEN 1 and 6 THEN 'current'\n            WHEN is_active = TRUE AND active_days_back_6d = 0 AND IFNULL(active_days_back_29d, 0) &gt; 0 THEN 'reactivated'\n            WHEN is_active = TRUE AND active_days_back_6d = 0 AND IFNULL(active_days_back_29d, 0) = 0 THEN 'resurrected'\n            WHEN is_active = FALSE AND active_days_back_6d &gt; 0 THEN 'at_risk_wau'\n            WHEN is_active = FALSE AND active_days_back_6d = 0 AND ifnull(active_days_back_29d, 0) &gt; 0 THEN 'at_risk_mau'\n            ELSE 'dormant'\n        END AS state\n    FROM dau_full\n)\nSELECT user_id, date, state FROM states\nWHERE date BETWEEN '{DATASET_START}' AND '{DATASET_END}'\nORDER BY user_id, date\n\"\"\"\nstates = duckdb.sql(query).df()\n\n\nThe query results are kept in the states DataFrame:\n\nstates.head()\n\n\n\n\n\n\n\n\nuser_id\ndate\nstate\n\n\n\n\n0\n00002b68-adba-5a55-92d7-8ea8934c6db3\n2023-06-23\nnew\n\n\n1\n00002b68-adba-5a55-92d7-8ea8934c6db3\n2023-06-24\ncurrent\n\n\n2\n00002b68-adba-5a55-92d7-8ea8934c6db3\n2023-06-25\ncurrent\n\n\n3\n00002b68-adba-5a55-92d7-8ea8934c6db3\n2023-06-26\ncurrent\n\n\n4\n00002b68-adba-5a55-92d7-8ea8934c6db3\n2023-06-27\ncurrent\n\n\n\n\n\n\n\n\n\n3.4 Calculating the transition matrix\nHaving obtained these states, we can calculate state transition frequencies. In the Section 4.3 we’ll study how the prediction depends on a period in which transitions are considered, so it’s reasonable to pre-aggregate this data on daily basis. The resulting transitions DataFrame contains date, state_from, state_to, and cnt columns.\n\n\nToggle the code\nquery = f\"\"\"\nSELECT\n    date,\n    state_from,\n    state_to,\n    COUNT(*) AS cnt,\nFROM (\n    SELECT\n        date,\n        state AS state_to,\n        lag(state) OVER (PARTITION BY user_id ORDER BY date) AS state_from\n    FROM states\n)\nWHERE state_from IS NOT NULL\nGROUP BY date, state_from, state_to\nORDER BY date, state_from, state_to;\n\"\"\"\ntransitions = duckdb.sql(query).df()\n\n\n\ntransitions.head()\n\n\n\n\n\n\n\n\ndate\nstate_from\nstate_to\ncnt\n\n\n\n\n0\n2020-11-02\nat_risk_mau\nat_risk_mau\n273\n\n\n1\n2020-11-02\nat_risk_mau\ndormant\n4\n\n\n2\n2020-11-02\nat_risk_mau\nreactivated\n14\n\n\n3\n2020-11-02\nat_risk_wau\nat_risk_mau\n18\n\n\n4\n2020-11-02\nat_risk_wau\nat_risk_wau\n138\n\n\n\n\n\n\n\nNow, we can calculate the transition matrix \\(M\\). We implement the get_transition_matrix function, which accepts the transitions DataFrame and a pair of dates that encompass the transitions period to be considered.\n\n\nToggle the code\nstates_order = ['new', 'current', 'reactivated', 'resurrected', 'at_risk_wau', 'at_risk_mau', 'dormant']\n\ndef get_transition_matrix(transitions, date1, date2):\n    if pd.to_datetime(date1) &gt; pd.to_datetime(DATASET_END):\n        date1 = pd.to_datetime(DATASET_END) - pd.Timedelta(days=30)\n\n    probs = transitions\\\n        .loc[lambda _df: _df['date'].between(date1, date2)]\\\n        .groupby(['state_from', 'state_to'], as_index=False)\\\n        ['cnt'].sum()\\\n        .assign(\n            supp=lambda _df: _df.groupby('state_from')['cnt'].transform('sum'),\n            prob=lambda _df: _df['cnt'] / _df['supp']\n        )\n\n    M = probs.pivot(index='state_from', columns='state_to', values='prob')\\\n        .reindex(states_order, axis=0)\\\n        .reindex(states_order, axis=1)\\\n        .fillna(0)\\\n        .astype(float)\n\n    return M\n\n\nAs a baseline, let’s calculate the transition matrix for the whole year from 2022-11-01 to 2023-10-31.\n\nM = get_transition_matrix(transitions, '2022-11-01', '2023-10-31')\nM\n\n\n\n\n\n\n\nstate_to\nnew\ncurrent\nreactivated\nresurrected\nat_risk_wau\nat_risk_mau\ndormant\n\n\nstate_from\n\n\n\n\n\n\n\n\n\n\n\nnew\n0.0\n0.515934\n0.000000\n0.000000\n0.484066\n0.000000\n0.000000\n\n\ncurrent\n0.0\n0.851325\n0.000000\n0.000000\n0.148675\n0.000000\n0.000000\n\n\nreactivated\n0.0\n0.365867\n0.000000\n0.000000\n0.634133\n0.000000\n0.000000\n\n\nresurrected\n0.0\n0.316474\n0.000000\n0.000000\n0.683526\n0.000000\n0.000000\n\n\nat_risk_wau\n0.0\n0.098246\n0.004472\n0.000000\n0.766263\n0.131020\n0.000000\n\n\nat_risk_mau\n0.0\n0.000000\n0.009598\n0.000173\n0.000000\n0.950109\n0.040120\n\n\ndormant\n0.0\n0.000000\n0.000000\n0.000387\n0.000000\n0.000000\n0.999613\n\n\n\n\n\n\n\nThe sum of each row of any transition matrix equals 1 since it represents the probabilities of moving from one state to any other state.\n\n\n3.5 Getting the initial state counts\nAn initial state is retrieved from the states DataFrame by the get_state0 function and the corresponding SQL query. The only argument of the function is the date for which we want to get the initial state. We assign the result to the state0 variable.\n\n\nToggle the code\ndef get_state0(date):\n    query = f\"\"\"\n    SELECT state, count(*) AS cnt\n    FROM states\n    WHERE date = '{date}'\n    GROUP BY state\n    \"\"\"\n\n    state0 = duckdb.sql(query).df()\n    state0 = state0.set_index('state').reindex(states_order)['cnt']\n    \n    return state0\n\n\n\nstate0 = get_state0(DATASET_END)\nstate0\n\nstate\nnew               20\ncurrent          475\nreactivated       15\nresurrected       19\nat_risk_wau      404\nat_risk_mau     1024\ndormant        49523\nName: cnt, dtype: int64\n\n\n\n\n3.6 Predicting DAU\nThe predict_dau function below accepts all the previous variables required for the DAU prediction and makes this prediction for a date range defined by the start_date and end_date arguments.\n\n\nToggle the code\ndef predict_dau(M, state0, start_date, end_date, new_users):\n    \"\"\"\n    Predicts DAU over a given date range.\n\n    Parameters\n    ----------\n    M : pandas.DataFrame\n        Transition matrix representing user state changes.\n    state0 : pandas.Series\n        counts of initial state of users.\n    start_date : str\n        Start date of the prediction period in 'YYYY-MM-DD' format.\n    end_date : str\n        End date of the prediction period in 'YYYY-MM-DD' format.\n    new_users : int or pandas.Series\n        The expected amount of new users for each day between `start_date` and `end_date`.\n        If a Series, it should have dates as the index.\n        If an int, the same number is used for each day.\n        \n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame containing the predicted DAU, WAU, and MAU for each day in the date range,\n        with columns for different user states and tot.\n    \"\"\"\n    \n    dates = pd.date_range(start_date, end_date)\n    dates.name = 'date'\n    dau_pred = []\n    new_dau = state0.copy()\n    for date in dates:\n        new_dau = (M.transpose() @ new_dau).astype(int)\n        if isinstance(new_users, int):\n            new_users_today = new_users\n        else:\n            new_users_today = new_users.astype(int).loc[date] \n        new_dau.loc['new'] = new_users_today\n        dau_pred.append(new_dau.tolist())\n\n    dau_pred = pd.DataFrame(dau_pred, index=dates, columns=states_order)\n    dau_pred['dau'] = dau_pred['new'] + dau_pred['current'] + dau_pred['reactivated'] + dau_pred['resurrected']\n    dau_pred['wau'] = dau_pred['dau'] + dau_pred['at_risk_wau']\n    dau_pred['mau'] = dau_pred['dau'] + dau_pred['at_risk_wau'] + dau_pred['at_risk_mau']\n    \n    return dau_pred\n\n\n\ndau_pred = predict_dau(M, state0, PREDICTION_START, PREDICTION_END, new_users_pred)\ndau_pred\n\n\n\n\n\n\n\n\nnew\ncurrent\nreactivated\nresurrected\nat_risk_wau\nat_risk_mau\ndormant\ndau\nwau\nmau\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2023-11-01\n29\n465\n11\n19\n412\n1025\n49544\n524\n936\n1961\n\n\n2023-11-02\n25\n461\n11\n19\n418\n1027\n49565\n516\n934\n1961\n\n\n2023-11-03\n21\n456\n11\n19\n420\n1030\n49587\n507\n927\n1957\n\n\n2023-11-04\n22\n450\n11\n19\n419\n1033\n49609\n502\n921\n1954\n\n\n2023-11-05\n34\n445\n11\n19\n418\n1036\n49631\n509\n927\n1963\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2024-12-27\n52\n505\n12\n24\n487\n1129\n61599\n593\n1080\n2209\n\n\n2024-12-28\n56\n516\n13\n24\n497\n1136\n61620\n609\n1106\n2242\n\n\n2024-12-29\n71\n529\n13\n24\n509\n1144\n61641\n637\n1146\n2290\n\n\n2024-12-30\n79\n549\n13\n24\n527\n1153\n61663\n665\n1192\n2345\n\n\n2024-12-31\n74\n572\n13\n24\n548\n1164\n61685\n683\n1231\n2395\n\n\n\n\n427 rows × 10 columns\n\n\n\nThis is how the DAU prediction dau_pred looks like for the PREDICTION_START - PREDICTION_END period. Besides the expected dau, wau, and mau columns, the output contains the number of users in each state for each prediction date.\nFinally, we calculate the ground-truth values of DAU, WAU, and MAU (along with the user state counts), keep them in the dau_true DataFrame, and plot the predicted and true values altogether.\n\n\nToggle the code\nquery = f\"\"\"\nSELECT date, state, COUNT(*) AS cnt\nFROM states\nGROUP BY date, state\nORDER BY date, state;\n\"\"\"\n\ndau_true = duckdb.sql(query).df()\ndau_true['date'] = pd.to_datetime(dau_true['date'])\ndau_true = dau_true.pivot(index='date', columns='state', values='cnt')\ndau_true['dau'] = dau_true['new'] + dau_true['current'] + dau_true['reactivated'] + dau_true['resurrected']\ndau_true['wau'] = dau_true['dau'] + dau_true['at_risk_wau']\ndau_true['mau'] = dau_true['dau'] + dau_true['at_risk_wau'] + dau_true['at_risk_mau']\n\n\n\ndau_true.head()\n\n\n\n\n\n\n\nstate\nat_risk_mau\nat_risk_wau\ncurrent\ndormant\nnew\nreactivated\nresurrected\ndau\nwau\nmau\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020-11-01\n291.0\n207.0\n293.0\n840.0\n36.0\n14.0\n3.0\n346.0\n553.0\n844.0\n\n\n2020-11-02\n291.0\n208.0\n327.0\n836.0\n53.0\n14.0\n8.0\n402.0\n610.0\n901.0\n\n\n2020-11-03\n296.0\n205.0\n383.0\n840.0\n41.0\n10.0\n3.0\n437.0\n642.0\n938.0\n\n\n2020-11-04\n296.0\n246.0\n375.0\n842.0\n27.0\n13.0\n6.0\n421.0\n667.0\n963.0\n\n\n2020-11-05\n300.0\n275.0\n373.0\n845.0\n33.0\n8.0\n4.0\n418.0\n693.0\n993.0\n\n\n\n\n\n\n\n\n\nToggle the code\npd.concat([dau_true['dau'], dau_pred['dau']])\\\n    .plot(title='DAU, historical & predicted');\nplt.axvline(PREDICTION_START, color='k', linestyle='--');\n\n\n\n\n\n\n\n\n\nWe’ve obtained the prediction but so far it’s not clear whether it’s fair or not. In the next section, we’ll evaluate the model."
  },
  {
    "objectID": "posts/2024-12-02_dau_prediction/dau_prediction.html#model-evaluation",
    "href": "posts/2024-12-02_dau_prediction/dau_prediction.html#model-evaluation",
    "title": "Modeling DAU with Markov chain",
    "section": "4 Model evaluation",
    "text": "4 Model evaluation\n\n4.1 Baseline comparison\nFirst of all, let’s check whether we really need to build a complex model to predict DAU. Wouldn’t it be better to predict DAU as a general time-series using the mentioned prophet library? The function predict_dau_simple implements this. We try to use some tweaks available in the library in order to make the prediction more accurate:\n\nwe use logistic model instead of linear in order to avoid negative values;\nwe add explicitly monthly and yearly seasonality;\nwe remove the outliers.\nwe set explicitly define peak values for January and February as “holidays”.\n\nThe fact that the code turns out to be quite sophisticated indicates that one can’t simply apply prophet to the DAU time-series.\n\n\nToggle the code\ndef predict_dau_prophet(prediction_start, prediction_end, dau_true, show_plot=True):\n    # assigning peak days for the new year\n    holidays = pd.DataFrame({\n        'holiday': 'january_spike',\n        'ds': pd.date_range('2022-01-01', '2022-01-31', freq='D').tolist() + \\\n              pd.date_range('2023-01-01', '2023-01-31', freq='D').tolist(),\n        'lower_window': 0,\n        'upper_window': 40\n    })\n\n    m = Prophet(growth='logistic', holidays=holidays)\n    m.add_seasonality(name='monthly', period=30.5, fourier_order=3)\n    m.add_seasonality(name='yearly', period=365, fourier_order=3)\n\n    train = dau_true.loc[(dau_true.index &lt; prediction_start) & (dau_true.index &gt;= '2021-08-01')]\n    train_prophet = pd.DataFrame({'ds': train.index, 'y': train.values})\n    # removining outliers\n    train_prophet.loc[train_prophet['ds'].between('2022-06-07', '2022-06-09'), 'y'] = None\n    train_prophet['new_year_peak'] = (train_prophet['ds'] &gt;= '2022-01-01') &\\\n                                     (train_prophet['ds'] &lt;= '2022-02-14')\n    m.add_regressor('new_year_peak')\n    # setting logistic upper and lower bounds\n    train_prophet['cap'] = dau_true.max() * 1.1\n    train_prophet['floor'] = 0\n\n    m.fit(train_prophet)\n\n    periods = len(pd.date_range(prediction_start, prediction_end))\n    future = m.make_future_dataframe(periods=periods)\n    future['new_year_peak'] = (future['ds'] &gt;= '2022-01-01') & (future['ds'] &lt;= '2022-02-14')\n    future['cap'] = dau_true.max() * 1.1\n    future['floor'] = 0\n    pred = m.predict(future)\n\n    if show_plot:\n        m.plot(pred);\n\n    # converting the predictions to an appropriate format\n    pred = pred\\\n        .assign(yhat=lambda _df: _df['yhat'].astype(int))\\\n        .rename(columns={'ds': 'date', 'yhat': 'count'})\\\n        .set_index('date')\\\n        .clip(lower=0)\\\n        ['count']\\\n        .loc[lambda s: (s.index &gt;= prediction_start) & (s.index &lt;= prediction_end)]\n\n    return pred\n\n\nHereafter we test a prediction for multiple predicting horizonts: 3, 6, and 12 months. As a result, we get 3 test sets:\n\n3-months horizont: 2023-08-01 - 2023-10-31,\n6-months horizont: 2023-05-01 - 2023-10-31,\n1-year horizont: 2022-11-01 - 2023-10-31.\n\nFor each test set we calculate the MAPE loss function.\n\n\nToggle the code\nfrom sklearn.metrics import mean_absolute_percentage_error\n\nmapes = []\nprediction_end = '2023-10-31'\nprediction_horizont = [3, 6, 12]\n\nfor offset in prediction_horizont:\n    prediction_start = pd.to_datetime(prediction_end) - pd.DateOffset(months=offset - 1)\n    prediction_start = prediction_start.replace(day=1)\n    prediction_end = '2023-10-31'\n    pred = predict_dau_prophet(prediction_start, prediction_end, dau_true['dau'], show_plot=False)\n    mape = mean_absolute_percentage_error(dau_true['dau'].reindex(pred.index), pred)\n    mapes.append(mape)\n\nmapes = pd.DataFrame({'horizont': prediction_horizont, 'MAPE': mapes})\nmapes\n\n\n\n\n\n\n\n\n\nhorizont\nMAPE\n\n\n\n\n0\n3\n0.350167\n\n\n1\n6\n0.185246\n\n\n2\n12\n0.215338\n\n\n\n\n\n\n\nThe MAPE error turns out to be high: 18% - 35%. The fact that the shortest horizont has the highest error means that the model is tuned for the long-term predictions. This is another inconvenience of such an approach: we have to tune the model for each prediction horizont.\nAnyway, this is our baseline. In the next section we’ll compare it with more advanced models.\n\n\n4.2 General evaluation\nIn this section we evaluate the model implemented in the section 3. So far we set the transition period as 1 year before the prediction start. We’ll study how the prediction depends on the transition period in the next section. As for the new users, we run the model using two options: the real values and the predicted ones. Similarly, we fix the same 3 prediction horizonts and test the model on them.\nThe make_predicion helper function below implements the described opttions. It accepts prediction_start, prediction_end arguments defining the prediction period for a given horizont, new_users_mode which can be either true or predict, and transition_period. The options of the latter argument will be explained further.\n\n\nToggle the code\nimport re\n\n\ndef make_prediction(prediction_start, prediction_end, new_users_mode='predict', transition_period='last_30d'):\n    prediction_start_minus_1d = pd.to_datetime(prediction_start) - pd.Timedelta('1d')\n    state0 = get_state0(prediction_start_minus_1d)\n    \n    if new_users_mode == 'predict':\n        new_users_pred = predict_new_users(prediction_start, prediction_end, new_users, show_plot=False)\n    elif new_users_mode == 'true':\n        new_users_pred = new_users.copy()\n\n    if transition_period.startswith('last_'):\n        shift = int(re.search(r'last_(\\d+)d', transition_period).group(1))\n        transitions_start = pd.to_datetime(prediction_start) - pd.Timedelta(shift, 'd')\n        M = get_transition_matrix(transitions, transitions_start, prediction_start_minus_1d)\n        dau_pred = predict_dau(M, state0, prediction_start, prediction_end, new_users_pred)\n    else:\n        transitions_start = pd.to_datetime(prediction_start) - pd.Timedelta(240, 'd')\n        M_base = get_transition_matrix(transitions, transitions_start, prediction_start_minus_1d)\n        dau_pred = pd.DataFrame()\n\n        month_starts = pd.date_range(prediction_start, prediction_end, freq='1MS')\n        N = len(month_starts)\n\n        for i, prediction_month_start in enumerate(month_starts):\n            prediction_month_end = pd.offsets.MonthEnd().rollforward(prediction_month_start)\n            transitions_month_start = prediction_month_start - pd.Timedelta('365D')\n            transitions_month_end = prediction_month_end - pd.Timedelta('365D')\n\n            M_seasonal = get_transition_matrix(transitions, transitions_month_start, transitions_month_end)\n            if transition_period == 'smoothing':\n                i = min(i, 12)\n                M = M_seasonal * i / (N - 1)  + (1 - i / (N - 1)) * M_base\n            elif transition_period.startswith('seasonal_'):\n                seasonal_coef = float(re.search(r'seasonal_(0\\.\\d+)', transition_period).group(1))\n                M = seasonal_coef * M_seasonal + (1 - seasonal_coef) * M_base\n            \n            dau_tmp = predict_dau(M, state0, prediction_month_start, prediction_month_end, new_users_pred)\n            dau_pred = pd.concat([dau_pred, dau_tmp])\n\n            state0 = dau_tmp.loc[prediction_month_end][states_order]\n\n    return dau_pred\n\ndef prediction_details(dau_true, dau_pred, show_plot=True, ax=None):\n    y_true = dau_true.reindex(dau_pred.index)['dau']\n    y_pred = dau_pred['dau']\n    mape = mean_absolute_percentage_error(y_true, y_pred) \n\n    if show_plot:\n        prediction_start = str(y_true.index.min().date())\n        prediction_end = str(y_true.index.max().date())\n        if ax is None:\n            y_true.plot(label='DAU true')\n            y_pred.plot(label='DAU pred')\n            plt.title(f'DAU prediction, {prediction_start} - {prediction_end}')\n            plt.legend()\n        else:\n            y_true.plot(label='DAU true', ax=ax)\n            y_pred.plot(label='DAU pred', ax=ax)\n            ax.set_title(f'DAU prediction, {prediction_start} - {prediction_end}')\n            ax.legend()\n    return mape\n\n\nIn total, we have 6 prediction scenarios: 2 options for new users and 3 prediction horizonts. The diagram below illustrate the results. The charts on the left relate to the new_users_mode = 'true' option, while the right ones relate to the new_users_mode = 'predict' option.\n\n\nToggle the code\nfig, axs = plt.subplots(3, 2, figsize=(15, 6))\nmapes = []\nprediction_end = '2023-10-31'\nprediction_horizont = [3, 6, 12]\n\nfor i, offset in enumerate(prediction_horizont):\n    prediction_start = pd.to_datetime(prediction_end) - pd.DateOffset(months=offset - 1)\n    prediction_start = prediction_start.replace(day=1)\n    args = {\n        'prediction_start': prediction_start,\n        'prediction_end': prediction_end,\n        'transition_period': 'last_365d'\n    }\n    for j, new_users_mode in enumerate(['predict', 'true']):\n        args['new_users_mode'] = new_users_mode\n        dau_pred = make_prediction(**args)\n        mape = prediction_details(dau_true, dau_pred, ax=axs[i, j])\n        mapes.append([offset, new_users_mode, mape])\n\nmapes = pd.DataFrame(mapes, columns=['horizont', 'new_users', 'MAPE'])\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nAnd here are the MAPE values summarizing the prediction quality:\n\n\nToggle the code\nmapes.pivot(index='horizont', columns='new_users', values='MAPE')\n\n\n\n\n\n\n\n\nnew_users\npredict\ntrue\n\n\nhorizont\n\n\n\n\n\n\n3\n0.079538\n0.103631\n\n\n6\n0.125035\n0.084652\n\n\n12\n0.651502\n0.086885\n\n\n\n\n\n\n\nWe notice multiple things.\n\nIn general, the model demonstrates much better results than the baseline. Indeed, the baseline is based on the historical DAU data only, while the model uses the user states information.\nHowever, for the 1-year horizont and new_users_mode='predict' the MAPE error is huge: 65%. This is 3 times higher than the corresponding baseline error (21%). On the other hand, new_users_mode='true' option gives a much better result: 8%. It means that the new users prediction has a huge impact on the model, especially for long-term predictions. For the shorter periods the difference is less dramatic. The major reason for such a difference is that 1-year period includes Christmas with its extreme values. As a result, i) it’s hard to predict such high new user values, ii) the period heavily impacts user behavior, the transition matrix and, consequently, DAU values. Hence, we strongly recommend to implement the new user prediction carefully. The baseline model was specially tuned for this Christmas period, so it’s not surprising that it outperforms the Markov model.\nWhen the new users prediction is accurate, the model captures trends well. It means that using last 365 days for the transition matrix calculation is a reasonable choice.\nInterestingly, the true new users data provides worse results for the 3-months prediction. This is nothing but a coincidence. The wrong new users prediction in October 2023 reversed the predicted DAU trend and made MAPE a bit lower.\n\nNow, let’s decompose the prediction error and see which states contribure the most. By error we mean here dau_pred - dau_true values, by relative error – (dau_pred - dau_true) / dau_true – see left and right diagrams below correspondingly. In order to focus on this aspect, we’ll narrow the configuration to the 3-months prediction horizont and the new_users_mode='true' option.\n\n\nToggle the code\ndau_component_cols = ['new', 'current', 'reactivated', 'resurrected']\n\ndau_pred = make_prediction('2023-08-01', '2023-10-31', new_users_mode='true', transition_period='last_365d')\nfigure, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\ndau_pred[dau_component_cols]\\\n    .subtract(dau_true[dau_component_cols])\\\n    .reindex(dau_pred.index)\\\n    .plot(title='Prediction error by state', ax=ax1)\n\ndau_pred[['current']]\\\n    .subtract(dau_true[['current']])\\\n    .div(dau_true[['current']])\\\n    .reindex(dau_pred.index)\\\n    .plot(title='Relative prediction error (current state)', ax=ax2);\n\n\n\n\n\n\n\n\n\nFrom the left chart we notice that the error is basically contributed by the current state. It’s not surprising since this state contributes to DAU the most. The error for the reactivated, and resurrected states is quite low. Another interesting thing is that this error is mostly negative for the current state and mostly positive for the resurrected state. The former might be explained by the fact that the new users who appeared in the prediction period are more engaged that the users from the past. The latter indicates that the resurrected users in reality contribute to DAU less than the transition matrix expects.\nAs for the relative error on the right chart, it makes sense to analyze it for the current state only. This is because the daily amount of the reactivated and resurrected states are low so the relative error is high and noisy. The relative error for the current state varies between -25% and 4% which is quite high. And since we’ve fixed the new users prediction, this error is explained by the transition matrix inaccuracy only. In particular, the current -&gt; current conversion rate is roughly 0.8 which is high and, as a result, it contributes to the error a lot. So if we want to improve the prediction we need to consider tuning primarily this conversion rate.\n\n\n4.3 Transitions period impact\nIn the previous section we kept the transitions period fixed – 1 year before a prediction start. Now we’re going to study how long this period should be to get more accurate prediction. We consider the same prediction horizonts of 3, 6, and 12 months. In order to mitigate the noise from the new users prediction, we use the real values of the new users amount: new_users_mode='true'.\nHere comes varying of the transition_period argument. Its values are masked with the last_&lt;N&gt;d pattern where N stands for the number of days in a transitions period. For each prediction horizont we calculate 12 different transition periods of 1, 2, …, 12 months. Then we calculate the MAPE error for each of the option and plot the results.\n\n\nToggle the code\nresult = []\n\nfor prediction_offset in prediction_horizont:\n    prediction_start = pd.to_datetime(prediction_end) - pd.DateOffset(months=prediction_offset - 1)\n    prediction_start = prediction_start.replace(day=1)\n\n    for transition_offset in range(1, 13):\n        dau_pred = make_prediction(\n            prediction_start, prediction_end, new_users_mode='true',\n            transition_period=f'last_{transition_offset*30}d'\n        )\n        mape = prediction_details(dau_true, dau_pred, show_plot=False)\n        result.append([prediction_offset, transition_offset, mape])\nresult = pd.DataFrame(result, columns=['prediction_period', 'transition_period', 'mape'])\n\nresult.pivot(index='transition_period', columns='prediction_period', values='mape')\\\n    .plot(title='MAPE by prediction and transition period');\n\n\n\n\n\n\n\n\n\nIt turns out that the optimal transitions period depends on the prediction horizont. Shorter horizonts require shorter transitions periods: the minimal MAPE error is achieved at 1, 4, and 8 transition periods for the 3, 6, and 12 months correspondingly. Apparently, this is because the longer horizonts contain some seasonal effects that could be captured only by the longer transitions periods. Also, it seems that for the longer prediction horizonts the MAPE curve is U-shaped meaning that too long and too short transitions periods are both not good for the prediction. We’ll develop this idea in the next section.\n\n\n4.4 Obsolence and seasonality\nNevertheless, fixing a single transition matrix for predicting the whole year ahead doesn’t seem to be a good idea: such a model would be too rigid. Usually, the user behavior varies depending on a season. For example, users who appear after Christmas might have some shifts in behavior. Another typical situation is when users change their behavior in summer. In this section, we’ll try to take into account these seasonal effects.\nSo we want to predict DAU for the 1 year ahead starting from November 2022. Instead of using a single transition matrix \\(M_{base}\\) which is calculated for the last 8 months before the prediction start, according to the previous subsection results (and labeled as the last_240d option below), we’ll consider a mixture of this matrix and a seasonal one \\(M_{seasonal}\\). The latter is calculated on monthly basis lagging 1 year behind. For example, to predict DAU for November 2022 we define \\(M_{seasonal}\\) as the transition matrix for November 2021. Then we shift the prediction horizont to December 2022 and calculate \\(M_{seasonal}\\) for December 2021, etc.\nIn order to mix \\(M_{base}\\) and \\(M_{seasonal}\\) we define the following two options.\n\nseasonal_0.3: \\(M = 0.3 \\cdot M_{seasonal} + 0.7 \\cdot M_{base}\\). 0.3 is a weight that was chosen as a local minimum after some experiments.\nsmoothing: \\(M = \\frac{i}{N - 1} M_{seasonal} + (1 - \\frac{i}{N - 1}) M_{base}\\) where \\(N\\) is the number of months within the predicting period, \\(i = 0, \\ldots, N - 1\\) – the month index. The idea of this configuration is to gradually switch from the most recent transition matrix \\(M_{base}\\) to seasonal ones as the prediction month moves forward from the prediction start.\n\n\n\nToggle the code\nresult = pd.DataFrame()\nfor transition_period in ['last_240d', 'seasonal_0.3', 'smoothing']:\n    result[transition_period] = make_prediction('2022-11-01', '2023-10-31', 'true', transition_period)['dau']\nresult['true'] = dau_true['dau']\nresult['true'] = result['true'].astype(int)\nresult.plot(title='DAU prediction by different transition matrices');\n\n\n\n\n\n\n\n\n\n\n\nToggle the code\nmape = pd.DataFrame()\nfor col in result.columns:\n    if col != 'true':\n        mape.loc[col, 'mape'] = mean_absolute_percentage_error(result['true'], result[col])\nmape\n\n\n\n\n\n\n\n\n\nmape\n\n\n\n\nlast_240d\n0.080804\n\n\nseasonal_0.3\n0.077545\n\n\nsmoothing\n0.097802\n\n\n\n\n\n\n\nAccording to the MAPE errors, seasonal_0.3 configuration provides the best results. Interestingly, smoothing approach has appeared to be even worse than the last_240d. From the diagram above we see that all three models start to underestimate the DAU values from July 2023, especially the smoothing model. It seems that the new users who started appearing from July 2023 are more engaged than the users from 2022. Probably, the app was improved sufficiently or the marketing team did a great job. As a result, the smoothing model that much relies on the outdated transitions data from July 2022 - October 2022 fails more than the other models.\n\n\n4.5 Final solution\nTo sum things up, let’s make a final prediction for the 2024 year. We use the seasonal_0.3 configuration and the predicted values for new users.\n\n\nToggle the code\ndau_pred = make_prediction(\n    PREDICTION_START, PREDICTION_END,\n    new_users_mode='predict',\n    transition_period='seasonal_0.3'\n)\ndau_true['dau'].plot(label='true')\ndau_pred['dau'].plot(label='seasonal_0.3')\nplt.title('DAU, historical & predicted')\nplt.axvline(PREDICTION_START, color='k', linestyle='--')\nplt.legend();"
  },
  {
    "objectID": "posts/2024-12-02_dau_prediction/dau_prediction.html#discussion",
    "href": "posts/2024-12-02_dau_prediction/dau_prediction.html#discussion",
    "title": "Modeling DAU with Markov chain",
    "section": "5 Discussion",
    "text": "5 Discussion\nIn the Section 4 we studied the model performance from the prediction accuracy perspective. Now let’s discuss the model from the practical point of view.\nBesides poor accuracy, predicting DAU as a time-series (see the Section 4.1) makes this approach very stiff. Essentially, it makes a prediction in such a manner so it would fit historical data best. In practice, when making plans for a next year we usually have some certain expectations about the future. For example,\n\nthe marketing team is going to launch some new more effective campaings,\nthe activation team is planning to improve the onboarding process,\nthe product team will release some new features that would engage and retain users more.\n\nOur model can take into account such expectations. For the examples above we can adjust the new users prediction, the new→current and the current→current conversion rates respectively. As a result, we can get a prediction that doesn’t match with the historical data but nevertheless would be more realistic. This model’s property is not just flexible – it’s interpretable. You can easily discuss all these adjustments with the stakeholders, and they can understand how the prediction works.\nAnother advantage of the model is that it doesn’t require predicting whether a certain user will be active on a certain day. Sometimes binary classifiers are used for this purpose. The downside of this approach is that we need to apply such a classifier to each user including all the dormant users and each day from a prediction horizon. This is a tremedous computational cost. In contrast, the Markov model requires only the initial amount of states (state0). Moreover, such classiffiers are often black-box models: they are poorly interpretable and hard to adjust.\nThe Markov model also has some limitations. As we already have seen, it’s sensitive to the new users prediction. It’s easy to totally ruin the prediction by a wrong new users amount. Another problem is that the Markov model is memoryless meaning that it doesn’t take into account the user’s history. For example, it doesn’t distinguish whether a current user is a newbie, experienced, or reactivated/resurrected one. The retention rate of these user types should be certainly different. Also, as we discussed earlier, the user behavior might be of different nature depending on the season, marketing sources, countries, etc. So far our model is not able to capture these differences. However, this might be a subject for further research: we could extend the model by fitting more transition matrices for different user segments.\nFinally, as we promised in the introduction, we provide a DAU spreadsheet calculator. In the Prediction sheet you’ll need to fill the initial states distribution row (marked with blue) and the new users prediction column (marked with purple). In the Conversions sheet you can adjust the transition matrix values. Remember that the sum of each row of the matrix should be equal to 1.\n\n\n\nA screenshot of the DAU spreadsheet calculator\n\n\nThat’s all for now. I hope that this article was useful for you. In case of any questions or suggestions, feel free to ask in the comments below or contact me directly on LinkedIn."
  },
  {
    "objectID": "posts/2024-12-02_dau_prediction/dau_prediction.html#sec-model-evaluation",
    "href": "posts/2024-12-02_dau_prediction/dau_prediction.html#sec-model-evaluation",
    "title": "Modeling DAU with Markov chain",
    "section": "4 Model evaluation",
    "text": "4 Model evaluation\n\n4.1 Baseline model\nFirst of all, let’s check whether we really need to build a complex model to predict DAU. Wouldn’t it be better to predict DAU as a general time-series using the mentioned prophet library? The function predict_dau_prophet below implements this. We try to use some tweaks available in the library in order to make the prediction more accurate. In particular:\n\nwe use logistic model instead of linear to avoid negative values;\nwe add explicitly monthly and yearly seasonality;\nwe remove the outliers;\nwe explicitly define a peak period in January and February as “holidays”.\n\n\n\nToggle the code\ndef predict_dau_prophet(prediction_start, prediction_end, dau_true, show_plot=True):\n    # assigning peak days for the new year\n    holidays = pd.DataFrame({\n        'holiday': 'january_spike',\n        'ds': pd.date_range('2022-01-01', '2022-01-31', freq='D').tolist() + \\\n              pd.date_range('2023-01-01', '2023-01-31', freq='D').tolist(),\n        'lower_window': 0,\n        'upper_window': 40\n    })\n\n    m = Prophet(growth='logistic', holidays=holidays)\n    m.add_seasonality(name='monthly', period=30.5, fourier_order=3)\n    m.add_seasonality(name='yearly', period=365, fourier_order=3)\n\n    train = dau_true.loc[(dau_true.index &lt; prediction_start) & (dau_true.index &gt;= '2021-08-01')]\n    train_prophet = pd.DataFrame({'ds': train.index, 'y': train.values})\n    # removining outliers\n    train_prophet.loc[train_prophet['ds'].between('2022-06-07', '2022-06-09'), 'y'] = None\n    train_prophet['new_year_peak'] = (train_prophet['ds'] &gt;= '2022-01-01') &\\\n                                     (train_prophet['ds'] &lt;= '2022-02-14')\n    m.add_regressor('new_year_peak')\n    # setting logistic upper and lower bounds\n    train_prophet['cap'] = dau_true.max() * 1.1\n    train_prophet['floor'] = 0\n\n    m.fit(train_prophet)\n\n    periods = len(pd.date_range(prediction_start, prediction_end))\n    future = m.make_future_dataframe(periods=periods)\n    future['new_year_peak'] = (future['ds'] &gt;= '2022-01-01') & (future['ds'] &lt;= '2022-02-14')\n    future['cap'] = dau_true.max() * 1.1\n    future['floor'] = 0\n    pred = m.predict(future)\n\n    if show_plot:\n        m.plot(pred);\n\n    # converting the predictions to an appropriate format\n    pred = pred\\\n        .assign(yhat=lambda _df: _df['yhat'].astype(int))\\\n        .rename(columns={'ds': 'date', 'yhat': 'count'})\\\n        .set_index('date')\\\n        .clip(lower=0)\\\n        ['count']\\\n        .loc[lambda s: (s.index &gt;= prediction_start) & (s.index &lt;= prediction_end)]\n\n    return pred\n\n\nThe fact that the code turns out to be quite sophisticated indicates that one can’t simply apply prophet to the DAU time-series.\nHereafter we test a prediction for multiple predicting horizons: 3, 6, and 12 months. As a result, we get 3 test sets:\n\n3-months horizon: 2023-08-01 - 2023-10-31,\n6-months horizon: 2023-05-01 - 2023-10-31,\n1-year horizon: 2022-11-01 - 2023-10-31.\n\nFor each test set we calculate the MAPE loss function.\n\n\nToggle the code\nfrom sklearn.metrics import mean_absolute_percentage_error\n\nmapes = []\nprediction_end = '2023-10-31'\nprediction_horizon = [3, 6, 12]\n\nfor offset in prediction_horizon:\n    prediction_start = pd.to_datetime(prediction_end) - pd.DateOffset(months=offset - 1)\n    prediction_start = prediction_start.replace(day=1)\n    prediction_end = '2023-10-31'\n    pred = predict_dau_prophet(prediction_start, prediction_end, dau_true['dau'], show_plot=False)\n    mape = mean_absolute_percentage_error(dau_true['dau'].reindex(pred.index), pred)\n    mapes.append(mape)\n\nmapes = pd.DataFrame({'horizon': prediction_horizon, 'MAPE': mapes})\nmapes\n\n\n\n\n\n\n\n\n\nhorizon\nMAPE\n\n\n\n\n0\n3\n0.350167\n\n\n1\n6\n0.185246\n\n\n2\n12\n0.215338\n\n\n\n\n\n\n\nThe MAPE error turns out to be high: 18% - 35%. The fact that the shortest horizon has the highest error means that the model is tuned for the long-term predictions. This is another inconvenience of such an approach: we have to tune the model for each prediction horizon. Anyway, this is our baseline. In the next section we’ll compare it with more advanced models.\n\n\n4.2 General evaluation\nIn this section we evaluate the model implemented in the Section 3.6. So far we set the transition period as 1 year before the prediction start. We’ll study how the prediction depends on the transition period in the Section 4.3. As for the new users, we run the model using two options: the real values and the predicted ones. Similarly, we fix the same 3 prediction horizon and test the model on them.\nThe make_predicion helper function below implements the described options. It accepts prediction_start, prediction_end arguments defining the prediction period for a given horizon, new_users_mode which can be either true or predict, and transition_period. The options of the latter argument will be explained further.\n\n\nToggle the code\nimport re\n\n\ndef make_prediction(prediction_start, prediction_end, new_users_mode='predict', transition_period='last_30d'):\n    prediction_start_minus_1d = pd.to_datetime(prediction_start) - pd.Timedelta('1d')\n    state0 = get_state0(prediction_start_minus_1d)\n    \n    if new_users_mode == 'predict':\n        new_users_pred = predict_new_users(prediction_start, prediction_end, new_users, show_plot=False)\n    elif new_users_mode == 'true':\n        new_users_pred = new_users.copy()\n\n    if transition_period.startswith('last_'):\n        shift = int(re.search(r'last_(\\d+)d', transition_period).group(1))\n        transitions_start = pd.to_datetime(prediction_start) - pd.Timedelta(shift, 'd')\n        M = get_transition_matrix(transitions, transitions_start, prediction_start_minus_1d)\n        dau_pred = predict_dau(M, state0, prediction_start, prediction_end, new_users_pred)\n    else:\n        transitions_start = pd.to_datetime(prediction_start) - pd.Timedelta(240, 'd')\n        M_base = get_transition_matrix(transitions, transitions_start, prediction_start_minus_1d)\n        dau_pred = pd.DataFrame()\n\n        month_starts = pd.date_range(prediction_start, prediction_end, freq='1MS')\n        N = len(month_starts)\n\n        for i, prediction_month_start in enumerate(month_starts):\n            prediction_month_end = pd.offsets.MonthEnd().rollforward(prediction_month_start)\n            transitions_month_start = prediction_month_start - pd.Timedelta('365D')\n            transitions_month_end = prediction_month_end - pd.Timedelta('365D')\n\n            M_seasonal = get_transition_matrix(transitions, transitions_month_start, transitions_month_end)\n            if transition_period == 'smoothing':\n                i = min(i, 12)\n                M = M_seasonal * i / (N - 1)  + (1 - i / (N - 1)) * M_base\n            elif transition_period.startswith('seasonal_'):\n                seasonal_coef = float(re.search(r'seasonal_(0\\.\\d+)', transition_period).group(1))\n                M = seasonal_coef * M_seasonal + (1 - seasonal_coef) * M_base\n            \n            dau_tmp = predict_dau(M, state0, prediction_month_start, prediction_month_end, new_users_pred)\n            dau_pred = pd.concat([dau_pred, dau_tmp])\n\n            state0 = dau_tmp.loc[prediction_month_end][states_order]\n\n    return dau_pred\n\ndef prediction_details(dau_true, dau_pred, show_plot=True, ax=None):\n    y_true = dau_true.reindex(dau_pred.index)['dau']\n    y_pred = dau_pred['dau']\n    mape = mean_absolute_percentage_error(y_true, y_pred) \n\n    if show_plot:\n        prediction_start = str(y_true.index.min().date())\n        prediction_end = str(y_true.index.max().date())\n        if ax is None:\n            y_true.plot(label='DAU true')\n            y_pred.plot(label='DAU pred')\n            plt.title(f'DAU prediction, {prediction_start} - {prediction_end}')\n            plt.legend()\n        else:\n            y_true.plot(label='DAU true', ax=ax)\n            y_pred.plot(label='DAU pred', ax=ax)\n            ax.set_title(f'DAU prediction, {prediction_start} - {prediction_end}')\n            ax.legend()\n    return mape\n\n\nIn total, we have 6 prediction scenarios: 2 options for new users and 3 prediction horizons. The diagram below illustrates the results. The charts on the left relate to the new_users_mode = 'predict' option, while the right ones relate to the new_users_mode = 'true' option.\n\n\nToggle the code\nfig, axs = plt.subplots(3, 2, figsize=(15, 6))\nmapes = []\nprediction_end = '2023-10-31'\nprediction_horizon = [3, 6, 12]\n\nfor i, offset in enumerate(prediction_horizon):\n    prediction_start = pd.to_datetime(prediction_end) - pd.DateOffset(months=offset - 1)\n    prediction_start = prediction_start.replace(day=1)\n    args = {\n        'prediction_start': prediction_start,\n        'prediction_end': prediction_end,\n        'transition_period': 'last_365d'\n    }\n    for j, new_users_mode in enumerate(['predict', 'true']):\n        args['new_users_mode'] = new_users_mode\n        dau_pred = make_prediction(**args)\n        mape = prediction_details(dau_true, dau_pred, ax=axs[i, j])\n        mapes.append([offset, new_users_mode, mape])\n\nmapes = pd.DataFrame(mapes, columns=['horizon', 'new_users', 'MAPE'])\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nAnd here are the MAPE values summarizing the prediction quality:\n\n\nToggle the code\nmapes.pivot(index='horizon', columns='new_users', values='MAPE')\n\n\n\n\n\n\n\n\nnew_users\npredict\ntrue\n\n\nhorizon\n\n\n\n\n\n\n3\n0.079538\n0.103631\n\n\n6\n0.125035\n0.084652\n\n\n12\n0.651502\n0.086885\n\n\n\n\n\n\n\nWe notice multiple things.\n\nIn general, the model demonstrates much better results than the baseline. Indeed, the baseline is based on the historical DAU data only, while the model uses the user states information.\nHowever, for the 1-year horizon and new_users_mode='predict' the MAPE error is huge: 65%. This is 3 times higher than the corresponding baseline error (21%). On the other hand, new_users_mode='true' option gives a much better result: 8%. It means that the new users prediction has a huge impact on the model, especially for long-term predictions. For the shorter periods the difference is less dramatic. The major reason for such a difference is that 1-year period includes Christmas with its extreme values. As a result, i) it’s hard to predict such high new user values, ii) the period heavily impacts user behavior, the transition matrix and, consequently, DAU values. Hence, we strongly recommend to implement the new user prediction carefully. The baseline model was specially tuned for this Christmas period, so it’s not surprising that it outperforms the Markov model.\nWhen the new users prediction is accurate, the model captures trends well. It means that using last 365 days for the transition matrix calculation is a reasonable choice.\nInterestingly, the true new users data provides worse results for the 3-months prediction. This is nothing but a coincidence. The wrong new users prediction in October 2023 reversed the predicted DAU trend and made MAPE a bit lower.\n\nNow, let’s decompose the prediction error and see which states contribure the most. By error we mean here dau_pred - dau_true values, by relative error – (dau_pred - dau_true) / dau_true – see left and right diagrams below correspondingly. In order to focus on this aspect, we’ll narrow the configuration to the 3-months prediction horizon and the new_users_mode='true' option.\n\n\nToggle the code\ndau_component_cols = ['new', 'current', 'reactivated', 'resurrected']\n\ndau_pred = make_prediction('2023-08-01', '2023-10-31', new_users_mode='true', transition_period='last_365d')\nfigure, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\ndau_pred[dau_component_cols]\\\n    .subtract(dau_true[dau_component_cols])\\\n    .reindex(dau_pred.index)\\\n    .plot(title='Prediction error by state', ax=ax1)\n\ndau_pred[['current']]\\\n    .subtract(dau_true[['current']])\\\n    .div(dau_true[['current']])\\\n    .reindex(dau_pred.index)\\\n    .plot(title='Relative prediction error (current state)', ax=ax2);\n\n\n\n\n\n\n\n\n\nFrom the left chart we notice that the error is basically contributed by the current state. It’s not surprising since this state contributes to DAU the most. The error for the reactivated, and resurrected states is quite low. Another interesting thing is that this error is mostly negative for the current state and mostly positive for the resurrected state. The former might be explained by the fact that the new users who appeared in the prediction period are more engaged that the users from the past. The latter indicates that the resurrected users in reality contribute to DAU less than the transition matrix expects, so the dormant→resurrected conversion rate is overestimated.\nAs for the relative error, it makes sense to analyze it for the current state only. This is because the daily amount of the reactivated and resurrected states are low so the relative error is high and noisy. The relative error for the current state varies between -25% and 4% which is quite high. And since we’ve fixed the new users prediction, this error is explained by the transition matrix inaccuracy only. In particular, the current→current conversion rate is roughly 0.8 which is high and, as a result, it contributes to the error a lot. So if we want to improve the prediction we need to consider tuning this conversion rate foremost.\n\n\n4.3 Transitions period impact\nIn the previous section we kept the transitions period fixed: 1 year before a prediction start. Now we’re going to study how long this period should be to get more accurate prediction. We consider the same prediction horizon of 3, 6, and 12 months. In order to mitigate the noise from the new users prediction, we use the real values of the new users amount: new_users_mode='true'.\nHere comes varying of the transition_period argument. Its values are masked with the last_&lt;N&gt;d pattern where N stands for the number of days in a transitions period. For each prediction horizon we calculate 12 different transition periods of 1, 2, …, 12 months. Then we calculate the MAPE error for each of the options and plot the results.\n\n\nToggle the code\nresult = []\n\nfor prediction_offset in prediction_horizon:\n    prediction_start = pd.to_datetime(prediction_end) - pd.DateOffset(months=prediction_offset - 1)\n    prediction_start = prediction_start.replace(day=1)\n\n    for transition_offset in range(1, 13):\n        dau_pred = make_prediction(\n            prediction_start, prediction_end, new_users_mode='true',\n            transition_period=f'last_{transition_offset*30}d'\n        )\n        mape = prediction_details(dau_true, dau_pred, show_plot=False)\n        result.append([prediction_offset, transition_offset, mape])\nresult = pd.DataFrame(result, columns=['prediction_period', 'transition_period', 'mape'])\n\nresult.pivot(index='transition_period', columns='prediction_period', values='mape')\\\n    .plot(title='MAPE by prediction and transition period');\n\n\n\n\n\n\n\n\n\nIt turns out that the optimal transitions period depends on the prediction horizon. Shorter horizons require shorter transitions periods: the minimal MAPE error is achieved at 1, 4, and 8 transition periods for the 3, 6, and 12 months correspondingly. Apparently, this is because the longer horizons contain some seasonal effects that could be captured only by the longer transitions periods. Also, it seems that for the longer prediction horizons the MAPE curve is U-shaped meaning that too long and too short transitions periods are both not good for the prediction. We’ll develop this idea in the next section.\n\n\n4.4 Obsolence and seasonality\nNevertheless, fixing a single transition matrix for predicting the whole year ahead doesn’t seem to be a good idea: such a model would be too rigid. Usually, user behavior varies depending on a season. For example, users who appear after Christmas might have some shifts in behavior. Another typical situation is when users change their behavior in summer. In this section, we’ll try to take into account these seasonal effects.\nSo we want to predict DAU for 1 year ahead starting from November 2022. Instead of using a single transition matrix \\(M_{base}\\) which is calculated for the last 8 months before the prediction start, according to the previous subsection results (and labeled as the last_240d option below), we’ll consider a mixture of this matrix and a seasonal one \\(M_{seasonal}\\). The latter is calculated on monthly basis lagging 1 year behind. For example, to predict DAU for November 2022 we define \\(M_{seasonal}\\) as the transition matrix for November 2021. Then we shift the prediction horizon to December 2022 and calculate \\(M_{seasonal}\\) for December 2021, etc.\nIn order to mix \\(M_{base}\\) and \\(M_{seasonal}\\) we define the following two options.\n\nseasonal_0.3: \\(M = 0.3 \\cdot M_{seasonal} + 0.7 \\cdot M_{base}\\). 0.3 is a weight that was chosen as a local minimum after some experiments.\nsmoothing: \\(M = \\frac{i}{N - 1} M_{seasonal} + (1 - \\frac{i}{N - 1}) M_{base}\\) where \\(N\\) is the number of months within the predicting period, \\(i = 0, \\ldots, N - 1\\) – the month index. The idea of this configuration is to gradually switch from the most recent transition matrix \\(M_{base}\\) to seasonal ones as the prediction month moves forward from the prediction start.\n\n\n\nToggle the code\nresult = pd.DataFrame()\nfor transition_period in ['last_240d', 'seasonal_0.3', 'smoothing']:\n    result[transition_period] = make_prediction('2022-11-01', '2023-10-31', 'true', transition_period)['dau']\nresult['true'] = dau_true['dau']\nresult['true'] = result['true'].astype(int)\nresult.plot(title='DAU prediction by different transition matrices');\n\n\n\n\n\n\n\n\n\n\n\nToggle the code\nmape = pd.DataFrame()\nfor col in result.columns:\n    if col != 'true':\n        mape.loc[col, 'mape'] = mean_absolute_percentage_error(result['true'], result[col])\nmape\n\n\n\n\n\n\n\n\n\nmape\n\n\n\n\nlast_240d\n0.080804\n\n\nseasonal_0.3\n0.077545\n\n\nsmoothing\n0.097802\n\n\n\n\n\n\n\nAccording to the MAPE errors, seasonal_0.3 configuration provides the best results. Interestingly, smoothing approach has appeared to be even worse than the last_240d. From the diagram above we see that all three models start to underestimate the DAU values in July 2023, especially the smoothing model. It seems that the new users who started appearing in July 2023 are more engaged than the users from 2022. Probably, the app was improved sufficiently or the marketing team did a great job. As a result, the smoothing model that much relies on the outdated transitions data from July 2022 - October 2022 fails more than the other models.\n\n\n4.5 Final solution\nTo sum things up, let’s make a final prediction for the 2024 year. We use the seasonal_0.3 configuration and the predicted values for new users.\n\n\nToggle the code\ndau_pred = make_prediction(\n    PREDICTION_START, PREDICTION_END,\n    new_users_mode='predict',\n    transition_period='seasonal_0.3'\n)\ndau_true['dau'].plot(label='true')\ndau_pred['dau'].plot(label='seasonal_0.3')\nplt.title('DAU, historical & predicted')\nplt.axvline(PREDICTION_START, color='k', linestyle='--')\nplt.legend();"
  }
]