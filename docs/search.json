[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Vladimir Kukushkin, and I have over 10 years of experience as a data scientist / data analyst. The slash signifies that I typically work at the intersection of product analytics and data science, tackling problems that require advanced techniques like predictive analytics, causal inference, and building statistical models. My specialization is user behavior analysis a.k.a. quantitative UX research or, more broadly, sequential data analysis.\nMy projects:\n\nretentioneering. An open-source Python library for user behavior analysis.\n\nMy publications:\n\nPrediction of Hourly Earnings and Completion Time on a Crowdsourcing Platform (KDD-2020)\nOn the Relation Between Assessor’s Agreement and Accuracy in Gamified Relevance Assessment (SIGIR-2015)"
  },
  {
    "objectID": "posts/2024-10-22_Welcome/index.html",
    "href": "posts/2024-10-22_Welcome/index.html",
    "title": "Welcome to Journal Papers Product Minds Club",
    "section": "",
    "text": "I’ve been working as a data scientist / data analyst over 10 years. The slash signifies that I typically work at the intersection of product analytics and data science, tackling problems that require advanced techniques like predictive analytics, causal inference, and building statistical models. My specialization is in user behavior analysis a.k.a. quantitative UX research or, more broadly, in analyzing sequential data. In addition to working as a data analyst, I played a key role in developing an open-source Python library retentioneering, which aims to simplify research in this area.\nIn my view, this field deserves much more attention than it currently receives. While these analyses are more complex than traditional funnel or cohort analysis, quantitative UX provides a deeper understanding of how users interact with your product. It reveals insights such as:\n\nWhat drives users to churn, make purchases, subscribe, etc. (causal inference),\nWhat segments users form (cluster analysis),\nWhat caused unexpected behavior in dashboard charts (root cause analysis)\nWhat distinguishes one user group from another, e.g. mobile VS desktop, AB-experiment groups, before and after release.\n\nThese research questions are not an empty sound. Finding the answers helps identify product bottlenecks, understand what drives user satisfaction or dissatisfaction, and ultimately improve your product. This is the core purpose of product analytics.\nGiven the lack of substantial literature on the subject, here I aim to collect and review papers, books, and publications that I find insightful.\nLike how Sgt. Pepper’s Lonely Hearts Club Band expanded horizons in music, I hope this journal club will attract data analysts and data scientists eager to broaden their understanding of these topics. The term “club” in the title reflects my wish to engage with enthusiasts and foster discussions around these fascinating subjects.  ♫ Music playing ♫ We’re Sgt. Pepper’s Lonely Hearts Club Band We hope you will enjoy the show"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Vladimir Kukushkin’s blog",
    "section": "",
    "text": "Sequen-C: A Multilevel Overview of Temporal Event Sequences\n\n\n\n\n\n\njournalclub\n\n\nclickstream visualizations\n\n\n\n\n\n\n\n\n\nNov 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to Journal Papers Product Minds Club\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 22, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-11-05_Sequen-C/index.html",
    "href": "posts/2024-11-05_Sequen-C/index.html",
    "title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences",
    "section": "",
    "text": "My name is Vladimir Kukushkin. I’ve been working as a data analyst/data scientist for more than 10 years. A couple of years ago I heavily shifted to quantitative UX research area which turned into my passion. While eagerly trying to find some sources to read regarding this topic, I realized that there are not so many of them. That’s why I decided to write a series of posts towards quantitative UX research. Basically, it will be reviews of papers, books and other publications that I find insightful. I’d be happy to find here enthusiasts who are also interested in this domain.\nLet me start our journal club with the most impressive paper related to clickstream visualizations I’ve ever read: Sequen-C: A Multilevel Overview of Temporal Event Sequences by Jessica Magallanes, Tony Stone, Paul D. Morris, Suzanne Mason, Steven Wood, and Maria-Cruz Villa-Uriol.\nThe authors address two very common issues that any quantitative UX researcher encounters:\nIn the paper, the authors describe an interactive visualization tool that tackles both these issues and provides a methodology they call Align-Score-Simplify."
  },
  {
    "objectID": "posts/2024-11-05_Sequen-C/index.html#methodology",
    "href": "posts/2024-11-05_Sequen-C/index.html#methodology",
    "title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences",
    "section": "Methodology",
    "text": "Methodology\nThe first problem is addressed by applying (agglomerative) clustering. I’ll explain further why they chose this algorithm. The second problem is solved using their framework called Align-Score-Simplify.\n\nAlign\nThis part is the most elegant, amazing, and surprizing in the whole paper. They apply the MSA algorithm (Multiple Sequence Alignment) stemming from bioinformatics. Originally, it was created to align amino acids or nucleotides in DNA sequences to identify common subsequences across DNAs belonging to multiple species. But this is exactly one of the goals we pursue in clickstream data analysis. Once we align user trajectories in similar way, we understand what events are common for the most users at some specific steps.\n\n\n\n\nInserting - as gaps to align events in sequences. The coloured events might be considered as common at some steps. The others are considered as noise.\n\n\n\n\n\nScore\nLet \\(\\lambda\\) be the output of the MSA algorithm applied to a set of unique sequences \\(S\\) (the frequency of each path is denoted as \\(P_i\\)). They calculate the information score \\(I_j\\) for each column \\(j\\) in \\(\\lambda\\) as a measure of the column’s impurity, somewhat similar to entropy but with some additional penalty for the high amount of gaps:\n\\[\nI_j = 1 - \\frac{E_j}{\\log_2(|A| + 1)}, \\;\\;\\;\\; E_j = \\sum_{a\\in A_j\\cup\\{-\\}}\n\\begin{cases}\n-P_a\\log_2\\left(\\frac{P_a}{G_j}\\right)\\, & \\text{if } a = \\text{'-'}\\\\\n-P_a\\log_2 P_a, & \\text{otherwise},\n\\end{cases}\n\\]\nwhere\n\n\\(A\\) – the set of unique events in \\(S\\),\n\\(A_j\\) – the set of unique events in column \\(j\\),\n\\(P_a\\) – the probability of the event \\(a\\) in column \\(j\\),\n\\(G_j\\) – is the count of gaps in column \\(j\\).\n\n\n\nSimplify\nOnce the \\(I_j\\) score is calculated for each column \\(j\\), we can treat each column that exceeds a certain threshold \\(I_\\tau\\) as an event that prevails in this column, while all other columns might be collapsed as noise. As a result, we get a new table \\(\\alpha\\) that simplifies the initial clickstream \\(S\\), as shown in the image below.\n\n\n\n\nThe outline of the Align-Score-Simplify workflow.\n\n\n\nFinally, we can granulate our clickstream representation using two axes:\n\nWe can consider different clusters of the agglomerative clustering output tree \\(T\\). Now it’s clear why they preferred this clustering algorithm instead of, say, K-Means. Considering different levels of \\(T\\), we obtain more/less homogeneous clusters. More homogeneous clusters provide better and clearer output of the MSA algorithm.\n\\(\\tau\\). By varying \\(\\tau\\), we can make the sequence visualization more/less coarse.\n\nAs a result, we get the interface like this:\n\n\n\n\nThe outline of the whole framework. (A) Building aggregate tree \\(T\\) for input unique sequences \\(S = \\{s_1, ...s_6\\}\\). (B) Each node in \\(T\\) has an alignment matrix \\(\\lambda\\) for its child sequences, a row-wise probabilities vector \\(P\\), and a column-wise information score vector \\(I\\). Two or more consecutive columns in \\(\\lambda\\) with \\(I_j &lt; 0.8\\) are not coloured. (C) Multilevel overviews for a range of number of clusters \\(k\\) retrieved from \\(T\\), where black blocks represent merged columns. Image by authors.\n\n\n\nIn the following sections, the authors describe the GUI of a more complex tool developed as an application for a couple of particular datasets from the public health domain. I won’t talk about it here since its underlying idea is the same, while the interface is more sophisticated and includes some information about events distribution, unique sequence view, and individual sequence view.\nThe datasets are more related to process mining, so the event cardinality is low. This is much lower than what happens in product analytics. The paths are very structured with few deviations from the main flow. That’s why I’m quite skeptical about applying this framework to product analytics clickstream data. Additionally, it’s not clear how fast MSA would work with clickstream data.\nAnyway, the idea of the paper and the framework are super interesting to me. It would be great to implement such a tool someday."
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html",
    "title": "Modeling DAU, WAU, MAU with Markov chain",
    "section": "",
    "text": "Doubtlessly, DAU, WAU, and MAU are critical business metrics. An article “How Duolingo reignited user growth” by Jorge Mazal, former CPO of Duolingo is #1 the Growth section of Lenny’s Newsletter blog. In this article Jorge payed special attention to the methodology Duolingo used to model DAU metrics (see another article “Meaningful metrics: how data sharpened the focus of product teams” by Erin Gustafson). This methodology has multiple strenghts but I focus here on how one can use it for DAU forecasting.\nNew year is coming soon, so many companies are planning their budgets for the next year these days. Cost estimations are often require DAU forecast. In this article I’ll show you how you can get this prediction using the Duolingo’s growth model and share a DAU & MAU “calculator” designed as a Google Spreadsheet calculator."
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#duolingos-growth-model",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#duolingos-growth-model",
    "title": "DAU & MAU prediction",
    "section": "Duolingo’s growth model",
    "text": "Duolingo’s growth model\nA quick recap on how the Duolingo’s growth model works. At day \\(d\\) (\\(d=1,2,\\ldots,\\)) of a user’s lifetime the user can be in one of the following 7 (mutually-exclusive) states:\n\n\n\n\nstate\n\n\nd = 1\n\n\nis active today\n\n\nwas active in [d-6, d-1]\n\n\nwas active in [d-29, d-7]\n\n\nwas active before d-30\n\n\n\n\n\nnew\n\n\n✅\n\n\n❓\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n\n\ncurrent\n\n\n❌\n\n\n✅\n\n\n✅\n\n\n❓\n\n\n❓\n\n\n\n\nreactivated\n\n\n❌\n\n\n✅\n\n\n❌\n\n\n✅\n\n\n❓\n\n\n\n\nresurrected\n\n\n❌\n\n\n✅\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n\n\nat_risk_wau\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n❓\n\n\n❓\n\n\n\n\nat_risk_mau\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n❓\n\n\n\n\ndormant\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n\nHaving these states defined, we can consider a user’s lifetime trajectory as a Markov chain. Let \\(M\\) be a transition matrix\nThe beauty and simplicity of this approach is that now we can fully describe states of the all users registered by a given day.\nIndeed, let \\(M\\) be the transition matrix derived from the historical data."
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#methodology",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#methodology",
    "title": "Modeling DAU, WAU, MAU with Markov chain",
    "section": "Methodology",
    "text": "Methodology\nA quick recap on how the Duolingo’s growth model works. At day \\(d\\) (\\(d=1,2,\\ldots,\\)) of a user’s lifetime the user can be in one of the following 7 (mutually-exclusive) states:\n\n\n\n\nstate\n\n\nd = 1\n\n\nis active today\n\n\nwas active in [d-6, d-1]\n\n\nwas active in [d-29, d-7]\n\n\nwas active before d-30\n\n\n\n\n\nnew\n\n\n✅\n\n\n❓\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n\n\ncurrent\n\n\n❌\n\n\n✅\n\n\n✅\n\n\n❓\n\n\n❓\n\n\n\n\nreactivated\n\n\n❌\n\n\n✅\n\n\n❌\n\n\n✅\n\n\n❓\n\n\n\n\nresurrected\n\n\n❌\n\n\n✅\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n\n\nat_risk_wau\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n❓\n\n\n❓\n\n\n\n\nat_risk_mau\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n❓\n\n\n\n\ndormant\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n\nHaving these states defined (as set \\(S\\)), we can consider a user’s lifetime trajectory as a Markov chain. Let \\(M\\) be a transition matrix associated with this Markov chain: \\(m_{i, j} = P(s_j | s_i)\\) are the probabilities that a user moves to state \\(s_j\\) right after being at state \\(s_i\\), \\(s_i, s_j \\in S\\). The matrix values are easily fetched from the historical data.\nThe beauty and simplicity of this approach is that matrix \\(M\\) fully describes states of the all users in the future. Suppose that vector \\(u_0\\) of length 7 contains the counts of users being in certain states at some calendar day denoted as 0. Thus, according to the Markov model, in the next day \\(u_1\\) we expect to have the following amount of users:\n\\[\n\\underbrace{\n\\begin{pmatrix}  \\#New_1 \\\\ \\#Current_1 \\\\ \\#Reactivated_1 \\\\ \\#Resurrected_1 \\\\ \\#AtRiskWau_1 \\\\ \\#AtRiskMau_1 \\\\ \\#Dormant_1 \\end{pmatrix}\n}_{u_1} = M^T \\cdot\n\\underbrace{\n\\begin{pmatrix}  \\#New_0 \\\\ \\#Current_0 \\\\ \\#Reactivated_0 \\\\ \\#Resurrected_0 \\\\ \\#AtRiskWau_0 \\\\ \\#AtRiskMau_0 \\\\ \\#Dormant_0 \\end{pmatrix}\n}_{u_0}\n\\]\nApplying this formula recursevely, we derive the amount of the users at any arbitrary day \\(t &gt; 0\\) in the future. The only thing we need to provide despite of the initial distribution \\(u_0\\) is to the amount of new users that would appear in the product each day in the future. We’ll get it by using historical data on new users appeared in the past and appyling the prophet library.\nNow, having \\(u_t\\) calculated, we can calculate DAU values at day t: \\[DAU_t = \\#New_t + \\#Current_t + \\#Reactivated_t +\\#Resurrected_t.\\]\nAdditionally, we can easily calculate WAU and MAU metrics: \\[WAU_t = DAU_t +\\#AtRiskWau_t,\\] \\[MAU_t = DAU_t +\\#AtRiskWau_t + \\#AtRiskMau_t.\\]\nFinally, the algorithm looks like this:\n\nCalculate initial counts \\(u_0\\) corresponding to the day right before prediction.\nFor each prediction day \\(t=1, ..., T\\) calculate the expected amount of new users \\(\\#New_1, \\ldots, \\#New_T\\).\nCalculate recursively \\(u_{t+1} = M^T u_t\\).\nCalculate DAU, WAU, MAU for each prediction day \\(t=1, ..., T\\)."
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#getting-the-states",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#getting-the-states",
    "title": "Modeling DAU, WAU, MAU with Markov chain",
    "section": "Getting the states",
    "text": "Getting the states\nWe use a simulated dataset that is stored in the event_log.csv file.\n\nimport duckdb\nimport pandas as pd"
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#pred",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#pred",
    "title": "DAU prediction",
    "section": "Pred",
    "text": "Pred"
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#predicting-new-users-amount",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#predicting-new-users-amount",
    "title": "Modeling DAU, WAU, MAU with Markov chain",
    "section": "Predicting new users amount",
    "text": "Predicting new users amount"
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#predicting-dau",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#predicting-dau",
    "title": "Modeling DAU, WAU, MAU with Markov chain",
    "section": "Predicting DAU",
    "text": "Predicting DAU"
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#prediction-testing",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#prediction-testing",
    "title": "Modeling DAU, WAU, MAU with Markov chain",
    "section": "Prediction testing",
    "text": "Prediction testing\n\nstate0 = get_state0('2022-06-30')\nM = get_transition_matrix(transitions, '2022-06-23', '2022-06-30')\ndau_pred = predict_dau(M, state0, '2022-07-01', '2022-10-31', new_users)\n\ndau_true\\\n    .join(dau_pred[['dau']], how='inner', lsuffix='_true', rsuffix='_pred')\\\n    .plot()"
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#conclusions",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#conclusions",
    "title": "Modeling DAU, WAU, MAU with Markov chain",
    "section": "Conclusions",
    "text": "Conclusions"
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#discussion",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#discussion",
    "title": "Modeling DAU, WAU, MAU with Markov chain",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#implementation",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#implementation",
    "title": "Modeling DAU, WAU, MAU with Markov chain",
    "section": "Implementation",
    "text": "Implementation\nSuppose that today is 2022-10-31 and we want to predict the DAU metric for the next year. We define PREDICTION_START and PREDICTION_END constants to set this prediction range.\n\nPREDICTION_START = '2022-11-01'\nPREDICTION_END = '2023-12-31'\n\nWe use a simulated dataset which is based on historical data of some SAAS app. The data is stored in the dau_data.csv file and contains three columns user_id, date, registration_date. Each record indicates a day when a user was active.\nThe data includes the activity indicators for all users spotted in the app from 2020-11-01 to 2022-10-31. An additional month 2020-10 is included in order to calculate user states correctly (AtRiskMau and dormant states requires data 1 month behind).\n\nimport pandas as pd\n\ndf = pd.read_csv('dau_data.csv.gz', compression='gzip')\nprint(f'Shape: {df.shape}')\nprint(f'Total users: {df['user_id'].nunique()}')\nprint(f'Data range: [{df['date'].min()}, {df['date'].max()}]')\ndf.head()\n\nShape: (447048, 3)\nTotal users: 38093\nData range: [2020-10-01, 2022-10-31]\n\n\n\n\n\n\n\n\n\nuser_id\ndate\nregistration_date\n\n\n\n\n0\n7a010840-b4d1-543d-bd4c-fbb4ae2198c5\n2020-10-01\n2020-08-26\n\n\n1\nd565a211-1996-538b-b067-11a38616c8cf\n2020-10-01\n2020-10-01\n\n\n2\n41fb29c7-8122-59db-a690-e9d9502f3c38\n2020-10-01\n2020-09-02\n\n\n3\n310c15a2-fe92-5703-be1f-80270c1b83bd\n2020-10-01\n2020-08-27\n\n\n4\n234c3035-145f-5694-bd7f-bec58a5f0c5c\n2020-10-01\n2020-05-31\n\n\n\n\n\n\n\nIn practice, the most calculations are reasonable to run as SQL queries to a database where the data is stored. To simulate this, I’ll use the duckdb library.\n\nimport duckdb\n\n\nPredicting new users amount\nLet’s start from the new users prediction. One of the easiest ways to do this is to use the prophet library. It simply requires a historical time-series and extends it in the future. The new_users Series contains such a historical data. A condition that indicates that a new user appeared is df['date'] == df['registration_date'].\n\nnew_users = df[df['date'] == df['registration_date']]\\\n    .assign(date=pd.to_datetime(df['date']))\\\n    .groupby('date').size()\n\nnew_users.head()\n\ndate\n2020-10-01    4\n2020-10-02    3\n2020-10-03    3\n2020-10-04    4\n2020-10-05    8\ndtype: int64\n\n\nprohet requires a time-series as a DataFrame containing two columns, so we reformat new_users Series to new_users_prophet DataFrame. Another thing we need to prepare is to create the future variable containing certain days for prediction: from PREDICTION_START to PREDICTION_END. The plot illustrates predictions for both past and future dates.\n\nfrom prophet import Prophet\n\nm = Prophet()\nnew_users_prophet = pd.DataFrame({'ds': new_users.index, 'y': new_users.values})\nm.fit(new_users_prophet)\n\nperiods = len(pd.date_range(PREDICTION_START, PREDICTION_END))\nfuture = m.make_future_dataframe(periods=periods)\nm.plot(m.predict(future));\n\n21:53:31 - cmdstanpy - INFO - Chain [1] start processing\n21:53:31 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n\n\n\n\nNow, let’s create a Series new_users_pred where we’ll keep predictions for the new users amount.\n\nnew_users_pred = m.predict(future)\\\n    .assign(yhat=lambda _df: _df['yhat'].astype(int))\\\n    .rename(columns={'ds': 'date', 'yhat': 'count'})\\\n    .loc[lambda _df: _df['date'] &gt;= PREDICTION_START]\\\n    .set_index('date')\\\n    ['count']\n\nnew_users_pred.head()\n\ndate\n2022-11-01    49\n2022-11-02    47\n2022-11-03    43\n2022-11-04    39\n2022-11-05    41\nName: count, dtype: int64\n\n\n\n\nGetting the states\nSo we want assign one of 7 states for each day of a user’s lifetime (meaning lifetime within the app). According to the definition, for each day we need to consider at least 30 days in the past. This is when SQL window functions come in. However, since df data contains only active day indicators, we need to explicitly extend it with the days when a user was not active.\nFor readability purposes we split the next SQL query into multiple subqueries.\n\ndau. Simply transforms dates from string to date data type.\nfull_range. Creates a full sequence of dates for each user.\n\n\nDATASET_START = '2021-11-01'\nDATASET_END = '2022-10-31'\nOBSERVATION_START = '2021-10-01'\n\nquery = f\"\"\"\nWITH\ndau AS (\n    SELECT\n        user_id,\n        date::date AS date,\n        registration_date::date AS registration_date\n    FROM df\n),\nfull_range AS (\n    SELECT\n        user_id, UNNEST(generate_series(greatest(registration_date, '{OBSERVATION_START}'), date '{DATASET_END}', INTERVAL 1 DAY))::date AS date\n    FROM (\n        SELECT DISTINCT user_id, registration_date FROM dau\n    )\n),\ndau_full AS (\n    SELECT\n        fr.user_id,\n        fr.date,\n        dau.date IS NOT NULL AS is_active,\n        registration_date\n    FROM full_range AS fr\n    LEFT JOIN dau USING(user_id, date)\n),\nstates AS (\n    SELECT\n        user_id,\n        date,\n        is_active,\n        first_value(registration_date IGNORE NULLS) OVER (PARTITION BY user_id ORDER BY date) AS registration_date,\n        SUM(is_active::int) OVER (PARTITION BY user_id ORDER BY date ROWS BETWEEN 6 PRECEDING and 1 PRECEDING) AS active_days_back_6d,\n        SUM(is_active::int) OVER (PARTITION BY user_id ORDER BY date ROWS BETWEEN 29 PRECEDING and 1 PRECEDING) AS active_days_back_29d,\n        CASE\n            WHEN date = registration_date THEN 'new'\n            WHEN is_active = TRUE AND active_days_back_6d BETWEEN 1 and 6 THEN 'current'\n            WHEN is_active = TRUE AND active_days_back_6d = 0 AND IFNULL(active_days_back_29d, 0) &gt; 0 THEN 'reactivated'\n            WHEN is_active = TRUE AND active_days_back_6d = 0 AND IFNULL(active_days_back_29d, 0) = 0 THEN 'resurrected'\n            WHEN is_active = FALSE AND active_days_back_6d &gt; 0 THEN 'at_risk_wau'\n            WHEN is_active = FALSE AND active_days_back_6d = 0 AND ifnull(active_days_back_29d, 0) &gt; 0 THEN 'at_risk_mau'\n            ELSE 'dormant'\n        END AS state\n    FROM dau_full\n)\nSELECT user_id, date, state FROM states\nWHERE date BETWEEN '{DATASET_START}' AND '{DATASET_END}'\nORDER BY user_id, date\n\"\"\"\nstates = duckdb.sql(query).df()\n\n\nquery = f\"\"\"\nSELECT\n    date,\n    state_from,\n    state_to,\n    COUNT(*) AS cnt,\nFROM (\n    SELECT\n        date,\n        state AS state_to,\n        lag(state) OVER (PARTITION BY user_id ORDER BY date) AS state_from\n    FROM states\n)\nWHERE state_from IS NOT NULL\nGROUP BY date, state_from, state_to\nORDER BY date, state_from, state_to;\n\"\"\"\ntransitions = duckdb.sql(query).df()\n\n\nPREDICTION_START = '2022-11-01'\nPREDICTION_END = '2023-12-31'\n\nstates_order = ['new', 'current', 'reactivated', 'resurrected', 'at_risk_wau', 'at_risk_mau', 'dormant']\n\ndef get_transition_matrix(transitions, date1, date2):\n    probs = transitions\\\n        .loc[lambda _df: _df['date'].between(date1, date2)]\\\n        .groupby(['state_from', 'state_to'], as_index=False)\\\n        ['cnt'].sum()\\\n        .assign(\n            supp=lambda _df: _df.groupby('state_from')['cnt'].transform('sum'),\n            prob=lambda _df: _df['cnt'] / _df['supp']\n        )\n\n    M = probs.pivot(index='state_from', columns='state_to', values='prob')\\\n        .reindex(states_order, axis=0)\\\n        .reindex(states_order, axis=1)\\\n        .fillna(0)\\\n        .astype(float)\n\n    return M\n\n\n\nPredicting DAU\n\ndef predict_dau(M, state0, date1, date2, new_users):\n    \"\"\"\n    Predicts DAU over a given date range.\n\n    Parameters\n    ----------\n    M : pandas.DataFrame\n        Transition matrix representing user state changes.\n    state0 : pandas.Series\n        counts of initial state of users.\n    date1 : str\n        Start date of the prediction period in 'YYYY-MM-DD' format.\n    date2 : str\n        End date of the prediction period in 'YYYY-MM-DD' format.\n    new_users : int or pandas.Series\n        The expected amount of new users for each day between date1 and date2.\n        If a Series, it should have dates as the index.\n        If an int, the same number is used for each day.\n        \n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame containing the predicted DAU, WAU, and MAU for each day in the date range,\n        with columns for different user states and tot.\n    \"\"\"\n    \n    dates = pd.date_range(date1, date2)\n    dates.name = 'date'\n    dau_pred = []\n    new_dau = state0.copy()\n    for date in dates:\n        new_dau = (M.transpose() @ new_dau).astype(int)\n        if isinstance(new_users, int):\n            new_users_today = new_users\n        else:\n            new_users_today = new_users.astype(int).loc[date] \n        new_dau.loc['new'] = new_users_today\n        dau_pred.append(new_dau.tolist())\n\n    dau_pred = pd.DataFrame(dau_pred, index=dates, columns=states_order)\n    dau_pred['dau'] = dau_pred['new'] + dau_pred['current'] + dau_pred['reactivated'] + dau_pred['resurrected']\n    dau_pred['wau'] = dau_pred['dau'] + dau_pred['at_risk_wau']\n    dau_pred['mau'] = dau_pred['dau'] + dau_pred['at_risk_wau'] + dau_pred['at_risk_mau']\n    \n    return dau_pred\n\n\ndef get_state0(date):\n    query = f\"\"\"\n    SELECT state, count(*) AS cnt\n    FROM states\n    WHERE date = '{date}'\n    GROUP BY state\n    \"\"\"\n\n    state0 = duckdb.sql(query).df()\n    state0 = state0.set_index('state').reindex(states_order)['cnt']\n    \n    return state0\n\n\nM = get_transition_matrix(transitions, '2022-10-25', '2022-10-31')\nstate0 = get_state0(DATASET_END)\ndau_pred = predict_dau(M, state0, PREDICTION_START, PREDICTION_END, new_users_pred)\ndau_pred\n\n\n\n\n\n\n\n\nnew\ncurrent\nreactivated\nresurrected\nat_risk_wau\nat_risk_mau\ndormant\ndau\nwau\nmau\n\n\n\n\n2022-11-01\n49\n444\n13\n13\n451\n1269\n16795\n519\n970\n2239\n\n\n2022-11-02\n47\n446\n13\n13\n462\n1267\n16830\n519\n981\n2248\n\n\n2022-11-03\n43\n448\n13\n13\n470\n1267\n16865\n517\n987\n2254\n\n\n2022-11-04\n39\n448\n13\n13\n474\n1268\n16900\n513\n987\n2255\n\n\n2022-11-05\n41\n447\n13\n13\n475\n1269\n16935\n514\n989\n2258\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-12-27\n135\n1172\n34\n36\n1266\n3247\n45910\n1377\n2643\n5890\n\n\n2023-12-28\n134\n1187\n34\n36\n1282\n3258\n45999\n1391\n2673\n5931\n\n\n2023-12-29\n132\n1201\n34\n36\n1297\n3271\n46088\n1403\n2700\n5971\n\n\n2023-12-30\n137\n1213\n34\n36\n1309\n3285\n46178\n1420\n2729\n6014\n\n\n2023-12-31\n151\n1227\n34\n36\n1323\n3300\n46268\n1448\n2771\n6071\n\n\n\n\n426 rows × 10 columns\n\n\n\n\nquery = \"\"\"\nSELECT date, COUNT(*) AS dau\nFROM states\nWHERE state IN ('new', 'current', 'reactivated', 'resurrected')\nGROUP BY date\nORDER BY date\n\"\"\"\n\ndau_true = duckdb.sql(query).df()\\\n    .set_index('date')\npd.concat([dau_true, dau_pred['dau']]).plot()"
  }
]