[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Vladimir Kukushkin, and I have over 10 years of experience as a data scientist / data analyst. The slash signifies that I typically work at the intersection of product analytics and data science, tackling problems that require advanced techniques like predictive analytics, causal inference, and building statistical models. My specialization is user behavior analysis a.k.a. quantitative UX research or, more broadly, sequential data analysis.\nMy projects:\n\nretentioneering. An open-source Python library for user behavior analysis.\n\nMy publications:\n\nPrediction of Hourly Earnings and Completion Time on a Crowdsourcing Platform (KDD-2020)\nOn the Relation Between Assessor’s Agreement and Accuracy in Gamified Relevance Assessment (SIGIR-2015)"
  },
  {
    "objectID": "posts/2024-10-22_Welcome/index.html",
    "href": "posts/2024-10-22_Welcome/index.html",
    "title": "Welcome to Journal Papers Product Minds Club",
    "section": "",
    "text": "I’ve been working as a data scientist / data analyst over 10 years. The slash signifies that I typically work at the intersection of product analytics and data science, tackling problems that require advanced techniques like predictive analytics, causal inference, and building statistical models. My specialization is in user behavior analysis a.k.a. quantitative UX research or, more broadly, in analyzing sequential data. In addition to working as a data analyst, I played a key role in developing an open-source Python library retentioneering, which aims to simplify research in this area.\nIn my view, this field deserves much more attention than it currently receives. While these analyses are more complex than traditional funnel or cohort analysis, quantitative UX provides a deeper understanding of how users interact with your product. It reveals insights such as:\n\nWhat drives users to churn, make purchases, subscribe, etc. (causal inference),\nWhat segments users form (cluster analysis),\nWhat caused unexpected behavior in dashboard charts (root cause analysis)\nWhat distinguishes one user group from another, e.g. mobile VS desktop, AB-experiment groups, before and after release.\n\nThese research questions are not an empty sound. Finding the answers helps identify product bottlenecks, understand what drives user satisfaction or dissatisfaction, and ultimately improve your product. This is the core purpose of product analytics.\nGiven the lack of substantial literature on the subject, here I aim to collect and review papers, books, and publications that I find insightful.\nLike how Sgt. Pepper’s Lonely Hearts Club Band expanded horizons in music, I hope this journal club will attract data analysts and data scientists eager to broaden their understanding of these topics. The term “club” in the title reflects my wish to engage with enthusiasts and foster discussions around these fascinating subjects.  ♫ Music playing ♫ We’re Sgt. Pepper’s Lonely Hearts Club Band We hope you will enjoy the show"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Modeling DAU, WAU, MAU with Markov chain (WIP)\n\n\n\n\n\n\nproduct analytics\n\n\npredictive analytics\n\n\n\n\n\n\n\n\n\nNov 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSequen-C: A Multilevel Overview of Temporal Event Sequences\n\n\n\n\n\n\njournalclub\n\n\nclickstream visualizations\n\n\n\n\n\n\n\n\n\nNov 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to Journal Papers Product Minds Club\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 22, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-11-05_Sequen-C/index.html",
    "href": "posts/2024-11-05_Sequen-C/index.html",
    "title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences",
    "section": "",
    "text": "My name is Vladimir Kukushkin. I’ve been working as a data analyst/data scientist for more than 10 years. A couple of years ago I heavily shifted to quantitative UX research area which turned into my passion. While eagerly trying to find some sources to read regarding this topic, I realized that there are not so many of them. That’s why I decided to write a series of posts towards quantitative UX research. Basically, it will be reviews of papers, books and other publications that I find insightful. I’d be happy to find here enthusiasts who are also interested in this domain.\nLet me start our journal club with the most impressive paper related to clickstream visualizations I’ve ever read: Sequen-C: A Multilevel Overview of Temporal Event Sequences by Jessica Magallanes, Tony Stone, Paul D. Morris, Suzanne Mason, Steven Wood, and Maria-Cruz Villa-Uriol.\nThe authors address two very common issues that any quantitative UX researcher encounters:\nIn the paper, the authors describe an interactive visualization tool that tackles both these issues and provides a methodology they call Align-Score-Simplify."
  },
  {
    "objectID": "posts/2024-11-05_Sequen-C/index.html#methodology",
    "href": "posts/2024-11-05_Sequen-C/index.html#methodology",
    "title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences",
    "section": "Methodology",
    "text": "Methodology\nThe first problem is addressed by applying (agglomerative) clustering. I’ll explain further why they chose this algorithm. The second problem is solved using their framework called Align-Score-Simplify.\n\nAlign\nThis part is the most elegant, amazing, and surprizing in the whole paper. They apply the MSA algorithm (Multiple Sequence Alignment) stemming from bioinformatics. Originally, it was created to align amino acids or nucleotides in DNA sequences to identify common subsequences across DNAs belonging to multiple species. But this is exactly one of the goals we pursue in clickstream data analysis. Once we align user trajectories in similar way, we understand what events are common for the most users at some specific steps.\n\n\n\n\nInserting - as gaps to align events in sequences. The coloured events might be considered as common at some steps. The others are considered as noise.\n\n\n\n\n\nScore\nLet \\(\\lambda\\) be the output of the MSA algorithm applied to a set of unique sequences \\(S\\) (the frequency of each path is denoted as \\(P_i\\)). They calculate the information score \\(I_j\\) for each column \\(j\\) in \\(\\lambda\\) as a measure of the column’s impurity, somewhat similar to entropy but with some additional penalty for the high amount of gaps:\n\\[\nI_j = 1 - \\frac{E_j}{\\log_2(|A| + 1)}, \\;\\;\\;\\; E_j = \\sum_{a\\in A_j\\cup\\{-\\}}\n\\begin{cases}\n-P_a\\log_2\\left(\\frac{P_a}{G_j}\\right)\\, & \\text{if } a = \\text{'-'}\\\\\n-P_a\\log_2 P_a, & \\text{otherwise},\n\\end{cases}\n\\]\nwhere\n\n\\(A\\) – the set of unique events in \\(S\\),\n\\(A_j\\) – the set of unique events in column \\(j\\),\n\\(P_a\\) – the probability of the event \\(a\\) in column \\(j\\),\n\\(G_j\\) – is the count of gaps in column \\(j\\).\n\n\n\nSimplify\nOnce the \\(I_j\\) score is calculated for each column \\(j\\), we can treat each column that exceeds a certain threshold \\(I_\\tau\\) as an event that prevails in this column, while all other columns might be collapsed as noise. As a result, we get a new table \\(\\alpha\\) that simplifies the initial clickstream \\(S\\), as shown in the image below.\n\n\n\n\nThe outline of the Align-Score-Simplify workflow.\n\n\n\nFinally, we can granulate our clickstream representation using two axes:\n\nWe can consider different clusters of the agglomerative clustering output tree \\(T\\). Now it’s clear why they preferred this clustering algorithm instead of, say, K-Means. Considering different levels of \\(T\\), we obtain more/less homogeneous clusters. More homogeneous clusters provide better and clearer output of the MSA algorithm.\n\\(\\tau\\). By varying \\(\\tau\\), we can make the sequence visualization more/less coarse.\n\nAs a result, we get the interface like this:\n\n\n\n\nThe outline of the whole framework. (A) Building aggregate tree \\(T\\) for input unique sequences \\(S = \\{s_1, ...s_6\\}\\). (B) Each node in \\(T\\) has an alignment matrix \\(\\lambda\\) for its child sequences, a row-wise probabilities vector \\(P\\), and a column-wise information score vector \\(I\\). Two or more consecutive columns in \\(\\lambda\\) with \\(I_j &lt; 0.8\\) are not coloured. (C) Multilevel overviews for a range of number of clusters \\(k\\) retrieved from \\(T\\), where black blocks represent merged columns. Image by authors.\n\n\n\nIn the following sections, the authors describe the GUI of a more complex tool developed as an application for a couple of particular datasets from the public health domain. I won’t talk about it here since its underlying idea is the same, while the interface is more sophisticated and includes some information about events distribution, unique sequence view, and individual sequence view.\nThe datasets are more related to process mining, so the event cardinality is low. This is much lower than what happens in product analytics. The paths are very structured with few deviations from the main flow. That’s why I’m quite skeptical about applying this framework to product analytics clickstream data. Additionally, it’s not clear how fast MSA would work with clickstream data.\nAnyway, the idea of the paper and the framework are super interesting to me. It would be great to implement such a tool someday."
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "",
    "text": "Doubtlessly, DAU, WAU, and MAU are critical business metrics. An article “How Duolingo reignited user growth” by Jorge Mazal, former CPO of Duolingo is #1 the Growth section of Lenny’s Newsletter blog. In this article Jorge payed special attention to the methodology Duolingo used to model DAU metrics (see another article “Meaningful metrics: how data sharpened the focus of product teams” by Erin Gustafson). This methodology has multiple strenghts but I focus here on how one can use it for DAU forecasting.\nNew year is coming soon, so many companies are planning their budgets for the next year these days. Cost estimations are often require DAU forecast. In this article I’ll show you how you can get this prediction using the Duolingo’s growth model and share a DAU & MAU “calculator” designed as a Google Spreadsheet calculator."
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#duolingos-growth-model",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#duolingos-growth-model",
    "title": "DAU & MAU prediction",
    "section": "Duolingo’s growth model",
    "text": "Duolingo’s growth model\nA quick recap on how the Duolingo’s growth model works. At day \\(d\\) (\\(d=1,2,\\ldots,\\)) of a user’s lifetime the user can be in one of the following 7 (mutually-exclusive) states:\n\n\n\n\nstate\n\n\nd = 1\n\n\nis active today\n\n\nwas active in [d-6, d-1]\n\n\nwas active in [d-29, d-7]\n\n\nwas active before d-30\n\n\n\n\n\nnew\n\n\n✅\n\n\n❓\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n\n\ncurrent\n\n\n❌\n\n\n✅\n\n\n✅\n\n\n❓\n\n\n❓\n\n\n\n\nreactivated\n\n\n❌\n\n\n✅\n\n\n❌\n\n\n✅\n\n\n❓\n\n\n\n\nresurrected\n\n\n❌\n\n\n✅\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n\n\nat_risk_wau\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n❓\n\n\n❓\n\n\n\n\nat_risk_mau\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n❓\n\n\n\n\ndormant\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n\nHaving these states defined, we can consider a user’s lifetime trajectory as a Markov chain. Let \\(M\\) be a transition matrix\nThe beauty and simplicity of this approach is that now we can fully describe states of the all users registered by a given day.\nIndeed, let \\(M\\) be the transition matrix derived from the historical data."
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#methodology",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#methodology",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "Methodology",
    "text": "Methodology\nA quick recap on how the Duolingo’s growth model works. At day \\(d\\) (\\(d=1,2,\\ldots,\\)) of a user’s lifetime the user can be in one of the following 7 (mutually-exclusive) states:\n\n\n\n\nstate\n\n\nd = 1\n\n\nis active today\n\n\nwas active in [d-6, d-1]\n\n\nwas active in [d-29, d-7]\n\n\nwas active before d-30\n\n\n\n\n\nnew\n\n\n✅\n\n\n❓\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n\n\ncurrent\n\n\n❌\n\n\n✅\n\n\n✅\n\n\n❓\n\n\n❓\n\n\n\n\nreactivated\n\n\n❌\n\n\n✅\n\n\n❌\n\n\n✅\n\n\n❓\n\n\n\n\nresurrected\n\n\n❌\n\n\n✅\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n\n\nat_risk_wau\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n❓\n\n\n❓\n\n\n\n\nat_risk_mau\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n❓\n\n\n\n\ndormant\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n\nHaving these states defined (as set \\(S\\)), we can consider a user’s lifetime trajectory as a Markov chain. Let \\(M\\) be a transition matrix associated with this Markov chain: \\(m_{i, j} = P(s_j | s_i)\\) are the probabilities that a user moves to state \\(s_j\\) right after being at state \\(s_i\\), \\(s_i, s_j \\in S\\). The matrix values are easily fetched from the historical data.\nThe beauty and simplicity of this approach is that matrix \\(M\\) fully describes states of the all users in the future. Suppose that vector \\(u_0\\) of length 7 contains the counts of users being in certain states at some calendar day denoted as 0. Thus, according to the Markov model, in the next day \\(u_1\\) we expect to have the following amount of users:\n\\[\n\\underbrace{\n\\begin{pmatrix}  \\#New_1 \\\\ \\#Current_1 \\\\ \\#Reactivated_1 \\\\ \\#Resurrected_1 \\\\ \\#AtRiskWau_1 \\\\ \\#AtRiskMau_1 \\\\ \\#Dormant_1 \\end{pmatrix}\n}_{u_1} = M^T \\cdot\n\\underbrace{\n\\begin{pmatrix}  \\#New_0 \\\\ \\#Current_0 \\\\ \\#Reactivated_0 \\\\ \\#Resurrected_0 \\\\ \\#AtRiskWau_0 \\\\ \\#AtRiskMau_0 \\\\ \\#Dormant_0 \\end{pmatrix}\n}_{u_0}\n\\]\nApplying this formula recursevely, we derive the amount of the users at any arbitrary day \\(t &gt; 0\\) in the future. The only thing we need to provide despite of the initial distribution \\(u_0\\) is to the amount of new users that would appear in the product each day in the future. We’ll get it by using historical data on new users appeared in the past and appyling the prophet library.\nNow, having \\(u_t\\) calculated, we can calculate DAU values at day t: \\[DAU_t = \\#New_t + \\#Current_t + \\#Reactivated_t +\\#Resurrected_t.\\]\nAdditionally, we can easily calculate WAU and MAU metrics: \\[WAU_t = DAU_t +\\#AtRiskWau_t,\\] \\[MAU_t = DAU_t +\\#AtRiskWau_t + \\#AtRiskMau_t.\\]\nFinally, the algorithm looks like this:\n\nCalculate initial counts \\(u_0\\) corresponding to the day right before prediction.\nFor each prediction day \\(t=1, ..., T\\) calculate the expected amount of new users \\(\\#New_1, \\ldots, \\#New_T\\).\nCalculate recursively \\(u_{t+1} = M^T u_t\\).\nCalculate DAU, WAU, MAU for each prediction day \\(t=1, ..., T\\)."
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#getting-the-states",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#getting-the-states",
    "title": "Modeling DAU, WAU, MAU with Markov chain",
    "section": "Getting the states",
    "text": "Getting the states\nWe use a simulated dataset that is stored in the event_log.csv file.\n\nimport duckdb\nimport pandas as pd"
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#pred",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#pred",
    "title": "DAU prediction",
    "section": "Pred",
    "text": "Pred"
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#predicting-new-users-amount",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#predicting-new-users-amount",
    "title": "Modeling DAU, WAU, MAU with Markov chain",
    "section": "Predicting new users amount",
    "text": "Predicting new users amount"
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#predicting-dau",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#predicting-dau",
    "title": "Modeling DAU, WAU, MAU with Markov chain",
    "section": "Predicting DAU",
    "text": "Predicting DAU"
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#prediction-testing",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#prediction-testing",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "Prediction testing",
    "text": "Prediction testing\n\nstate0 = get_state0('2022-06-30')\nM = get_transition_matrix(transitions, '2022-06-23', '2022-06-30')\ndau_pred = predict_dau(M, state0, '2022-07-01', '2022-10-31', new_users)\n\ndau_true\\\n    .join(dau_pred[['dau']], how='inner', lsuffix='_true', rsuffix='_pred')\\\n    .plot()"
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#conclusions",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#conclusions",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "Conclusions",
    "text": "Conclusions"
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#discussion",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#discussion",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#implementation",
    "href": "notebooks/2024-11-01_dau_prediction/dau_prediction.html#implementation",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "Implementation",
    "text": "Implementation\nSuppose that today is 2022-10-31 and we want to predict the DAU metric for the next year. We define PREDICTION_START and PREDICTION_END constants to set this prediction range.\n\nPREDICTION_START = '2022-11-01'\nPREDICTION_END = '2023-12-31'\n\nWe use a simulated dataset which is based on historical data of some SAAS app. The data is stored in the dau_data.csv file and contains three columns user_id, date, registration_date. Each record indicates a day when a user was active.\nThe data includes the activity indicators for all users spotted in the app from 2020-11-01 to 2022-10-31. An additional month 2020-10 is included in order to calculate user states correctly (AtRiskMau and dormant states requires data 1 month behind).\n\nimport pandas as pd\n\ndf = pd.read_csv('dau_data.csv.gz', compression='gzip')\nprint(f'Shape: {df.shape}')\nprint(f'Total users: {df['user_id'].nunique()}')\nprint(f'Data range: [{df['date'].min()}, {df['date'].max()}]')\ndf.head()\n\nShape: (447048, 3)\nTotal users: 38093\nData range: [2020-10-01, 2022-10-31]\n\n\n\n\n\n\n\n\n\nuser_id\ndate\nregistration_date\n\n\n\n\n0\n7a010840-b4d1-543d-bd4c-fbb4ae2198c5\n2020-10-01\n2020-08-26\n\n\n1\nd565a211-1996-538b-b067-11a38616c8cf\n2020-10-01\n2020-10-01\n\n\n2\n41fb29c7-8122-59db-a690-e9d9502f3c38\n2020-10-01\n2020-09-02\n\n\n3\n310c15a2-fe92-5703-be1f-80270c1b83bd\n2020-10-01\n2020-08-27\n\n\n4\n234c3035-145f-5694-bd7f-bec58a5f0c5c\n2020-10-01\n2020-05-31\n\n\n\n\n\n\n\nIn practice, the most calculations are reasonable to run as SQL queries to a database where the data is stored. To simulate this, I’ll use the duckdb library.\n\nimport duckdb\n\n\nPredicting new users amount\nLet’s start from the new users prediction. One of the easiest ways to do this is to use the prophet library. It simply requires a historical time-series and extends it in the future. The new_users Series contains such a historical data. A condition that indicates that a new user appeared is df['date'] == df['registration_date'].\n\nnew_users = df[df['date'] == df['registration_date']]\\\n    .assign(date=pd.to_datetime(df['date']))\\\n    .groupby('date').size()\n\nnew_users.head()\n\ndate\n2020-10-01    4\n2020-10-02    3\n2020-10-03    3\n2020-10-04    4\n2020-10-05    8\ndtype: int64\n\n\nprohet requires a time-series as a DataFrame containing two columns, so we reformat new_users Series to new_users_prophet DataFrame. Another thing we need to prepare is to create the future variable containing certain days for prediction: from PREDICTION_START to PREDICTION_END. The plot illustrates predictions for both past and future dates.\n\nfrom prophet import Prophet\n\nm = Prophet()\nnew_users_prophet = pd.DataFrame({'ds': new_users.index, 'y': new_users.values})\nm.fit(new_users_prophet)\n\nperiods = len(pd.date_range(PREDICTION_START, PREDICTION_END))\nfuture = m.make_future_dataframe(periods=periods)\nm.plot(m.predict(future));\n\n21:53:31 - cmdstanpy - INFO - Chain [1] start processing\n21:53:31 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n\n\n\n\nNow, let’s create a Series new_users_pred where we’ll keep predictions for the new users amount.\n\nnew_users_pred = m.predict(future)\\\n    .assign(yhat=lambda _df: _df['yhat'].astype(int))\\\n    .rename(columns={'ds': 'date', 'yhat': 'count'})\\\n    .loc[lambda _df: _df['date'] &gt;= PREDICTION_START]\\\n    .set_index('date')\\\n    ['count']\n\nnew_users_pred.head()\n\ndate\n2022-11-01    49\n2022-11-02    47\n2022-11-03    43\n2022-11-04    39\n2022-11-05    41\nName: count, dtype: int64\n\n\n\n\nGetting the states\nSo we want assign one of 7 states for each day of a user’s lifetime (meaning lifetime within the app). According to the definition, for each day we need to consider at least 30 days in the past. This is when SQL window functions come in. However, since df data contains only active day indicators, we need to explicitly extend it with the days when a user was not active.\nFor readability purposes we split the next SQL query into multiple subqueries.\n\ndau. Simply transforms dates from string to date data type.\nfull_range. Creates a full sequence of dates for each user.\n\n\nDATASET_START = '2021-11-01'\nDATASET_END = '2022-10-31'\nOBSERVATION_START = '2021-10-01'\n\nquery = f\"\"\"\nWITH\ndau AS (\n    SELECT\n        user_id,\n        date::date AS date,\n        registration_date::date AS registration_date\n    FROM df\n),\nfull_range AS (\n    SELECT\n        user_id, UNNEST(generate_series(greatest(registration_date, '{OBSERVATION_START}'), date '{DATASET_END}', INTERVAL 1 DAY))::date AS date\n    FROM (\n        SELECT DISTINCT user_id, registration_date FROM dau\n    )\n),\ndau_full AS (\n    SELECT\n        fr.user_id,\n        fr.date,\n        dau.date IS NOT NULL AS is_active,\n        registration_date\n    FROM full_range AS fr\n    LEFT JOIN dau USING(user_id, date)\n),\nstates AS (\n    SELECT\n        user_id,\n        date,\n        is_active,\n        first_value(registration_date IGNORE NULLS) OVER (PARTITION BY user_id ORDER BY date) AS registration_date,\n        SUM(is_active::int) OVER (PARTITION BY user_id ORDER BY date ROWS BETWEEN 6 PRECEDING and 1 PRECEDING) AS active_days_back_6d,\n        SUM(is_active::int) OVER (PARTITION BY user_id ORDER BY date ROWS BETWEEN 29 PRECEDING and 1 PRECEDING) AS active_days_back_29d,\n        CASE\n            WHEN date = registration_date THEN 'new'\n            WHEN is_active = TRUE AND active_days_back_6d BETWEEN 1 and 6 THEN 'current'\n            WHEN is_active = TRUE AND active_days_back_6d = 0 AND IFNULL(active_days_back_29d, 0) &gt; 0 THEN 'reactivated'\n            WHEN is_active = TRUE AND active_days_back_6d = 0 AND IFNULL(active_days_back_29d, 0) = 0 THEN 'resurrected'\n            WHEN is_active = FALSE AND active_days_back_6d &gt; 0 THEN 'at_risk_wau'\n            WHEN is_active = FALSE AND active_days_back_6d = 0 AND ifnull(active_days_back_29d, 0) &gt; 0 THEN 'at_risk_mau'\n            ELSE 'dormant'\n        END AS state\n    FROM dau_full\n)\nSELECT user_id, date, state FROM states\nWHERE date BETWEEN '{DATASET_START}' AND '{DATASET_END}'\nORDER BY user_id, date\n\"\"\"\nstates = duckdb.sql(query).df()\n\n\nquery = f\"\"\"\nSELECT\n    date,\n    state_from,\n    state_to,\n    COUNT(*) AS cnt,\nFROM (\n    SELECT\n        date,\n        state AS state_to,\n        lag(state) OVER (PARTITION BY user_id ORDER BY date) AS state_from\n    FROM states\n)\nWHERE state_from IS NOT NULL\nGROUP BY date, state_from, state_to\nORDER BY date, state_from, state_to;\n\"\"\"\ntransitions = duckdb.sql(query).df()\n\n\nPREDICTION_START = '2022-11-01'\nPREDICTION_END = '2023-12-31'\n\nstates_order = ['new', 'current', 'reactivated', 'resurrected', 'at_risk_wau', 'at_risk_mau', 'dormant']\n\ndef get_transition_matrix(transitions, date1, date2):\n    probs = transitions\\\n        .loc[lambda _df: _df['date'].between(date1, date2)]\\\n        .groupby(['state_from', 'state_to'], as_index=False)\\\n        ['cnt'].sum()\\\n        .assign(\n            supp=lambda _df: _df.groupby('state_from')['cnt'].transform('sum'),\n            prob=lambda _df: _df['cnt'] / _df['supp']\n        )\n\n    M = probs.pivot(index='state_from', columns='state_to', values='prob')\\\n        .reindex(states_order, axis=0)\\\n        .reindex(states_order, axis=1)\\\n        .fillna(0)\\\n        .astype(float)\n\n    return M\n\n\n\nPredicting DAU\n\ndef predict_dau(M, state0, date1, date2, new_users):\n    \"\"\"\n    Predicts DAU over a given date range.\n\n    Parameters\n    ----------\n    M : pandas.DataFrame\n        Transition matrix representing user state changes.\n    state0 : pandas.Series\n        counts of initial state of users.\n    date1 : str\n        Start date of the prediction period in 'YYYY-MM-DD' format.\n    date2 : str\n        End date of the prediction period in 'YYYY-MM-DD' format.\n    new_users : int or pandas.Series\n        The expected amount of new users for each day between date1 and date2.\n        If a Series, it should have dates as the index.\n        If an int, the same number is used for each day.\n        \n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame containing the predicted DAU, WAU, and MAU for each day in the date range,\n        with columns for different user states and tot.\n    \"\"\"\n    \n    dates = pd.date_range(date1, date2)\n    dates.name = 'date'\n    dau_pred = []\n    new_dau = state0.copy()\n    for date in dates:\n        new_dau = (M.transpose() @ new_dau).astype(int)\n        if isinstance(new_users, int):\n            new_users_today = new_users\n        else:\n            new_users_today = new_users.astype(int).loc[date] \n        new_dau.loc['new'] = new_users_today\n        dau_pred.append(new_dau.tolist())\n\n    dau_pred = pd.DataFrame(dau_pred, index=dates, columns=states_order)\n    dau_pred['dau'] = dau_pred['new'] + dau_pred['current'] + dau_pred['reactivated'] + dau_pred['resurrected']\n    dau_pred['wau'] = dau_pred['dau'] + dau_pred['at_risk_wau']\n    dau_pred['mau'] = dau_pred['dau'] + dau_pred['at_risk_wau'] + dau_pred['at_risk_mau']\n    \n    return dau_pred\n\n\ndef get_state0(date):\n    query = f\"\"\"\n    SELECT state, count(*) AS cnt\n    FROM states\n    WHERE date = '{date}'\n    GROUP BY state\n    \"\"\"\n\n    state0 = duckdb.sql(query).df()\n    state0 = state0.set_index('state').reindex(states_order)['cnt']\n    \n    return state0\n\n\nM = get_transition_matrix(transitions, '2022-10-25', '2022-10-31')\nstate0 = get_state0(DATASET_END)\ndau_pred = predict_dau(M, state0, PREDICTION_START, PREDICTION_END, new_users_pred)\ndau_pred\n\n\n\n\n\n\n\n\nnew\ncurrent\nreactivated\nresurrected\nat_risk_wau\nat_risk_mau\ndormant\ndau\nwau\nmau\n\n\n\n\n2022-11-01\n49\n444\n13\n13\n451\n1269\n16795\n519\n970\n2239\n\n\n2022-11-02\n47\n446\n13\n13\n462\n1267\n16830\n519\n981\n2248\n\n\n2022-11-03\n43\n448\n13\n13\n470\n1267\n16865\n517\n987\n2254\n\n\n2022-11-04\n39\n448\n13\n13\n474\n1268\n16900\n513\n987\n2255\n\n\n2022-11-05\n41\n447\n13\n13\n475\n1269\n16935\n514\n989\n2258\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-12-27\n135\n1172\n34\n36\n1266\n3247\n45910\n1377\n2643\n5890\n\n\n2023-12-28\n134\n1187\n34\n36\n1282\n3258\n45999\n1391\n2673\n5931\n\n\n2023-12-29\n132\n1201\n34\n36\n1297\n3271\n46088\n1403\n2700\n5971\n\n\n2023-12-30\n137\n1213\n34\n36\n1309\n3285\n46178\n1420\n2729\n6014\n\n\n2023-12-31\n151\n1227\n34\n36\n1323\n3300\n46268\n1448\n2771\n6071\n\n\n\n\n426 rows × 10 columns\n\n\n\n\nquery = \"\"\"\nSELECT date, COUNT(*) AS dau\nFROM states\nWHERE state IN ('new', 'current', 'reactivated', 'resurrected')\nGROUP BY date\nORDER BY date\n\"\"\"\n\ndau_true = duckdb.sql(query).df()\\\n    .set_index('date')\npd.concat([dau_true, dau_pred['dau']]).plot()"
  },
  {
    "objectID": "posts/2024-11-01_dau_prediction/dau_prediction.html",
    "href": "posts/2024-11-01_dau_prediction/dau_prediction.html",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "",
    "text": "Doubtlessly, DAU, WAU, and MAU are critical business metrics. An article “How Duolingo reignited user growth” by Jorge Mazal, former CPO of Duolingo is #1 the Growth section of Lenny’s Newsletter blog. In this article Jorge payed special attention to the methodology Duolingo used to model DAU metrics (see another article “Meaningful metrics: how data sharpened the focus of product teams” by Erin Gustafson). This methodology has multiple strenghts but I focus here on how one can use it for DAU forecasting.\nNew year is coming soon, so many companies are planning their budgets for the next year these days. Cost estimations are often require DAU forecast. In this article I’ll show you how you can get this prediction using the Duolingo’s growth model and share a DAU & MAU “calculator” designed as a Google Spreadsheet calculator."
  },
  {
    "objectID": "posts/2024-11-01_dau_prediction/dau_prediction.html#methodology",
    "href": "posts/2024-11-01_dau_prediction/dau_prediction.html#methodology",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "2. Methodology",
    "text": "2. Methodology\nA quick recap on how the Duolingo’s growth model works. At day \\(d\\) (\\(d=1,2,\\ldots,\\)) of a user’s lifetime the user can be in one of the following 7 (mutually-exclusive) states:\n\n\n\n\nstate\n\n\nd = 1\n\n\nactivetoday\n\n\nactive in[d-6, d-1]\n\n\nactive in[d-29, d-7]\n\n\nactivebefore d-30\n\n\n\n\n\nnew\n\n\n✅\n\n\n❓\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n\n\ncurrent\n\n\n❌\n\n\n✅\n\n\n✅\n\n\n❓\n\n\n❓\n\n\n\n\nreactivated\n\n\n❌\n\n\n✅\n\n\n❌\n\n\n✅\n\n\n❓\n\n\n\n\nresurrected\n\n\n❌\n\n\n✅\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n\n\nat_risk_wau\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n❓\n\n\n❓\n\n\n\n\nat_risk_mau\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n❓\n\n\n\n\ndormant\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n❌\n\n\n✅\n\n\n\nHaving these states defined (as set \\(S\\)), we can consider a user’s lifetime trajectory as a Markov chain. Let \\(M\\) be a transition matrix associated with this Markov chain: \\(m_{i, j} = P(s_j | s_i)\\) are the probabilities that a user moves to state \\(s_j\\) right after being at state \\(s_i\\), \\(s_i, s_j \\in S\\). The matrix values are easily fetched from the historical data.\nIf we assume that the user behavior is stationary, the matrix \\(M\\) fully describes states of the all users in the future. Suppose that vector \\(u_0\\) of length 7 contains the counts of users being in certain states at some calendar day denoted as 0. Thus, according to the Markov model, in the next day \\(u_1\\) we expect to have the following amount of users:\n\\[\n\\underbrace{\n\\begin{pmatrix}  \\#New_1 \\\\ \\#Current_1 \\\\ \\#Reactivated_1 \\\\ \\#Resurrected_1 \\\\ \\#AtRiskWau_1 \\\\ \\#AtRiskMau_1 \\\\ \\#Dormant_1 \\end{pmatrix}\n}_{u_1} = M^T \\cdot\n\\underbrace{\n\\begin{pmatrix}  \\#New_0 \\\\ \\#Current_0 \\\\ \\#Reactivated_0 \\\\ \\#Resurrected_0 \\\\ \\#AtRiskWau_0 \\\\ \\#AtRiskMau_0 \\\\ \\#Dormant_0 \\end{pmatrix}\n}_{u_0}\n\\]\nApplying this formula recursevely, we derive the amount of the users at any arbitrary day \\(t &gt; 0\\) in the future. The only thing we need to provide despite of the initial distribution \\(u_0\\) is to the amount of new users that would appear in the product each day in the future. We’ll get it by using historical data on new users appeared in the past and appyling the prophet library.\nNow, having \\(u_t\\) calculated, we can calculate DAU values at day t: \\[DAU_t = \\#New_t + \\#Current_t + \\#Reactivated_t +\\#Resurrected_t.\\]\nAdditionally, we can easily calculate WAU and MAU metrics: \\[WAU_t = DAU_t +\\#AtRiskWau_t,\\] \\[MAU_t = DAU_t +\\#AtRiskWau_t + \\#AtRiskMau_t.\\]\nFinally, the algorithm looks like this:\n\nFor each prediction day \\(t=1, ..., T\\) calculate the expected amount of new users \\(\\#New_1, \\ldots, \\#New_T\\).\nFor each lifetime day of each user define on of the 7 states.\nCalculate the transition matrix \\(M\\).\nCalculate initial counts \\(u_0\\) corresponding to \\(t=0\\) day.\nCalculate recursively \\(u_{t+1} = M^T u_t\\).\nCalculate DAU, WAU, MAU for each prediction day \\(t=1, ..., T\\)."
  },
  {
    "objectID": "posts/2024-11-01_dau_prediction/dau_prediction.html#implementation",
    "href": "posts/2024-11-01_dau_prediction/dau_prediction.html#implementation",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "3. Implementation",
    "text": "3. Implementation\n\n3.1 Dataset\nWe use a simulated dataset based on historical data of a SAAS app. The data is stored in the dau_data.csv.gz file and contains three columns: user_id, date, and registration_date. Each record indicates a day when a user was active.\nThe data includes activity indicators for all users from 2020-11-01 to 2022-10-31. An additional month, October 2020, is included to calculate user states correctly (at_risk_mau and dormant states require data from one month prior).\n\nimport pandas as pd\n\ndf = pd.read_csv('dau_data.csv.gz', compression='gzip')\ndf['date'] = pd.to_datetime(df['date'])\ndf['registration_date'] = pd.to_datetime(df['registration_date'])\n\nprint(f'Shape: {df.shape}')\nprint(f'Total users: {df['user_id'].nunique()}')\nprint(f'Data range: [{df['date'].min()}, {df['date'].max()}]')\ndf.head()\n\nShape: (447048, 3)\nTotal users: 38093\nData range: [2020-10-01 00:00:00, 2022-10-31 00:00:00]\n\n\n\n\n\n\n\n\n\nuser_id\ndate\nregistration_date\n\n\n\n\n0\n7a010840-b4d1-543d-bd4c-fbb4ae2198c5\n2020-10-01\n2020-08-26\n\n\n1\nd565a211-1996-538b-b067-11a38616c8cf\n2020-10-01\n2020-10-01\n\n\n2\n41fb29c7-8122-59db-a690-e9d9502f3c38\n2020-10-01\n2020-09-02\n\n\n3\n310c15a2-fe92-5703-be1f-80270c1b83bd\n2020-10-01\n2020-08-27\n\n\n4\n234c3035-145f-5694-bd7f-bec58a5f0c5c\n2020-10-01\n2020-05-31\n\n\n\n\n\n\n\nThis is how the DAU time-series looks like up to 2022-10-31.\n\ndf.groupby('date').size()\\\n    .plot(title='DAU, historical')\n\n\n\n\n\n\n\n\nSuppose that today is 2022-10-31 and we want to predict the DAU metric for the next 2023 year. We define a couple of constants PREDICTION_START and PREDICTION_END which define the prediction period.\n\nPREDICTION_START = '2022-11-01'\nPREDICTION_END = '2023-12-31'\n\n\n\n3.2 Predicting new users amount\nLet’s start from the new users prediction. We use the prophet library as one of the easiest ways to predict time-series data. The new_users Series contains such data. We extract it from the original df dataset selecting the rows where the registration date is equal to the date.\n\n\nToggle the code\nnew_users = df[df['date'] == df['registration_date']]\\\n    .assign(date=pd.to_datetime(df['date']))\\\n    .groupby('date').size()\n\n\n\nnew_users.head()\n\ndate\n2020-10-01    4\n2020-10-02    3\n2020-10-03    3\n2020-10-04    4\n2020-10-05    8\ndtype: int64\n\n\nprophet requires a time-series as a DataFrame containing two columns ds and y, so we reformat the new_users Series to the new_users_prophet DataFrame. Another thing we need to prepare is to create the future variable containing certain days for prediction: from PREDICTION_START to PREDICTION_END. The plot illustrates predictions for both past and future dates.\n\n\nToggle the code\nimport matplotlib.pyplot as plt\nfrom prophet import Prophet\n\nm = Prophet()\nnew_users_prophet = pd.DataFrame({'ds': new_users.index, 'y': new_users.values})\nm.fit(new_users_prophet)\n\nperiods = len(pd.date_range(PREDICTION_START, PREDICTION_END))\nfuture = m.make_future_dataframe(periods=periods)\nnew_users_pred = m.predict(future)\nm.plot(new_users_pred)\nplt.title('New users prediction');\n\nnew_users_pred = new_users_pred\\\n    .assign(yhat=lambda _df: _df['yhat'].astype(int))\\\n    .rename(columns={'ds': 'date', 'yhat': 'count'})\\\n    .set_index('date')\\\n    .clip(lower=0)\\\n    ['count']\n\n\n/Users/v.kukushkin/Documents/private/wowone.github.io/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n01:09:22 - cmdstanpy - INFO - Chain [1] start processing\n01:09:22 - cmdstanpy - INFO - Chain [1] done processing\n\n\n\n\n\n\n\n\n\nThe new_users_pred Series keeps the predicted users amount.\n\nnew_users_pred.tail(5)\n\ndate\n2023-12-27    135\n2023-12-28    134\n2023-12-29    132\n2023-12-30    137\n2023-12-31    151\nName: count, dtype: int64\n\n\n\n\n3.3 Getting the states\nIn practice, the most calculations are reasonable to execute as SQL queries to a database where the data is stored. Hereafter, we will simulate such quering with the duckdb library.\nWe want to assign one of the 7 states to each day of a user’s lifetime within the app. According to the definition, for each day, we need to consider at least the past 30 days. This is where SQL window functions come in. However, since the df data contains only records of active days, we need to explicitly extend it to include the days when a user was not active. In other words, instead of this list of records:\nuser_id    date          registration_date\n1234567    2022-01-01    2022-01-01\n1234567    2022-01-03    2022-01-01\nwe’d like to get a list like this:\nuser_id    date          is_active    registration_date\n1234567    2022-01-01    TRUE         2022-01-01\n1234567    2022-01-02    FALSE        2022-01-01\n1234567    2022-01-03    TRUE         2022-01-01\n1234567    2022-01-04    FALSE        2022-01-01\n1234567    2022-01-05    FALSE        2022-01-01\n...        ...           ...          ...\n1234567    2022-10-31    FALSE        2022-01-01\nFor readability purposes we split the following SQL query into multiple subqueries.\n\nfull_range: Create a full sequence of dates for each user.\ndau_full: Get the full list of both active and inactive records.\nstates: Assign one of the 7 states for each day of a user’s lifetime.\n\n\n\nToggle the code\nimport duckdb\n\nDATASET_START = '2020-11-01'\nDATASET_END = '2022-10-31'\nOBSERVATION_START = '2020-10-01'\n\nquery = f\"\"\"\nWITH\nfull_range AS (\n    SELECT\n        user_id, UNNEST(generate_series(greatest(registration_date, '{OBSERVATION_START}'), date '{DATASET_END}', INTERVAL 1 DAY))::date AS date\n    FROM (\n        SELECT DISTINCT user_id, registration_date FROM df\n    )\n),\ndau_full AS (\n    SELECT\n        fr.user_id,\n        fr.date,\n        df.date IS NOT NULL AS is_active,\n        registration_date\n    FROM full_range AS fr\n    LEFT JOIN df USING(user_id, date)\n),\nstates AS (\n    SELECT\n        user_id,\n        date,\n        is_active,\n        first_value(registration_date IGNORE NULLS) OVER (PARTITION BY user_id ORDER BY date) AS registration_date,\n        SUM(is_active::int) OVER (PARTITION BY user_id ORDER BY date ROWS BETWEEN 6 PRECEDING and 1 PRECEDING) AS active_days_back_6d,\n        SUM(is_active::int) OVER (PARTITION BY user_id ORDER BY date ROWS BETWEEN 29 PRECEDING and 1 PRECEDING) AS active_days_back_29d,\n        CASE\n            WHEN date = registration_date THEN 'new'\n            WHEN is_active = TRUE AND active_days_back_6d BETWEEN 1 and 6 THEN 'current'\n            WHEN is_active = TRUE AND active_days_back_6d = 0 AND IFNULL(active_days_back_29d, 0) &gt; 0 THEN 'reactivated'\n            WHEN is_active = TRUE AND active_days_back_6d = 0 AND IFNULL(active_days_back_29d, 0) = 0 THEN 'resurrected'\n            WHEN is_active = FALSE AND active_days_back_6d &gt; 0 THEN 'at_risk_wau'\n            WHEN is_active = FALSE AND active_days_back_6d = 0 AND ifnull(active_days_back_29d, 0) &gt; 0 THEN 'at_risk_mau'\n            ELSE 'dormant'\n        END AS state\n    FROM dau_full\n)\nSELECT user_id, date, state FROM states\nWHERE date BETWEEN '{DATASET_START}' AND '{DATASET_END}'\nORDER BY user_id, date\n\"\"\"\nstates = duckdb.sql(query).df()\n\n\nThe query results are kept in the states DataFrame:\n\nstates.head()\n\n\n\n\n\n\n\n\nuser_id\ndate\nstate\n\n\n\n\n0\n0000bdde-d6eb-5215-a3e1-50a97e4899a5\n2021-12-30\nnew\n\n\n1\n0000bdde-d6eb-5215-a3e1-50a97e4899a5\n2021-12-31\ncurrent\n\n\n2\n0000bdde-d6eb-5215-a3e1-50a97e4899a5\n2022-01-01\ncurrent\n\n\n3\n0000bdde-d6eb-5215-a3e1-50a97e4899a5\n2022-01-02\ncurrent\n\n\n4\n0000bdde-d6eb-5215-a3e1-50a97e4899a5\n2022-01-03\ncurrent\n\n\n\n\n\n\n\n\n\n3.4 Calculating the transition matrix\nHaving obtained these states, we can calculate state transition frequencies. In the real world, due to the large amount of data, it would be more effective to use a SQL query rather than a Python script. We calculate these frequencies day-wise since we’re going to study how the prediction depends on the period in which transitions are considered further.\n\n\nToggle the code\nquery = f\"\"\"\nSELECT\n    date,\n    state_from,\n    state_to,\n    COUNT(*) AS cnt,\nFROM (\n    SELECT\n        date,\n        state AS state_to,\n        lag(state) OVER (PARTITION BY user_id ORDER BY date) AS state_from\n    FROM states\n)\nWHERE state_from IS NOT NULL\nGROUP BY date, state_from, state_to\nORDER BY date, state_from, state_to;\n\"\"\"\ntransitions = duckdb.sql(query).df()\n\n\nThe result is stored in the transitions DataFrame.\n\ntransitions.head()\n\n\n\n\n\n\n\n\ndate\nstate_from\nstate_to\ncnt\n\n\n\n\n0\n2020-11-02\nat_risk_mau\nat_risk_mau\n271\n\n\n1\n2020-11-02\nat_risk_mau\ndormant\n4\n\n\n2\n2020-11-02\nat_risk_mau\nreactivated\n14\n\n\n3\n2020-11-02\nat_risk_wau\nat_risk_mau\n18\n\n\n4\n2020-11-02\nat_risk_wau\nat_risk_wau\n137\n\n\n\n\n\n\n\nNow, we can calculate the transition matrix \\(M\\). We define the get_transition_matrix function, which accepts the transitions DataFrame and a pair of dates that bounds the transitions to be considered.\n\n\nToggle the code\nstates_order = ['new', 'current', 'reactivated', 'resurrected', 'at_risk_wau', 'at_risk_mau', 'dormant']\n\ndef get_transition_matrix(transitions, date1, date2):\n    if pd.to_datetime(date1) &gt; pd.to_datetime(DATASET_END):\n        date1 = pd.to_datetime(DATASET_END) - pd.Timedelta(days=30)\n\n    probs = transitions\\\n        .loc[lambda _df: _df['date'].between(date1, date2)]\\\n        .groupby(['state_from', 'state_to'], as_index=False)\\\n        ['cnt'].sum()\\\n        .assign(\n            supp=lambda _df: _df.groupby('state_from')['cnt'].transform('sum'),\n            prob=lambda _df: _df['cnt'] / _df['supp']\n        )\n\n    M = probs.pivot(index='state_from', columns='state_to', values='prob')\\\n        .reindex(states_order, axis=0)\\\n        .reindex(states_order, axis=1)\\\n        .fillna(0)\\\n        .astype(float)\n\n    return M\n\n\nAs a baseline, let’s calculate the transition matrix for the whole year from 2021-11-01 to 2022-10-31.\n\nM = get_transition_matrix(transitions, '2021-11-01', '2022-10-31')\nM\n\n\n\n\n\n\n\nstate_to\nnew\ncurrent\nreactivated\nresurrected\nat_risk_wau\nat_risk_mau\ndormant\n\n\nstate_from\n\n\n\n\n\n\n\n\n\n\n\nnew\n0.0\n0.454626\n0.000000\n0.000000\n0.545374\n0.000000\n0.000000\n\n\ncurrent\n0.0\n0.846238\n0.000000\n0.000000\n0.153762\n0.000000\n0.000000\n\n\nreactivated\n0.0\n0.358974\n0.000000\n0.000000\n0.641026\n0.000000\n0.000000\n\n\nresurrected\n0.0\n0.359174\n0.000000\n0.000000\n0.640826\n0.000000\n0.000000\n\n\nat_risk_wau\n0.0\n0.090236\n0.004001\n0.000000\n0.771852\n0.133912\n0.000000\n\n\nat_risk_mau\n0.0\n0.000000\n0.008521\n0.000157\n0.000000\n0.951050\n0.040273\n\n\ndormant\n0.0\n0.000000\n0.000000\n0.000559\n0.000000\n0.000000\n0.999441\n\n\n\n\n\n\n\n\n\n3.5 Getting the initial state counts\nAn initial state is easily retrieved from the states DataFrame by the get_state0 function and the corresponding SQL query. We assign the result to the state0 variable.\n\n\nToggle the code\ndef get_state0(date):\n    query = f\"\"\"\n    SELECT state, count(*) AS cnt\n    FROM states\n    WHERE date = '{date}'\n    GROUP BY state\n    \"\"\"\n\n    state0 = duckdb.sql(query).df()\n    state0 = state0.set_index('state').reindex(states_order)['cnt']\n    \n    return state0\n\n\n\nstate0 = get_state0(DATASET_END)\nstate0\n\nstate\nnew               43\ncurrent          443\nreactivated       22\nresurrected       14\nat_risk_wau      433\nat_risk_mau     1273\ndormant        35865\nName: cnt, dtype: int64\n\n\n\n\n3.6 Predicting DAU\nThe predict_dau function below accepts all the previous variables required for the DAU prediction and makes this prediction for a date range defined by the start_date and end_date arguments.\n\n\nToggle the code\ndef predict_dau(M, state0, start_date, end_date, new_users):\n    \"\"\"\n    Predicts DAU over a given date range.\n\n    Parameters\n    ----------\n    M : pandas.DataFrame\n        Transition matrix representing user state changes.\n    state0 : pandas.Series\n        counts of initial state of users.\n    start_date : str\n        Start date of the prediction period in 'YYYY-MM-DD' format.\n    end_date : str\n        End date of the prediction period in 'YYYY-MM-DD' format.\n    new_users : int or pandas.Series\n        The expected amount of new users for each day between `start_date` and `end_date`.\n        If a Series, it should have dates as the index.\n        If an int, the same number is used for each day.\n        \n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame containing the predicted DAU, WAU, and MAU for each day in the date range,\n        with columns for different user states and tot.\n    \"\"\"\n    \n    dates = pd.date_range(start_date, end_date)\n    dates.name = 'date'\n    dau_pred = []\n    new_dau = state0.copy()\n    for date in dates:\n        new_dau = (M.transpose() @ new_dau).astype(int)\n        if isinstance(new_users, int):\n            new_users_today = new_users\n        else:\n            new_users_today = new_users.astype(int).loc[date] \n        new_dau.loc['new'] = new_users_today\n        dau_pred.append(new_dau.tolist())\n\n    dau_pred = pd.DataFrame(dau_pred, index=dates, columns=states_order)\n    dau_pred['dau'] = dau_pred['new'] + dau_pred['current'] + dau_pred['reactivated'] + dau_pred['resurrected']\n    dau_pred['wau'] = dau_pred['dau'] + dau_pred['at_risk_wau']\n    dau_pred['mau'] = dau_pred['dau'] + dau_pred['at_risk_wau'] + dau_pred['at_risk_mau']\n    \n    return dau_pred\n\n\n\ndau_pred = predict_dau(M, state0, PREDICTION_START, PREDICTION_END, new_users_pred)\ndau_pred\n\n\n\n\n\n\n\n\nnew\ncurrent\nreactivated\nresurrected\nat_risk_wau\nat_risk_mau\ndormant\ndau\nwau\nmau\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022-11-01\n49\n446\n12\n20\n448\n1268\n35896\n527\n975\n2243\n\n\n2022-11-02\n47\n451\n12\n20\n461\n1265\n35927\n530\n991\n2256\n\n\n2022-11-03\n43\n456\n12\n20\n471\n1264\n35957\n531\n1002\n2266\n\n\n2022-11-04\n39\n459\n12\n20\n477\n1265\n35987\n530\n1007\n2272\n\n\n2022-11-05\n41\n460\n12\n20\n480\n1266\n36017\n533\n1013\n2279\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-12-27\n135\n1180\n31\n36\n1236\n3144\n65077\n1382\n2618\n5762\n\n\n2023-12-28\n134\n1195\n31\n36\n1252\n3155\n65167\n1396\n2648\n5803\n\n\n2023-12-29\n132\n1209\n31\n36\n1266\n3168\n65257\n1408\n2674\n5842\n\n\n2023-12-30\n137\n1221\n32\n36\n1277\n3182\n65348\n1426\n2703\n5885\n\n\n2023-12-31\n151\n1235\n32\n37\n1291\n3197\n65439\n1455\n2746\n5943\n\n\n\n\n426 rows × 10 columns\n\n\n\nBesides the expected dau, wau, and mau columns, the output contains the number of users in each state for each prediction date.\nFinally, we calculate the ground-truth values of DAU, WAU, and MAU (along with the corresponding state decomposition), keep them in the dau_true DataFrame, and plot the predicted and true values altogether.\n\n\nToggle the code\nquery = f\"\"\"\nSELECT date, state, COUNT(*) AS cnt\nFROM states\nGROUP BY date, state\nORDER BY date, state;\n\"\"\"\n\ndau_true = duckdb.sql(query).df()\ndau_true['date'] = pd.to_datetime(dau_true['date'])\ndau_true = dau_true.pivot(index='date', columns='state', values='cnt')\ndau_true['dau'] = dau_true['new'] + dau_true['current'] + dau_true['reactivated'] + dau_true['resurrected']\ndau_true['wau'] = dau_true['dau'] + dau_true['at_risk_wau']\ndau_true['mau'] = dau_true['dau'] + dau_true['at_risk_wau'] + dau_true['at_risk_mau']\n\n\n\ndau_true.head()\n\n\n\n\n\n\n\nstate\nat_risk_mau\nat_risk_wau\ncurrent\ndormant\nnew\nreactivated\nresurrected\ndau\nwau\nmau\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020-11-01\n289.0\n206.0\n293.0\n801.0\n36.0\n14.0\n3.0\n346.0\n552.0\n841.0\n\n\n2020-11-02\n289.0\n207.0\n327.0\n797.0\n53.0\n14.0\n8.0\n402.0\n609.0\n898.0\n\n\n2020-11-03\n294.0\n204.0\n383.0\n801.0\n41.0\n10.0\n3.0\n437.0\n641.0\n935.0\n\n\n2020-11-04\n294.0\n245.0\n375.0\n803.0\n27.0\n13.0\n6.0\n421.0\n666.0\n960.0\n\n\n2020-11-05\n298.0\n274.0\n373.0\n806.0\n33.0\n8.0\n4.0\n418.0\n692.0\n990.0\n\n\n\n\n\n\n\n\n\nToggle the code\npd.concat([dau_true['dau'], dau_pred['dau']])\\\n    .plot(title='DAU, historical & predicted');\n\n\n\n\n\n\n\n\n\nAnd here we see that the prediction seems quite suspicious. It’s not clear why the DAU is going up that high while the historical data didn’t show the uptrend. We’ll discuss it in the next section."
  },
  {
    "objectID": "posts/2024-11-01_dau_prediction/dau_prediction.html#prediction-testing",
    "href": "posts/2024-11-01_dau_prediction/dau_prediction.html#prediction-testing",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "Prediction testing",
    "text": "Prediction testing\n\nstate0 = get_state0('2022-06-30')\nM = get_transition_matrix(transitions, '2022-06-23', '2022-06-30')\ndau_pred = predict_dau(M, state0, '2022-07-01', '2022-10-31', new_users)\n\ndau_true\\\n    .join(dau_pred[['dau']], how='inner', lsuffix='_true', rsuffix='_pred')\\\n    .plot()"
  },
  {
    "objectID": "posts/2024-11-01_dau_prediction/dau_prediction.html#discussion",
    "href": "posts/2024-11-01_dau_prediction/dau_prediction.html#discussion",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "Discussion",
    "text": "Discussion\nModeling DAU with Markov chain model seems"
  },
  {
    "objectID": "posts/2024-11-01_dau_prediction/dau_prediction.html#conclusions",
    "href": "posts/2024-11-01_dau_prediction/dau_prediction.html#conclusions",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "Conclusions",
    "text": "Conclusions"
  },
  {
    "objectID": "posts/2024-11-01_dau_prediction/dau_prediction.html#introdiction",
    "href": "posts/2024-11-01_dau_prediction/dau_prediction.html#introdiction",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "",
    "text": "Doubtlessly, DAU, WAU, and MAU are critical business metrics. An article “How Duolingo reignited user growth” by Jorge Mazal, former CPO of Duolingo is #1 the Growth section of Lenny’s Newsletter blog. In this article Jorge payed special attention to the methodology Duolingo used to model DAU metrics (see another article “Meaningful metrics: how data sharpened the focus of product teams” by Erin Gustafson). This methodology has multiple strenghts but I focus here on how one can use it for DAU forecasting.\nNew year is coming soon, so many companies are planning their budgets for the next year these days. Cost estimations are often require DAU forecast. In this article I’ll show you how you can get this prediction using the Duolingo’s growth model and share a DAU & MAU “calculator” designed as a Google Spreadsheet calculator."
  },
  {
    "objectID": "posts/2024-11-01_dau_prediction/dau_prediction.html#calculating-the-transition-matrix",
    "href": "posts/2024-11-01_dau_prediction/dau_prediction.html#calculating-the-transition-matrix",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "3.3 Calculating the transition matrix",
    "text": "3.3 Calculating the transition matrix\nNow, having these states obtained, we can calculate state transition frequencies. Again, due to large amount of data this part is better to execute with SQL on the database side, not with Python. For the reasons that will be explained further we calculate these frequencies day-wise. The result is stored in the transitions DataFrame.\n\n\nToggle the code\nquery = f\"\"\"\nSELECT\n    date,\n    state_from,\n    state_to,\n    COUNT(*) AS cnt,\nFROM (\n    SELECT\n        date,\n        state AS state_to,\n        lag(state) OVER (PARTITION BY user_id ORDER BY date) AS state_from\n    FROM states\n)\nWHERE state_from IS NOT NULL\nGROUP BY date, state_from, state_to\nORDER BY date, state_from, state_to;\n\"\"\"\ntransitions = duckdb.sql(query).df()\n\n\n\n\n\n\n\n\n\ndate\nstate_from\nstate_to\ncnt\n\n\n\n\n0\n2021-11-02\nat_risk_mau\nat_risk_mau\n1190\n\n\n1\n2021-11-02\nat_risk_mau\ndormant\n56\n\n\n2\n2021-11-02\nat_risk_mau\nreactivated\n16\n\n\n3\n2021-11-02\nat_risk_wau\nat_risk_mau\n53\n\n\n4\n2021-11-02\nat_risk_wau\nat_risk_wau\n409\n\n\n\n\n\n\n\n\ntransitions.head()\n\n\n\n\n\n\n\n\ndate\nstate_from\nstate_to\ncnt\n\n\n\n\n0\n2021-11-02\nat_risk_mau\nat_risk_mau\n1190\n\n\n1\n2021-11-02\nat_risk_mau\ndormant\n56\n\n\n2\n2021-11-02\nat_risk_mau\nreactivated\n16\n\n\n3\n2021-11-02\nat_risk_wau\nat_risk_mau\n53\n\n\n4\n2021-11-02\nat_risk_wau\nat_risk_wau\n409\n\n\n\n\n\n\n\n\nPREDICTION_START = '2022-11-01'\nPREDICTION_END = '2023-12-31'\n\nstates_order = ['new', 'current', 'reactivated', 'resurrected', 'at_risk_wau', 'at_risk_mau', 'dormant']\n\ndef get_transition_matrix(transitions, date1, date2):\n    probs = transitions\\\n        .loc[lambda _df: _df['date'].between(date1, date2)]\\\n        .groupby(['state_from', 'state_to'], as_index=False)\\\n        ['cnt'].sum()\\\n        .assign(\n            supp=lambda _df: _df.groupby('state_from')['cnt'].transform('sum'),\n            prob=lambda _df: _df['cnt'] / _df['supp']\n        )\n\n    M = probs.pivot(index='state_from', columns='state_to', values='prob')\\\n        .reindex(states_order, axis=0)\\\n        .reindex(states_order, axis=1)\\\n        .fillna(0)\\\n        .astype(float)\n\n    return M\n\n\nPredicting DAU\n\n\nCode\ndef predict_dau(M, state0, date1, date2, new_users):\n    \"\"\"\n    Predicts DAU over a given date range.\n\n    Parameters\n    ----------\n    M : pandas.DataFrame\n        Transition matrix representing user state changes.\n    state0 : pandas.Series\n        counts of initial state of users.\n    date1 : str\n        Start date of the prediction period in 'YYYY-MM-DD' format.\n    date2 : str\n        End date of the prediction period in 'YYYY-MM-DD' format.\n    new_users : int or pandas.Series\n        The expected amount of new users for each day between date1 and date2.\n        If a Series, it should have dates as the index.\n        If an int, the same number is used for each day.\n        \n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame containing the predicted DAU, WAU, and MAU for each day in the date range,\n        with columns for different user states and tot.\n    \"\"\"\n    \n    dates = pd.date_range(date1, date2)\n    dates.name = 'date'\n    dau_pred = []\n    new_dau = state0.copy()\n    for date in dates:\n        new_dau = (M.transpose() @ new_dau).astype(int)\n        if isinstance(new_users, int):\n            new_users_today = new_users\n        else:\n            new_users_today = new_users.astype(int).loc[date] \n        new_dau.loc['new'] = new_users_today\n        dau_pred.append(new_dau.tolist())\n\n    dau_pred = pd.DataFrame(dau_pred, index=dates, columns=states_order)\n    dau_pred['dau'] = dau_pred['new'] + dau_pred['current'] + dau_pred['reactivated'] + dau_pred['resurrected']\n    dau_pred['wau'] = dau_pred['dau'] + dau_pred['at_risk_wau']\n    dau_pred['mau'] = dau_pred['dau'] + dau_pred['at_risk_wau'] + dau_pred['at_risk_mau']\n    \n    return dau_pred\n\n\ndef get_state0(date):\n    query = f\"\"\"\n    SELECT state, count(*) AS cnt\n    FROM states\n    WHERE date = '{date}'\n    GROUP BY state\n    \"\"\"\n\n    state0 = duckdb.sql(query).df()\n    state0 = state0.set_index('state').reindex(states_order)['cnt']\n    \n    return state0\n\n\n\nnew_users_pred\n\ndate\n2022-11-01     49\n2022-11-02     47\n2022-11-03     43\n2022-11-04     39\n2022-11-05     41\n             ... \n2023-12-27    135\n2023-12-28    134\n2023-12-29    132\n2023-12-30    137\n2023-12-31    151\nName: count, Length: 426, dtype: int64\n\n\n\nM = get_transition_matrix(transitions, '2022-10-25', '2022-10-31')\nstate0 = get_state0(DATASET_END)\ndau_pred = predict_dau(M, state0, PREDICTION_START, PREDICTION_END, new_users_pred)\ndau_pred\n\n\n\n\n\n\n\n\nnew\ncurrent\nreactivated\nresurrected\nat_risk_wau\nat_risk_mau\ndormant\ndau\nwau\nmau\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2022-11-01\n49\n444\n13\n13\n451\n1269\n35901\n519\n970\n2239\n\n\n2022-11-02\n47\n446\n13\n13\n462\n1267\n35936\n519\n981\n2248\n\n\n2022-11-03\n43\n448\n13\n13\n470\n1267\n35971\n517\n987\n2254\n\n\n2022-11-04\n39\n448\n13\n13\n474\n1268\n36006\n513\n987\n2255\n\n\n2022-11-05\n41\n447\n13\n13\n475\n1269\n36041\n514\n989\n2258\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2023-12-27\n135\n1083\n31\n24\n1160\n2972\n65468\n1273\n2433\n5405\n\n\n2023-12-28\n134\n1098\n31\n24\n1176\n2983\n65558\n1287\n2463\n5446\n\n\n2023-12-29\n132\n1112\n31\n24\n1190\n2995\n65649\n1299\n2489\n5484\n\n\n2023-12-30\n137\n1124\n31\n24\n1202\n3008\n65740\n1316\n2518\n5526\n\n\n2023-12-31\n151\n1138\n31\n24\n1216\n3022\n65832\n1344\n2560\n5582\n\n\n\n\n426 rows × 10 columns\n\n\n\n\nquery = \"\"\"\nSELECT date, COUNT(*) AS dau\nFROM states\nWHERE state IN ('new', 'current', 'reactivated', 'resurrected')\nGROUP BY date\nORDER BY date\n\"\"\"\n\ndau_true = duckdb.sql(query).df()\\\n    .set_index('date')\npd.concat([dau_true, dau_pred['dau']]).plot(title='DAU, historical & predicted');"
  },
  {
    "objectID": "posts/2024-11-01_dau_prediction/dau_prediction.html#model-evaluation",
    "href": "posts/2024-11-01_dau_prediction/dau_prediction.html#model-evaluation",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "4. Model evaluation",
    "text": "4. Model evaluation\n\n4.1 General evaluation\nFirst of all, let’s test our model on historical data choosing two prediction periods: 2022-03-01 - 2022-10-31 and 2022-07-01 - 2022-10-31. As for the transitions period, so far we consider full history from DATASET_START to DATASET_END.\n\n\nToggle the code\nimport re\nfrom sklearn.metrics import mean_absolute_percentage_error\n\n\ndef make_prediction(prediction_start, prediction_end, new_users, mode='last_30d'):\n    prediction_start_minus_1d = pd.to_datetime(prediction_start) - pd.Timedelta('1d')\n    state0 = get_state0(prediction_start_minus_1d)\n\n    if mode.startswith('last_'):\n        shift = int(re.search(r'last_(\\d+)d', mode).group(1))\n        transitions_start = pd.to_datetime(prediction_start) - pd.Timedelta(shift, 'd')\n        M = get_transition_matrix(transitions, transitions_start, prediction_start_minus_1d)\n        dau_pred = predict_dau(M, state0, prediction_start, prediction_end, new_users)\n    else:\n        transitions_start = pd.to_datetime(prediction_start) - pd.Timedelta(30, 'd')\n        M_last_month = get_transition_matrix(transitions, transitions_start, prediction_start_minus_1d)\n        dau_pred = pd.DataFrame()\n\n        month_starts = pd.date_range(prediction_start, prediction_end, freq='1MS')\n        N = len(month_starts)\n\n        for i, prediction_month_start in enumerate(month_starts):\n            prediction_month_end = pd.offsets.MonthEnd().rollforward(prediction_month_start)\n            transitions_month_start = prediction_month_start - pd.Timedelta('365D')\n            transitions_month_end = prediction_month_end - pd.Timedelta('365D')\n\n            M_seasonal = get_transition_matrix(transitions, transitions_month_start, transitions_month_end)\n            if mode == 'smoothing':\n                i = min(i, 12)\n                M = M_seasonal * i / (N - 1)  + (1 - i / (N - 1)) * M_last_month\n            elif mode == 'seasonal_0.1':\n                M = 0.1 * M_seasonal + 0.9 * M_last_month\n            \n            dau_tmp = predict_dau(M, state0, prediction_month_start, prediction_month_end, new_users_pred)\n            dau_pred = pd.concat([dau_pred, dau_tmp])\n\n            state0 = dau_tmp.loc[prediction_month_end][states_order]\n\n    return dau_pred\n\ndef prediction_details(dau_true, dau_pred, show_plot=True):\n    y_true = dau_true.reindex(dau_pred.index)['dau']\n    y_pred = dau_pred['dau']\n    mape = mean_absolute_percentage_error(y_true, y_pred) \n\n    if show_plot:\n        y_true.plot(label='DAU true')\n        y_pred.plot(label='DAU pred')\n        prediction_start = str(y_true.index.min().date())\n        prediction_end = str(y_true.index.max().date())\n        plt.title(f'DAU prediction, {prediction_start} - {prediction_end}')\n        plt.legend()\n\n    return mape\n\n\n\ndau_pred = make_prediction('2022-07-01', '2022-10-31', new_users_pred, mode='last_365d')\nmape = prediction_details(dau_true, dau_pred)\nprint(f'MAPE: {mape:.4f}')\n\nMAPE: 0.0681\n\n\n\n\n\n\n\n\n\n\ndau_pred = make_prediction('2022-03-01', '2022-10-31', new_users_pred, mode='last_365d')\nmape = prediction_details(dau_true, dau_pred)\nprint(f'MAPE: {mape:.4f}')\n\nMAPE: 0.0874\n\n\n\n\n\n\n\n\n\n\ndau_pred = make_prediction('2021-11-01', '2022-10-31', new_users_pred, mode='last_365d')\nmape = prediction_details(dau_true, dau_pred)\nprint(f'MAPE: {mape:.4f}')\n\nMAPE: 0.1764\n\n\n\n\n\n\n\n\n\nWe notice multiple things. - While the MAPE values in both cases are reasonable (6-10%), the model doesn’t capture trends well. - The transition period matters. In the second case we see that from the beginning the predicted values are substantially lowen than the true values. It means that the transition matrix supposes that the users are less likely to retain that they are in reality. - The trend changes are caused by the new users prediction. - The predicted values are of less variance than the real DAU values. It’s explained by the fact that the model doesn’t consider weekly seasonality at all (i.e. day of week). - A huge spike in the beginning of June obviously relates to some unexpected activity – probably marketing. We’ll explore it further.\nSince the transition matrix is fixed for the whole prediction period, the only thing that can affect the trend changes is the variation in the new users prediction. Let’s plot the new_users_pred time-series for the corresponding period.\n\nnew_users_pred.loc['2022-03-01':'2022-10-31']\\\n    .plot(title='New users prediction');\n\n\n\n\n\n\n\n\nIt’s clear now that the predicted DAU trend changes coincide with the new users prediction trend changes.\nNow, let’s decompose the prediction error and see which states contribure the most.\n\n\nToggle the code\ndau_component_cols = ['new', 'current', 'reactivated', 'resurrected']\n\nfigure, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\ndau_pred[dau_component_cols]\\\n    .subtract(dau_true[dau_component_cols])\\\n    .reindex(dau_pred.index)\\\n    .plot(title='Prediction error by state', ax=ax1)\n\ndau_pred[['current']]\\\n    .subtract(dau_true[['current']])\\\n    .div(dau_true[['current']])\\\n    .reindex(dau_pred.index)\\\n    .plot(title='Relative prediction error (current state)', ax=ax2);\n\n\n\n\n\n\n\n\n\nFirst, we note that the error for new, reactivated, and resurrected states is quite low while for the current state is high. Unsurprisingly, this is because the current state contributes to DAU the most. As for the spike in July, we see now that it was caused my some CRM activity bacuse the corresponding spike occured particularly in the resurrected state.\nAnalysing relative errors for all the states seems to be not too informative, especially for the reactivated and resurrected states: their daily values are quite low, so the error appears to be to high and noisy. On the right diagram we see the relative error for the current state only. This error varies between -20% and 20% which we personally consider as high. And since the error of the new state is low, it can be explained only by the fact that the fixed transition matrix poorly describes user activity in the prediction period.\n\n\n4.2 Transitions period impact\nHere we’re going to estimate how long a period before the prediction date should be considered to calculate the transition matrix values. We fix the same prediction period 2022-03-01 - 2022-10-31 and vary the transitions period: from 1 week to 1 year. In order to mitigate the noice from the new users prediction, we use the real values of the new users amount.\n\n\nToggle the code\nprediction_period_start = '2022-03-01'\nprediction_period_end = '2022-10-31'\n\nresult = []\nfor shift in range(7, 180, 7):\n    dau_pred = make_prediction(prediction_period_start, prediction_period_end, new_users, mode=f'last_{shift}d')\n    mape = prediction_details(dau_true, dau_pred, show_plot=False)\n    result.append([shift, mape])\nresult = pd.DataFrame(result, columns=['shift', 'mape'])\n\nresult.set_index('shift')['mape']\\\n    .plot(title='MAPE by shift')\n\n\n\n\n\n\n\n\n\nAs we see, the MAPE curve has U-shape. This is quite clear: too short period can’t capture signal from the most recent users while too long period is too genral and captures signal that is not valid anymore. Hence, the optimal transitions period is 4-7 weeks.\n\n\n4.3 Obsolence and sesonality\nNevertheless, fixing a single transition matrix for predicting the whole year ahead doesn’t seem to be a good idea: such a model would be too rigid. Usually, the user behavior varies depending on a season when a user comes in for the first time. For example, the users who appear after Christmas are of a different type: they could be more motivated to use an app or whatever else. In this section, to predict DAU in some prediction period we will also consider a transition matrix \\(M_{seasonal}\\) from the corresponding period in the past. We consider three configurations:\n\nlast_30d: the transition matrix is calculated from the last 30 days of the training data.\nseasonal_0.1: \\(M = 0.1 \\cdot M_{seasonal} + 0.9 \\cdot M_{last\\_30d}\\).\nsmoothing: \\(M = \\frac{i}{N - 1} M_{seasonal} + (1 - \\frac{i}{N - 1}) M_{last\\_30d}\\) where \\(N\\) is the number of months in the predicting period, \\(i = 0, \\ldots, N - 1\\) – the month index. The idea of this configuration is to gradually switch from the most recent transition matrix to the seasonal one.\n\n\n\nToggle the code\nresult = pd.DataFrame()\nfor mode in ['last_30d', 'seasonal_0.1', 'smoothing']:\n    result[mode] = make_prediction('2021-11-01', '2022-10-31', new_users_pred, mode)['dau']\nresult['true'] = dau_true['dau']\nresult['true'] = result['true'].astype(int)\nresult.plot(title='DAU prediction by different transition matrices');\n\n\n\n\n\n\n\n\n\n\n\nToggle the code\nmape = pd.DataFrame()\nfor col in result.columns:\n    if col != 'true':\n        mape.loc[col, 'mape'] = mean_absolute_percentage_error(result['true'], result[col])\nmape\n\n\n\n\n\n\n\n\n\nmape\n\n\n\n\nlast_30d\n0.136540\n\n\nseasonal_0.1\n0.127246\n\n\nsmoothing\n0.093658\n\n\n\n\n\n\n\nAccording to the MAPE values, smoothing configuration provides the best results. The seasonal_0.1 option demonstrates a bit better results than the last_30d one. Perhaps, it could be improved by chossing weight other than 0.1.\nInterestingly, we notice how the difference between the predicted time-series of different configurations decays as the prediction period increases. It means that the impulse from the most recent \\(M_{last\\_30d}\\) matrix fades away and by the year, so that the DAU prediction is detrimented by the new users prediction and the transition matrix, converged and outdated.\n\n\nToggle the code\ndau_pred = make_prediction(PREDICTION_START, PREDICTION_END, new_users_pred, mode='smoothing')\ndau_pred2 = make_prediction(PREDICTION_START, PREDICTION_END, new_users_pred, mode='last_365d')\npd.concat([dau_true['dau'], dau_pred['dau']]).plot(title='DAU, historical & predicted')\ndau_pred2['dau'].plot(label='365d')\nplt.legend();"
  },
  {
    "objectID": "posts/2024-11-01_dau_prediction/dau_prediction.html#introduction",
    "href": "posts/2024-11-01_dau_prediction/dau_prediction.html#introduction",
    "title": "Modeling DAU, WAU, MAU with Markov chain (WIP)",
    "section": "",
    "text": "Doubtlessly, DAU, WAU, and MAU are critical business metrics. An article “How Duolingo reignited user growth” by Jorge Mazal, former CPO of Duolingo is #1 the Growth section of Lenny’s Newsletter blog. In this article Jorge payed special attention to the methodology Duolingo used to model DAU metrics (see another article “Meaningful metrics: how data sharpened the focus of product teams” by Erin Gustafson). This methodology has multiple strenghts but I focus here on how one can use it for DAU forecasting.\nNew year is coming soon, so many companies are planning their budgets for the next year these days. Cost estimations are often require DAU forecast. In this article I’ll show you how you can get this prediction using the Duolingo’s growth model and share a DAU & MAU “calculator” designed as a Google Spreadsheet calculator."
  }
]